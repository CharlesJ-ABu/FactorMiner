"""
å› å­æŒ–æ˜APIè·¯ç”±
æä¾›å› å­æŒ–æ˜ç›¸å…³çš„APIæ¥å£ï¼ŒåŒ…å«å®æ—¶è¿›åº¦åé¦ˆ
"""

from flask import Blueprint, request, jsonify, current_app, Response
import pandas as pd
import numpy as np
from datetime import datetime
import json
from pathlib import Path
import sys
import os
import time
import threading
from queue import Queue
import uuid
import psutil
import gc

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from factor_miner.api.factor_mining_api import FactorMiningAPI
from factor_miner.factors.ml_factors import MLFactorBuilder
from factor_miner.factors.technical import FactorCalculator
from factor_miner.factors.statistical import StatisticalFactorBuilder
from factor_miner.factors.advanced import AdvancedFactorBuilder

bp = Blueprint('mining_api', __name__, url_prefix='/api/mining')

# å…¨å±€å› å­æŒ–æ˜APIå®ä¾‹
_mining_api = None
_ml_factor_builder = None
_technical_factor_builder = None
_statistical_factor_builder = None
_advanced_factor_builder = None

# æŒ–æ˜ä¼šè¯ç®¡ç†
mining_sessions = {}
mining_progress = {}

# è¿›åº¦ä¼°ç®—é…ç½® - åŸºäºçœŸå®ç®—æ³•è°ƒæ•´
PROGRESS_ESTIMATES = {
    'data_loading': {
        'base_time': 5,  # åŸºç¡€æ—¶é—´ï¼ˆç§’ï¼‰
        'time_per_gb': 2,  # æ¯GBæ•°æ®é¢å¤–æ—¶é—´
        'max_progress_steps': 10
    },
    'factor_building': {
        'base_time': 25,  # åŸºç¡€æ—¶é—´ï¼ˆç§’ï¼‰
        'time_per_factor_type': 12,  # æ¯ç§å› å­ç±»å‹é¢å¤–æ—¶é—´
        'time_per_symbol': 5,  # æ¯ä¸ªäº¤æ˜“å¯¹é¢å¤–æ—¶é—´
        'time_per_ml_algorithm': 25,  # æ¯ä¸ªMLç®—æ³•é¢å¤–æ—¶é—´
        'time_per_technical_factor': 8,  # æ¯ä¸ªæŠ€æœ¯å› å­é¢å¤–æ—¶é—´
        'time_per_statistical_factor': 10,  # æ¯ä¸ªç»Ÿè®¡å› å­é¢å¤–æ—¶é—´
        'time_per_advanced_factor': 15,  # æ¯ä¸ªé«˜çº§å› å­é¢å¤–æ—¶é—´
        'max_progress_steps': 35
    },
    'factor_evaluation': {
        'base_time': 25,  # åŸºç¡€æ—¶é—´ï¼ˆç§’ï¼‰
        'time_per_factor': 0.8,  # æ¯ä¸ªå› å­é¢å¤–æ—¶é—´
        'time_per_symbol': 5,  # æ¯ä¸ªäº¤æ˜“å¯¹é¢å¤–æ—¶é—´
        'max_progress_steps': 30
    },
    'factor_optimization': {
        'base_time': 15,  # åŸºç¡€æ—¶é—´ï¼ˆç§’ï¼‰
        'time_per_factor': 0.5,  # æ¯ä¸ªå› å­é¢å¤–æ—¶é—´
        'max_progress_steps': 20
    },
    'result_saving': {
        'base_time': 5,  # åŸºç¡€æ—¶é—´ï¼ˆç§’ï¼‰
        'max_progress_steps': 8
    }
}

def get_mining_api():
    """è·å–å› å­æŒ–æ˜APIå®ä¾‹"""
    global _mining_api
    if _mining_api is None:
        _mining_api = FactorMiningAPI()
    return _mining_api

def get_ml_factor_builder():
    """è·å–MLå› å­æ„å»ºå™¨å®ä¾‹"""
    global _ml_factor_builder
    if _ml_factor_builder is None:
        _ml_factor_builder = MLFactorBuilder()
    return _ml_factor_builder

def get_technical_factor_builder():
    """è·å–æŠ€æœ¯å› å­æ„å»ºå™¨å®ä¾‹"""
    global _technical_factor_builder
    if _technical_factor_builder is None:
        _technical_factor_builder = FactorCalculator()
    return _technical_factor_builder

def get_statistical_factor_builder():
    """è·å–ç»Ÿè®¡å› å­æ„å»ºå™¨å®ä¾‹"""
    global _statistical_factor_builder
    if _statistical_factor_builder is None:
        _statistical_factor_builder = StatisticalFactorBuilder()
    return _statistical_factor_builder

def get_advanced_factor_builder():
    """è·å–é«˜çº§å› å­æ„å»ºå™¨å®ä¾‹"""
    global _advanced_factor_builder
    if _advanced_factor_builder is None:
        _advanced_factor_builder = AdvancedFactorBuilder()
    return _advanced_factor_builder

def estimate_step_time(step_name, config, data_info=None):
    """ä¼°ç®—æ­¥éª¤æ‰§è¡Œæ—¶é—´"""
    if step_name not in PROGRESS_ESTIMATES:
        return 10, 10  # é»˜è®¤æ—¶é—´å’Œè¿›åº¦æ­¥æ•°
    
    estimates = PROGRESS_ESTIMATES[step_name]
    base_time = estimates['base_time']
    max_steps = estimates['max_progress_steps']
    
    # æ ¹æ®é…ç½®è°ƒæ•´æ—¶é—´
    if step_name == 'data_loading':
        # æ•°æ®åŠ è½½æ—¶é—´åŸºäºæ•°æ®å¤§å°
        if data_info and 'data_size_mb' in data_info:
            data_size_gb = data_info['data_size_mb'] / 1024
            estimated_time = base_time + (data_size_gb * estimates['time_per_gb'])
        else:
            estimated_time = base_time + 5  # é»˜è®¤é¢å¤–5ç§’
        progress_steps = min(max_steps, int(estimated_time / 2))
        
    elif step_name == 'factor_building':
        # å› å­æ„å»ºæ—¶é—´åŸºäºå› å­ç±»å‹å’Œäº¤æ˜“å¯¹æ•°é‡
        factor_types = config.get('factor_types', [])
        symbols = len(config.get('symbols', []))
        
        # ä¸åŒç±»å‹å› å­çš„æ—¶é—´ä¼°ç®—
        total_factor_time = 0
        for factor_type in factor_types:
            if factor_type == 'ml':
                total_factor_time += estimates['time_per_ml_algorithm']
            elif factor_type == 'technical':
                total_factor_time += estimates['time_per_technical_factor']
            elif factor_type == 'statistical':
                total_factor_time += estimates['time_per_statistical_factor']
            elif factor_type == 'advanced':
                total_factor_time += estimates['time_per_advanced_factor']
            else:
                total_factor_time += estimates['time_per_factor_type']
        
        estimated_time = base_time + total_factor_time + (symbols * estimates['time_per_symbol'])
        progress_steps = min(max_steps, int(estimated_time / 3))
        
    elif step_name == 'factor_evaluation':
        # å› å­è¯„ä¼°æ—¶é—´åŸºäºå› å­æ•°é‡å’Œäº¤æ˜“å¯¹æ•°é‡
        factor_types = config.get('factor_types', [])
        symbols = len(config.get('symbols', []))
        
        # ä¸åŒç±»å‹å› å­ç”Ÿæˆçš„æ•°é‡ä¼°ç®—
        estimated_factors = 0
        for factor_type in factor_types:
            if factor_type == 'ml':
                estimated_factors += 35  # MLç±»å‹ç”Ÿæˆæ›´å¤šå› å­
            elif factor_type == 'technical':
                estimated_factors += 25  # æŠ€æœ¯å› å­æ•°é‡
            elif factor_type == 'statistical':
                estimated_factors += 30  # ç»Ÿè®¡å› å­æ•°é‡
            elif factor_type == 'advanced':
                estimated_factors += 20  # é«˜çº§å› å­æ•°é‡
            else:
                estimated_factors += 20
        
        estimated_time = (base_time + 
                         estimated_factors * estimates['time_per_factor'] +
                         symbols * estimates['time_per_symbol'])
        progress_steps = min(max_steps, int(estimated_time / 4))
        
    elif step_name == 'factor_optimization':
        # å› å­ä¼˜åŒ–æ—¶é—´åŸºäºå› å­æ•°é‡
        factor_types = config.get('factor_types', [])
        estimated_factors = 0
        for factor_type in factor_types:
            if factor_type == 'ml':
                estimated_factors += 35
            elif factor_type == 'technical':
                estimated_factors += 25
            elif factor_type == 'statistical':
                estimated_factors += 30
            elif factor_type == 'advanced':
                estimated_factors += 20
            else:
                estimated_factors += 20
        
        estimated_time = base_time + estimated_factors * estimates['time_per_factor']
        progress_steps = min(max_steps, int(estimated_time / 2))
        
    else:  # result_saving
        estimated_time = base_time
        progress_steps = max_steps
    
    return max(estimated_time, 1), progress_steps

def get_system_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯ç”¨äºè¿›åº¦ä¼°ç®—"""
    try:
        cpu_count = psutil.cpu_count()
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        
        return {
            'cpu_count': cpu_count,
            'memory_gb': round(memory_gb, 1),
            'memory_percent': memory.percent
        }
    except:
        return {
            'cpu_count': 4,
            'memory_gb': 8.0,
            'memory_percent': 50
        }

@bp.route('/start', methods=['POST'])
def start_mining():
    """å¯åŠ¨å› å­æŒ–æ˜"""
    try:
        data = request.get_json()
        
        # éªŒè¯å¿…è¦å‚æ•°
        required_fields = ['symbols', 'timeframes', 'factor_types', 'start_date', 'end_date']
        for field in required_fields:
            if field not in data:
                return jsonify({
                    'success': False,
                    'error': f'ç¼ºå°‘å¿…è¦å‚æ•°: {field}'
                }), 400
        
        # åˆ›å»ºæŒ–æ˜ä¼šè¯
        session_id = str(uuid.uuid4())
        
        # è·å–ç³»ç»Ÿä¿¡æ¯
        system_info = get_system_info()
        
        # åˆå§‹åŒ–è¿›åº¦ä¿¡æ¯
        progress_info = {}
        total_estimated_time = 0
        
        for step_name in ['data_loading', 'factor_building', 'factor_evaluation', 'factor_optimization', 'result_saving']:
            estimated_time, progress_steps = estimate_step_time(step_name, data)
            progress_info[step_name] = {
                'estimated_time': estimated_time,
                'progress_steps': progress_steps,
                'current_progress': 0,
                'start_time': None,
                'current_step_start': None
            }
            total_estimated_time += estimated_time
        
        mining_sessions[session_id] = {
            'status': 'pending',
            'start_time': datetime.now().isoformat(),
            'config': data,
            'progress': {
                'data_loading': 0,
                'factor_building': 0,
                'factor_evaluation': 0,
                'factor_optimization': 0,
                'result_saving': 0
            },
            'progress_info': progress_info,
            'total_estimated_time': total_estimated_time,
            'current_step': 'data_loading',
            'messages': [],
            'system_info': system_info
        }
        
        # æ„å»ºæŒ–æ˜é…ç½®
        mining_config = {
            'factor_types': data['factor_types'],
            'optimization': {
                'method': data.get('optimization_method', 'greedy'),
                'max_factors': data.get('max_factors', 15),
                'min_ic': data.get('min_ic', 0.02),
                'min_ir': data.get('min_ir', 0.1)
            },
            'evaluation': {
                'min_sample_size': data.get('min_sample_size', 30),
                'metrics': ['ic_pearson', 'ic_spearman', 'sharpe_ratio', 'win_rate', 'factor_decay']
            }
        }
        
        # åœ¨åå°å¯åŠ¨æŒ–æ˜
        def run_mining_background():
            try:
                api = get_mining_api()
                
                # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
                mining_sessions[session_id]['status'] = 'running'
                mining_sessions[session_id]['current_step'] = 'data_loading'
                
                # æ­¥éª¤1: æ•°æ®åŠ è½½
                step_start_time = time.time()
                mining_sessions[session_id]['progress_info']['data_loading']['start_time'] = step_start_time
                mining_sessions[session_id]['progress_info']['data_loading']['current_step_start'] = step_start_time
                
                update_session_progress(session_id, 'data_loading', 0, 'å¼€å§‹åŠ è½½å¸‚åœºæ•°æ®...')
                
                # è·å–æ•°æ®å¤§å°ä¿¡æ¯
                data_info = get_data_info(data['symbols'][0], data['timeframes'][0], data['start_date'], data['end_date'])
                
                data_result = api.load_data(
                    data['symbols'][0], 
                    data['timeframes'][0], 
                    data['start_date'], 
                    data['end_date']
                )
                
                if not data_result['success']:
                    raise Exception(f"æ•°æ®åŠ è½½å¤±è´¥: {data_result['error']}")
                
                # æ›´æ–°æ•°æ®ä¿¡æ¯
                if 'data_info' not in mining_sessions[session_id]:
                    mining_sessions[session_id]['data_info'] = {}
                mining_sessions[session_id]['data_info'].update(data_info)
                
                update_session_progress(session_id, 'data_loading', 100, 'æ•°æ®åŠ è½½å®Œæˆ')
                
                # æ­¥éª¤2: å› å­æ„å»º
                step_start_time = time.time()
                mining_sessions[session_id]['progress_info']['factor_building']['start_time'] = step_start_time
                mining_sessions[session_id]['progress_info']['factor_building']['current_step_start'] = step_start_time
                # åˆå§‹åŒ–å­è¿›åº¦å®¹å™¨ï¼Œç¡®ä¿å‰ç«¯å¯è§
                mining_sessions[session_id]['progress_info'].setdefault('sub_progress', {})
                mining_sessions[session_id]['progress_info'].setdefault('sub_messages', {})
                mining_sessions[session_id]['progress_info']['sub_progress'].setdefault('ml', 0)
                
                update_session_progress(session_id, 'factor_building', 0, 'å¼€å§‹æ„å»ºå› å­...')
                
                # æ ¹æ®é€‰æ‹©çš„å› å­ç±»å‹æ„å»ºçœŸå®å› å­
                factor_types = data.get('factor_types', [])
                total_types = len(factor_types)
                
                # å› å­ç±»å‹æ˜¾ç¤ºåç§°æ˜ å°„
                factor_type_names = {
                    'technical': 'æŠ€æœ¯å› å­',
                    'statistical': 'ç»Ÿè®¡å› å­',
                    'advanced': 'é«˜çº§å› å­',
                    'ml': 'æœºå™¨å­¦ä¹ å› å­',
                    'crypto': 'åŠ å¯†å› å­',
                    'pattern': 'å½¢æ€å› å­',
                    'composite': 'å¤åˆå› å­',
                    'sentiment': 'æƒ…æ„Ÿå› å­'
                }
                
                # æ„å»ºå› å­
                all_factors = pd.DataFrame(index=data_result['data'].index)
                
                for i, factor_type in enumerate(factor_types):
                    progress_percent = int((i / total_types) * 80) + 10  # 10% - 90%
                    factor_type_name = factor_type_names.get(factor_type, factor_type)
                    try:
                        if factor_type == 'ml':
                            # ä½¿ç”¨çœŸå®çš„MLå› å­æ„å»ºç®—æ³•
                            update_session_progress(session_id, 'factor_building', progress_percent, f'æ­£åœ¨æ„å»ºæœºå™¨å­¦ä¹ å› å­...')
                            
                            ml_builder = get_ml_factor_builder()
                            
                            # å¼€å§‹MLå› å­æ„å»º
                            
                            # æ‰§è¡ŒMLå› å­æ„å»ºï¼Œå¸¦è¿›åº¦æ›´æ–°
                            try:
                                # å¼€å§‹æ„å»ºé›†æˆå­¦ä¹ å› å­
                                update_session_progress(session_id, 'factor_building', progress_percent + 5, 'æ­£åœ¨æ„å»ºé›†æˆå­¦ä¹ å› å­...')
                                print(f"å¼€å§‹æ„å»ºé›†æˆå­¦ä¹ å› å­...")
                                
                                # å®šä¹‰æ™®é€šæŒ–æ˜çš„MLè¿›åº¦å›è°ƒ
                                def ml_progress_callback_normal(stage: str, progress: int, message: str = ""):
                                    try:
                                        pi = mining_sessions[session_id].setdefault('progress_info', {})
                                        sp = pi.setdefault('sub_progress', {})
                                        sm = pi.setdefault('sub_messages', {})
                                        sp[stage] = int(progress)
                                        print(f"ğŸ¯ æ™®é€šæŒ–æ˜MLè¿›åº¦: {stage} -> {progress}%")
                                        if message:
                                            sm[stage] = message
                                        # ä¸æ›´æ–°ä¸»è¿›åº¦ï¼Œåªæ›´æ–°å­è¿›åº¦ï¼Œé¿å…ä¸»è¿›åº¦æ¡"ç¼©æ”¾"æ•ˆæœ
                                    except Exception as e:
                                        print(f"âŒ æ™®é€šæŒ–æ˜MLè¿›åº¦å›è°ƒé”™è¯¯: {e}")
                                
                                # è®©tqdmæ­£å¸¸å·¥ä½œï¼Œæ˜¾ç¤ºçœŸå®è¿›åº¦
                                ensemble_factors = ml_builder.build_ensemble_factors(
                                    data_result['data'],
                                    progress_callback=ml_progress_callback_normal,
                                    window=252,
                                    n_estimators=100
                                )
                                print(f"é›†æˆå­¦ä¹ å› å­æ„å»ºå®Œæˆ: {len(ensemble_factors.columns)} ä¸ª")
                                
                                # æ„å»ºPCAå› å­
                                update_session_progress(session_id, 'factor_building', progress_percent + 10, 'æ­£åœ¨æ„å»ºPCAå› å­...')
                                print(f"å¼€å§‹æ„å»ºPCAå› å­...")
                                pca_factors = ml_builder.build_pca_factors(
                                    data_result['data'],
                                    progress_callback=ml_progress_callback_normal,
                                    n_components=10
                                )
                                print(f"PCAå› å­æ„å»ºå®Œæˆ: {len(pca_factors.columns)} ä¸ª")
                                
                                # æ„å»ºç‰¹å¾é€‰æ‹©å› å­
                                update_session_progress(session_id, 'factor_building', progress_percent + 15, 'æ­£åœ¨æ„å»ºç‰¹å¾é€‰æ‹©å› å­...')
                                print(f"å¼€å§‹æ„å»ºç‰¹å¾é€‰æ‹©å› å­...")
                                feature_factors = ml_builder.build_feature_selection_factors(
                                    data_result['data'],
                                    progress_callback=ml_progress_callback_normal,
                                    k_best=20
                                )
                                print(f"ç‰¹å¾é€‰æ‹©å› å­æ„å»ºå®Œæˆ: {len(feature_factors.columns)} ä¸ª")
                                
                                # æ„å»ºæ»šåŠ¨MLå› å­
                                update_session_progress(session_id, 'factor_building', progress_percent + 18, 'æ­£åœ¨æ„å»ºæ»šåŠ¨MLå› å­...')
                                print(f"å¼€å§‹æ„å»ºæ»šåŠ¨MLå› å­...")
                                rolling_factors = ml_builder.build_rolling_ml_factors(
                                    data_result['data'],
                                    progress_callback=ml_progress_callback_normal,
                                    window=252,
                                    rolling_window=60
                                )
                                print(f"æ»šåŠ¨MLå› å­æ„å»ºå®Œæˆ: {len(rolling_factors.columns)} ä¸ª")
                                
                                # æ„å»ºè‡ªé€‚åº”MLå› å­
                                update_session_progress(session_id, 'factor_building', progress_percent + 20, 'æ­£åœ¨æ„å»ºè‡ªé€‚åº”MLå› å­...')
                                print(f"å¼€å§‹æ„å»ºè‡ªé€‚åº”MLå› å­...")
                                adaptive_factors = ml_builder.build_adaptive_ml_factors(
                                    data_result['data'],
                                    progress_callback=ml_progress_callback_normal,
                                    threshold=0.1
                                )
                                print(f"è‡ªé€‚åº”MLå› å­æ„å»ºå®Œæˆ: {len(adaptive_factors.columns)} ä¸ª")
                                
                                # åˆå¹¶æ‰€æœ‰MLå› å­
                                ml_factors_list = []
                                if not ensemble_factors.empty:
                                    ml_factors_list.append(ensemble_factors)
                                if not pca_factors.empty:
                                    ml_factors_list.append(pca_factors)
                                if not feature_factors.empty:
                                    ml_factors_list.append(feature_factors)
                                if not rolling_factors.empty:
                                    ml_factors_list.append(rolling_factors)
                                if not adaptive_factors.empty:
                                    ml_factors_list.append(adaptive_factors)
                                
                                if ml_factors_list:
                                    ml_factors = pd.concat(ml_factors_list, axis=1)
                                    all_factors = pd.concat([all_factors, ml_factors], axis=1)
                                    print(f"âœ“ æˆåŠŸæ„å»ºMLå› å­: {len(ml_factors.columns)} ä¸ª")
                                    update_session_progress(session_id, 'factor_building', progress_percent + 20, f'æœºå™¨å­¦ä¹ å› å­æ„å»ºå®Œæˆ: {len(ml_factors.columns)} ä¸ª')
                                else:
                                    print("âœ— MLå› å­æ„å»ºå®Œæˆï¼Œä½†ç»“æœä¸ºç©º")
                                    update_session_progress(session_id, 'factor_building', progress_percent + 20, 'æœºå™¨å­¦ä¹ å› å­æ„å»ºå®Œæˆï¼Œä½†ç»“æœä¸ºç©º')
                                    
                            except Exception as ml_error:
                                print(f"âœ— MLå› å­æ„å»ºå¤±è´¥: {ml_error}")
                                update_session_progress(session_id, 'factor_building', progress_percent + 20, f'æœºå™¨å­¦ä¹ å› å­æ„å»ºå¤±è´¥: {str(ml_error)[:50]}...')
                                
                        elif factor_type == 'technical':
                            # æ„å»ºæŠ€æœ¯å› å­
                            update_session_progress(session_id, 'factor_building', progress_percent, f'æ­£åœ¨æ„å»ºæŠ€æœ¯å› å­...')
                            
                            technical_builder = get_technical_factor_builder()
                            
                            try:
                                # å…¼å®¹ä¸¤ç§æ¥å£ï¼šä¼˜å…ˆ calculate_all_factorsï¼Œå…¶æ¬¡ build_all_factors
                                if hasattr(technical_builder, 'calculate_all_factors'):
                                    technical_factors = technical_builder.calculate_all_factors(data_result['data'])
                                else:
                                    technical_factors = technical_builder.build_all_factors(data_result['data'])
                                
                                if not technical_factors.empty:
                                    all_factors = pd.concat([all_factors, technical_factors], axis=1)
                                    print(f"âœ“ æˆåŠŸæ„å»ºæŠ€æœ¯å› å­: {len(technical_factors.columns)} ä¸ª")
                                    update_session_progress(session_id, 'factor_building', progress_percent + 20, f'æŠ€æœ¯å› å­æ„å»ºå®Œæˆ: {len(technical_factors.columns)} ä¸ª')
                                else:
                                    print("âœ— æŠ€æœ¯å› å­æ„å»ºå®Œæˆï¼Œä½†ç»“æœä¸ºç©º")
                                    update_session_progress(session_id, 'factor_building', progress_percent + 20, 'æŠ€æœ¯å› å­æ„å»ºå®Œæˆï¼Œä½†ç»“æœä¸ºç©º')
                                    
                            except Exception as e:
                                print(f"âœ— æŠ€æœ¯å› å­æ„å»ºå¤±è´¥: {e}")
                                update_session_progress(session_id, 'factor_building', progress_percent + 20, f'æŠ€æœ¯å› å­æ„å»ºå¤±è´¥: {str(e)[:50]}...')
                                
                        elif factor_type == 'statistical':
                            # æ„å»ºç»Ÿè®¡å› å­
                            update_session_progress(session_id, 'factor_building', progress_percent, f'æ­£åœ¨æ„å»ºç»Ÿè®¡å› å­...')
                            
                            statistical_builder = get_statistical_factor_builder()
                            
                            try:
                                if hasattr(statistical_builder, 'calculate_all_factors'):
                                    statistical_df = statistical_builder.calculate_all_factors(data_result['data'])
                                else:
                                    statistical_df = statistical_builder.build_all_factors(data_result['data'])
                                
                                if not statistical_df.empty:
                                    all_factors = pd.concat([all_factors, statistical_df], axis=1)
                                    print(f"âœ“ æˆåŠŸæ„å»ºç»Ÿè®¡å› å­: {len(statistical_df.columns)} ä¸ª")
                                    update_session_progress(session_id, 'factor_building', progress_percent + 20, f'ç»Ÿè®¡å› å­æ„å»ºå®Œæˆ: {len(statistical_df.columns)} ä¸ª')
                                else:
                                    print("âœ— ç»Ÿè®¡å› å­æ„å»ºå®Œæˆï¼Œä½†ç»“æœä¸ºç©º")
                                    update_session_progress(session_id, 'factor_building', progress_percent + 20, 'ç»Ÿè®¡å› å­æ„å»ºå®Œæˆï¼Œä½†ç»“æœä¸ºç©º')
                                    
                            except Exception as e:
                                print(f"âœ— ç»Ÿè®¡å› å­æ„å»ºå¤±è´¥: {e}")
                                update_session_progress(session_id, 'factor_building', progress_percent + 20, f'ç»Ÿè®¡å› å­æ„å»ºå¤±è´¥: {str(e)[:50]}...')
                                
                        elif factor_type == 'advanced':
                            # æ„å»ºé«˜çº§å› å­
                            update_session_progress(session_id, 'factor_building', progress_percent, f'æ­£åœ¨æ„å»ºé«˜çº§å› å­...')
                            
                            advanced_builder = get_advanced_factor_builder()
                            
                            try:
                                if hasattr(advanced_builder, 'calculate_all_factors'):
                                    advanced_factors = advanced_builder.calculate_all_factors(data_result['data'])
                                else:
                                    advanced_factors = advanced_builder.build_all_factors(data_result['data'])
                                
                                if not advanced_factors.empty:
                                    all_factors = pd.concat([all_factors, advanced_factors], axis=1)
                                    print(f"âœ“ æˆåŠŸæ„å»ºé«˜çº§å› å­: {len(advanced_factors.columns)} ä¸ª")
                                    update_session_progress(session_id, 'factor_building', progress_percent + 20, f'é«˜çº§å› å­æ„å»ºå®Œæˆ: {len(advanced_factors.columns)} ä¸ª')
                                else:
                                    print("âœ— é«˜çº§å› å­æ„å»ºå®Œæˆï¼Œä½†ç»“æœä¸ºç©º")
                                    update_session_progress(session_id, 'factor_building', progress_percent + 20, 'é«˜çº§å› å­æ„å»ºå®Œæˆï¼Œä½†ç»“æœä¸ºç©º')
                                    
                            except Exception as e:
                                print(f"âœ— é«˜çº§å› å­æ„å»ºå¤±è´¥: {e}")
                                update_session_progress(session_id, 'factor_building', progress_percent + 20, f'é«˜çº§å› å­æ„å»ºå¤±è´¥: {str(e)[:50]}...')
                                
                        else:
                            # å…¶ä»–å› å­ç±»å‹
                            update_session_progress(session_id, 'factor_building', progress_percent, f'æ­£åœ¨æ„å»º{factor_type_name}...')
                            
                    except Exception as e:
                        print(f"âœ— {factor_type_name}æ„å»ºå¤±è´¥: {e}")
                        update_session_progress(session_id, 'factor_building', progress_percent, f'{factor_type_name}æ„å»ºå¤±è´¥: {str(e)[:50]}...')
                
                # å¦‚æœå› å­æ„å»ºæˆåŠŸï¼Œä½¿ç”¨æ„å»ºç»“æœï¼›å¦åˆ™ä½¿ç”¨åŸæœ‰API
                if not all_factors.empty:
                    # ä½¿ç”¨çœŸå®æ„å»ºçš„å› å­
                    factors_result = {
                        'success': True,
                        'factors': all_factors,
                        'info': {
                            'total_factors': len(all_factors.columns),
                            'factor_names': list(all_factors.columns),
                            'factor_types': factor_types
                        }
                    }
                else:
                    # ä½¿ç”¨åŸæœ‰APIæ„å»ºå› å­
                    factors_result = api.build_factors(
                        data_result['data'], 
                        data['factor_types'], 
                        mining_config
                    )
                
                if not factors_result['success']:
                    raise Exception(f"å› å­æ„å»ºå¤±è´¥: {factors_result['error']}")
                
                # æ•°æ®æ¸…ç†å’Œç´¢å¼•å¯¹é½
                print(f"å¼€å§‹æ¸…ç†å› å­æ•°æ®...")
                factors_df = factors_result['factors']
                print(f"åŸå§‹å› å­å½¢çŠ¶: {factors_df.shape}")
                print(f"åŸå§‹å› å­ç´¢å¼•èŒƒå›´: {factors_df.index.min()} åˆ° {factors_df.index.max()}")
                print(f"åŸå§‹å› å­ç´¢å¼•ç±»å‹: {type(factors_df.index)}")
                
                # æ£€æŸ¥å¹¶å¤„ç†é‡å¤ç´¢å¼•
                if factors_df.index.duplicated().any():
                    print(f"å‘ç°é‡å¤ç´¢å¼•ï¼Œå¼€å§‹æ¸…ç†...")
                    duplicate_count = factors_df.index.duplicated().sum()
                    print(f"é‡å¤ç´¢å¼•æ•°é‡: {duplicate_count}")
                    
                    # ä¿ç•™æœ€åä¸€ä¸ªé‡å¤å€¼
                    factors_df = factors_df[~factors_df.index.duplicated(keep='last')]
                    print(f"æ¸…ç†åå› å­å½¢çŠ¶: {factors_df.shape}")
                
                # ç¡®ä¿ç´¢å¼•æ˜¯datetimeç±»å‹
                if not isinstance(factors_df.index, pd.DatetimeIndex):
                    print(f"è½¬æ¢ç´¢å¼•ä¸ºdatetimeç±»å‹...")
                    factors_df.index = pd.to_datetime(factors_df.index)
                
                # ä¸å¸‚åœºæ•°æ®å¯¹é½
                market_data = data_result['data']
                print(f"å¸‚åœºæ•°æ®å½¢çŠ¶: {market_data.shape}")
                print(f"å¸‚åœºæ•°æ®ç´¢å¼•èŒƒå›´: {market_data.index.min()} åˆ° {market_data.index.max()}")
                
                # æ‰¾åˆ°å…±åŒçš„ç´¢å¼•
                common_index = factors_df.index.intersection(market_data.index)
                print(f"å…±åŒç´¢å¼•æ•°é‡: {len(common_index)}")
                
                if len(common_index) < 100:
                    raise Exception(f"å› å­æ•°æ®ä¸å¸‚åœºæ•°æ®å¯¹é½åæ ·æœ¬å¤ªå°‘: {len(common_index)} < 100")
                
                # å¯¹é½æ•°æ®
                factors_df_aligned = factors_df.loc[common_index]
                market_data_aligned = market_data.loc[common_index]
                
                print(f"å¯¹é½åå› å­å½¢çŠ¶: {factors_df_aligned.shape}")
                print(f"å¯¹é½åå¸‚åœºæ•°æ®å½¢çŠ¶: {market_data_aligned.shape}")
                
                # æ£€æŸ¥æ•°æ®è´¨é‡
                print(f"å› å­ç¼ºå¤±å€¼ç»Ÿè®¡:")
                for col in factors_df_aligned.columns:
                    missing_count = factors_df_aligned[col].isna().sum()
                    missing_ratio = missing_count / len(factors_df_aligned)
                    print(f"  {col}: {missing_count} ({missing_ratio:.2%})")
                
                # å¡«å……ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨å‰å‘å¡«å……ï¼‰
                factors_df_aligned = factors_df_aligned.fillna(method='ffill').fillna(0)
                print(f"ç¼ºå¤±å€¼å¡«å……å®Œæˆ")
                
                # æ›´æ–°æ•°æ®ç»“æœ
                data_result['data'] = market_data_aligned
                factors_result['factors'] = factors_df_aligned
                
                update_session_progress(session_id, 'factor_building', 100, f'å› å­æ„å»ºå®Œæˆï¼Œå…±{factors_df_aligned.shape[1]}ä¸ªå› å­')
                
                # æ­¥éª¤3: å› å­è¯„ä¼°
                step_start_time = time.time()
                mining_sessions[session_id]['progress_info']['factor_evaluation']['start_time'] = step_start_time
                mining_sessions[session_id]['progress_info']['factor_evaluation']['current_step_start'] = step_start_time
                
                update_session_progress(session_id, 'factor_evaluation', 0, 'å¼€å§‹è¯„ä¼°å› å­...')
                
                # å¼€å§‹å› å­è¯„ä¼°
                update_session_progress(session_id, 'factor_evaluation', 10, 'æ­£åœ¨è¯„ä¼°å› å­...')
                
                # æ‰§è¡ŒçœŸå®çš„å› å­è¯„ä¼°ï¼ˆä¼ å…¥å‰ç«¯å¯è§†åŒ–çš„è¿›åº¦å›è°ƒï¼‰
                evaluation_result = api.evaluate_factors(
                    factors_result['factors'], 
                    data_result['data'], 
                    mining_config
                )
                # åŸºäºæœ€å°é˜ˆå€¼ç­›é€‰ï¼ˆåªä¿ç•™ç¬¦åˆ min_ic/min_ir çš„å› å­ï¼‰
                try:
                    min_ic = float(mining_config.get('min_ic', 0)) if isinstance(mining_config, dict) else 0.0
                    min_ir = float(mining_config.get('min_ir', 0)) if isinstance(mining_config, dict) else 0.0
                    eval_map = evaluation_result.get('evaluation', {}) or {}
                    def pass_thresholds(res: dict) -> bool:
                        if not isinstance(res, dict):
                            return False
                        ic_candidates = [res.get('ic_pearson'), res.get('ic_spearman')]
                        ic_values = [abs(v) for v in ic_candidates if isinstance(v, (int, float))]
                        ic_ok = (max(ic_values) if ic_values else 0.0) >= min_ic
                        ir_value = res.get('ic_ir')
                        if not isinstance(ir_value, (int, float)):
                            ir_value = res.get('sharpe_ratio') if isinstance(res.get('sharpe_ratio'), (int, float)) else 0.0
                        ir_ok = ir_value >= min_ir
                        return ic_ok and ir_ok
                    selected_names = [name for name, res in eval_map.items() if pass_thresholds(res)]
                    if selected_names:
                        # è¿‡æ»¤å› å­ä¸è¯„ä¼°æ˜ å°„
                        factors_result['factors'] = factors_result['factors'][selected_names]
                        if 'info' in factors_result:
                            factors_result['info']['factor_names'] = selected_names
                            factors_result['info']['total_factors'] = len(selected_names)
                        evaluation_result['evaluation'] = {k: eval_map[k] for k in selected_names}
                        print(f"ç­›é€‰åä¿ç•™å› å­æ•°é‡: {len(selected_names)} (min_ic={min_ic}, min_ir={min_ir})")
                    else:
                        print(f"ç­›é€‰ç»“æœä¸ºç©ºï¼Œä¿ç•™å…¨éƒ¨å› å­ (min_ic={min_ic}, min_ir={min_ir})")
                except Exception as e:
                    print(f"ç­›é€‰å› å­å¤±è´¥: {e}")
                
                if not evaluation_result['success']:
                    raise Exception(f"å› å­è¯„ä¼°å¤±è´¥: {evaluation_result['error']}")
                
                update_session_progress(session_id, 'factor_evaluation', 100, 'å› å­è¯„ä¼°å®Œæˆ')
                
                # å°†è¯„ä¼°ç»“æœå­˜å‚¨åˆ°V3ç³»ç»Ÿ
                try:
                    update_session_progress(session_id, 'factor_evaluation', 95, 'æ­£åœ¨ä¿å­˜è¯„ä¼°ç»“æœåˆ°V3ç³»ç»Ÿ...')
                    api.save_evaluation_results_to_v3(
                        factors_result['factors'],
                        evaluation_result['evaluation'],
                        mining_config
                    )
                    update_session_progress(session_id, 'factor_evaluation', 100, 'è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°V3ç³»ç»Ÿ')
                except Exception as e:
                    print(f"ä¿å­˜è¯„ä¼°ç»“æœåˆ°V3ç³»ç»Ÿå¤±è´¥: {e}")
                    # ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ
                
                # æ­¥éª¤4: å› å­ä¼˜åŒ–
                step_start_time = time.time()
                mining_sessions[session_id]['progress_info']['factor_optimization']['start_time'] = step_start_time
                mining_sessions[session_id]['progress_info']['factor_optimization']['current_step_start'] = step_start_time
                
                update_session_progress(session_id, 'factor_optimization', 0, 'å¼€å§‹ä¼˜åŒ–å› å­...')
                
                # å¼€å§‹å› å­ä¼˜åŒ–
                update_session_progress(session_id, 'factor_optimization', 10, 'æ­£åœ¨ä¼˜åŒ–å› å­...')
                
                # æ‰§è¡ŒçœŸå®çš„å› å­ä¼˜åŒ–
                optimization_result = api.optimize_factor_combination(
                    factors_result['factors'], 
                    data_result['data'],
                    mining_config
                )
                
                if not optimization_result['success']:
                    raise Exception(f"å› å­ä¼˜åŒ–å¤±è´¥: {optimization_result['error']}")
                
                update_session_progress(session_id, 'factor_optimization', 100, 'å› å­ä¼˜åŒ–å®Œæˆ')
                
                # æ­¥éª¤5: ç»“æœä¿å­˜
                step_start_time = time.time()
                mining_sessions[session_id]['progress_info']['result_saving']['start_time'] = step_start_time
                mining_sessions[session_id]['progress_info']['result_saving']['current_step_start'] = step_start_time
                
                update_session_progress(session_id, 'result_saving', 0, 'å¼€å§‹ä¿å­˜ç»“æœ...')
                
                # ä¿å­˜ç»“æœ
                results = {
                    'success': True,
                    'session_id': session_id,
                    'data_info': data_result.get('info', {}),
                    'factors_info': factors_result.get('info', {}),
                    'evaluation': evaluation_result.get('evaluation', {}),
                    'optimization': optimization_result,
                    'output_path': '',
                    'report': f"æŒ–æ˜å®Œæˆï¼Œå…±ç”Ÿæˆ {factors_result.get('info', {}).get('total_factors', 0)} ä¸ªå› å­"
                }
                
                # ä¿å­˜å› å­å®šä¹‰åˆ°factorlib/definitionsæ–‡ä»¶å¤¹
                try:
                    print(f"å¼€å§‹ä¿å­˜å› å­å®šä¹‰åˆ°factorlib/definitions...")
                    from factor_miner.core.factor_builder import FactorBuilder
                    factor_builder = FactorBuilder()
                    
                    # æ„å»ºå› å­æ•°æ®å­—å…¸ï¼Œæ ¼å¼ä¸åŸæ¥çš„_save_factors_to_storageæ–¹æ³•ä¸€è‡´
                    built_factors = {}
                    for factor_type in data.get('factor_types', []):
                        if factor_type == 'ml' and 'ml_factors' in locals():
                            built_factors['ml'] = {col: ml_factors[col] for col in ml_factors.columns}
                        elif factor_type == 'technical' and 'technical_factors' in locals():
                            built_factors['technical'] = {col: technical_factors[col] for col in technical_factors.columns}
                        elif factor_type == 'statistical' and 'statistical_df' in locals():
                            built_factors['statistical'] = {col: statistical_df[col] for col in statistical_df.columns}
                        elif factor_type == 'advanced' and 'advanced_factors' in locals():
                            built_factors['advanced'] = {col: advanced_factors[col] for col in advanced_factors.columns}
                    
                    # è°ƒç”¨ä¿å­˜æ–¹æ³•
                    if built_factors:
                        factor_builder._save_factors_to_storage(built_factors, data_result['data'])
                        print(f"å› å­å®šä¹‰ä¿å­˜æˆåŠŸåˆ°factorlib/definitions")
                    else:
                        print(f"æ²¡æœ‰æ‰¾åˆ°å¯ä¿å­˜çš„å› å­æ•°æ®")
                except Exception as e:
                    print(f"ä¿å­˜å› å­å®šä¹‰å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                try:
                    print(f"å¼€å§‹ä¿å­˜æŒ–æ˜ç»“æœåˆ°æ–‡ä»¶...")
                    output_path = save_mining_results(session_id, results, data)
                    results['output_path'] = str(output_path)
                    print(f"æŒ–æ˜ç»“æœä¿å­˜æˆåŠŸ: {output_path}")
                except Exception as e:
                    print(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")  # ä½¿ç”¨printè€Œä¸æ˜¯logger
                    import traceback
                    traceback.print_exc()
                
                update_session_progress(session_id, 'result_saving', 100, 'ç»“æœä¿å­˜å®Œæˆ')
                
                # æ›´æ–°ä¼šè¯çŠ¶æ€
                mining_sessions[session_id]['status'] = 'completed'
                mining_sessions[session_id]['results'] = results
                mining_sessions[session_id]['end_time'] = datetime.now().isoformat()
                
                print(f"æŒ–æ˜ä¼šè¯ {session_id} å®Œæˆ")  # ä½¿ç”¨printè€Œä¸æ˜¯logger
                
            except Exception as e:
                print(f"æŒ–æ˜ä¼šè¯ {session_id} å¤±è´¥: {e}")  # ä½¿ç”¨printè€Œä¸æ˜¯logger
                
                # æ›´æ–°å¤±è´¥çŠ¶æ€çš„è¿›åº¦
                if 'factor_building' in mining_sessions[session_id]['progress']:
                    update_session_progress(session_id, 'factor_building', 0, f'å› å­æ„å»ºå¤±è´¥: {str(e)}')
                elif 'factor_evaluation' in mining_sessions[session_id]['progress']:
                    update_session_progress(session_id, 'factor_evaluation', 0, f'å› å­è¯„ä¼°å¤±è´¥: {str(e)}')
                elif 'factor_optimization' in mining_sessions[session_id]['progress']:
                    update_session_progress(session_id, 'factor_optimization', 0, f'å› å­ä¼˜åŒ–å¤±è´¥: {str(e)}')
                
                mining_sessions[session_id]['status'] = 'error'
                mining_sessions[session_id]['error'] = str(e)
                mining_sessions[session_id]['end_time'] = datetime.now().isoformat()
        
        # å¯åŠ¨åå°çº¿ç¨‹
        thread = threading.Thread(target=run_mining_background)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'session_id': session_id,
            'message': 'å› å­æŒ–æ˜å·²å¯åŠ¨ï¼Œè¯·ä½¿ç”¨session_idæŸ¥è¯¢è¿›åº¦',
            'estimated_time': total_estimated_time,
            'system_info': system_info
        })
        
    except Exception as e:
        print(f"å¯åŠ¨æŒ–æ˜å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'å¯åŠ¨æŒ–æ˜å¤±è´¥: {str(e)}'
        }), 500

@bp.route('/ml_mining', methods=['POST'])
def start_ml_mining():
    """å¯åŠ¨ä¸“é—¨çš„MLå› å­æŒ–æ˜"""
    try:
        data = request.get_json()
        
        # éªŒè¯å¿…è¦å‚æ•°
        required_fields = ['symbols', 'timeframes', 'start_date', 'end_date']
        for field in required_fields:
            if field not in data:
                return jsonify({
                    'success': False,
                    'error': f'ç¼ºå°‘å¿…è¦å‚æ•°: {field}'
                }), 400
        
        # å¼ºåˆ¶è®¾ç½®ä¸ºMLå› å­ç±»å‹
        data['factor_types'] = ['ml']
        
        # åˆ›å»ºæŒ–æ˜ä¼šè¯
        session_id = str(uuid.uuid4())
        
        # è·å–ç³»ç»Ÿä¿¡æ¯
        system_info = get_system_info()
        
        # åˆå§‹åŒ–è¿›åº¦ä¿¡æ¯ - MLæŒ–æ˜éœ€è¦æ›´å¤šæ—¶é—´
        progress_info = {}
        total_estimated_time = 0
        
        for step_name in ['data_loading', 'factor_building', 'factor_evaluation', 'factor_optimization', 'result_saving']:
            estimated_time, progress_steps = estimate_step_time(step_name, data)
            progress_info[step_name] = {
                'estimated_time': estimated_time,
                'progress_steps': progress_steps,
                'current_progress': 0,
                'start_time': None,
                'current_step_start': None
            }
            total_estimated_time += estimated_time
        
        mining_sessions[session_id] = {
            'status': 'pending',
            'start_time': datetime.now().isoformat(),
            'config': data,
            'progress': {
                'data_loading': 0,
                'factor_building': 0,
                'factor_evaluation': 0,
                'factor_optimization': 0,
                'result_saving': 0
            },
            'progress_info': progress_info,
            'total_estimated_time': total_estimated_time,
            'current_step': 'data_loading',
            'messages': [],
            'system_info': system_info,
            'mining_type': 'ml'  # æ ‡è®°ä¸ºMLæŒ–æ˜
        }
        
        # æ„å»ºMLæŒ–æ˜é…ç½®
        ml_config = {
            'factor_types': ['ml'],
            'ml_params': {
                'window': data.get('window', 252),  # æ»šåŠ¨çª—å£
                'n_components': data.get('n_components', 10),  # PCAç»„ä»¶æ•°
                'k_best': data.get('k_best', 20),  # ç‰¹å¾é€‰æ‹©æ•°é‡
                'ensemble_models': data.get('ensemble_models', ['random_forest', 'gradient_boosting', 'ridge', 'lasso']),
                'rolling_window': data.get('rolling_window', 252),
                'adaptive_threshold': data.get('adaptive_threshold', 0.8)
            },
            'optimization': {
                'method': data.get('optimization_method', 'greedy'),
                'max_factors': data.get('max_factors', 20),  # MLå› å­é€šå¸¸æ›´å¤š
                'min_ic': data.get('min_ic', 0.015),  # MLå› å­ICé˜ˆå€¼ç¨ä½
                'min_ir': data.get('min_ir', 0.08)
            },
            'evaluation': {
                'min_sample_size': data.get('min_sample_size', 50),  # MLéœ€è¦æ›´å¤šæ ·æœ¬
                'metrics': ['ic_pearson', 'ic_spearman', 'sharpe_ratio', 'win_rate', 'factor_decay', 'stability']
            }
        }
        
        # åœ¨åå°å¯åŠ¨MLæŒ–æ˜
        def run_ml_mining_background():
            try:
                api = get_mining_api()
                ml_builder = get_ml_factor_builder()
                
                # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
                mining_sessions[session_id]['status'] = 'running'
                mining_sessions[session_id]['current_step'] = 'data_loading'
                
                # æ­¥éª¤1: æ•°æ®åŠ è½½
                step_start_time = time.time()
                mining_sessions[session_id]['progress_info']['data_loading']['start_time'] = step_start_time
                mining_sessions[session_id]['progress_info']['data_loading']['current_step_start'] = step_start_time
                
                update_session_progress(session_id, 'data_loading', 0, 'å¼€å§‹åŠ è½½å¸‚åœºæ•°æ®...')
                
                # è·å–æ•°æ®å¤§å°ä¿¡æ¯
                data_info = get_data_info(data['symbols'][0], data['timeframes'][0], data['start_date'], data['end_date'])
                
                data_result = api.load_data(
                    data['symbols'][0], 
                    data['timeframes'][0], 
                    data['start_date'], 
                    data['end_date']
                )
                
                if not data_result['success']:
                    raise Exception(f"æ•°æ®åŠ è½½å¤±è´¥: {data_result['error']}")
                
                # æ›´æ–°æ•°æ®ä¿¡æ¯
                if 'data_info' not in mining_sessions[session_id]:
                    mining_sessions[session_id]['data_info'] = {}
                mining_sessions[session_id]['data_info'].update(data_info)
                
                update_session_progress(session_id, 'data_loading', 100, 'æ•°æ®åŠ è½½å®Œæˆ')
                
                # æ­¥éª¤2: MLå› å­æ„å»º
                step_start_time = time.time()
                mining_sessions[session_id]['progress_info']['factor_building']['start_time'] = step_start_time
                mining_sessions[session_id]['progress_info']['factor_building']['current_step_start'] = step_start_time
                
                update_session_progress(session_id, 'factor_building', 0, 'å¼€å§‹æ„å»ºMLå› å­...')
                
                # æ„å»ºMLå› å­ï¼Œå®æ—¶å›ä¼ å­è¿›åº¦ï¼ˆä¾›å‰ç«¯æ˜¾ç¤ºå­è¿›åº¦æ¡ï¼‰
                def ml_progress_callback(stage: str, progress: int, message: str = ""):
                    try:
                        pi = mining_sessions[session_id].setdefault('progress_info', {})
                        sp = pi.setdefault('sub_progress', {})
                        sm = pi.setdefault('sub_messages', {})
                        sp[stage] = int(progress)
                        print(f"ğŸ¯ MLè¿›åº¦å›è°ƒ: {stage} -> {progress}%")
                        if message:
                            sm[stage] = message
                        # åŒæ­¥æ¨è¿›å› å­æ„å»ºä¸»è¿›åº¦ï¼ˆæ˜ å°„åˆ°10%~90%åŒºé—´ï¼‰
                        mapped = 10 + int(max(0, min(100, progress)) * 0.8)
                        update_session_progress(session_id, 'factor_building', mapped, message or 'MLå­è¿›åº¦æ›´æ–°')
                    except Exception as e:
                        print(f"âŒ MLè¿›åº¦å›è°ƒé”™è¯¯: {e}")
                        pass

                ml_factors = ml_builder.build_all_ml_factors(
                    data_result['data'],
                    window=ml_config['ml_params']['window'],
                    n_components=ml_config['ml_params']['n_components'],
                    k_best=ml_config['ml_params']['k_best'],
                    progress_callback=ml_progress_callback
                )
                
                if ml_factors.empty:
                    raise Exception("MLå› å­æ„å»ºå¤±è´¥ï¼Œç»“æœä¸ºç©º")
                
                # æ•°æ®æ¸…ç†å’Œç´¢å¼•å¯¹é½
                print(f"å¼€å§‹æ¸…ç†MLå› å­æ•°æ®...")
                print(f"åŸå§‹MLå› å­å½¢çŠ¶: {ml_factors.shape}")
                print(f"åŸå§‹MLå› å­ç´¢å¼•èŒƒå›´: {ml_factors.index.min()} åˆ° {ml_factors.index.max()}")
                print(f"åŸå§‹MLå› å­ç´¢å¼•ç±»å‹: {type(ml_factors.index)}")
                
                # æ£€æŸ¥å¹¶å¤„ç†é‡å¤ç´¢å¼•
                if ml_factors.index.duplicated().any():
                    print(f"å‘ç°é‡å¤ç´¢å¼•ï¼Œå¼€å§‹æ¸…ç†...")
                    duplicate_count = ml_factors.index.duplicated().sum()
                    print(f"é‡å¤ç´¢å¼•æ•°é‡: {duplicate_count}")
                    
                    # ä¿ç•™æœ€åä¸€ä¸ªé‡å¤å€¼
                    ml_factors = ml_factors[~ml_factors.index.duplicated(keep='last')]
                    print(f"æ¸…ç†åMLå› å­å½¢çŠ¶: {ml_factors.shape}")
                
                # ç¡®ä¿ç´¢å¼•æ˜¯datetimeç±»å‹
                if not isinstance(ml_factors.index, pd.DatetimeIndex):
                    print(f"è½¬æ¢ç´¢å¼•ä¸ºdatetimeç±»å‹...")
                    ml_factors.index = pd.to_datetime(ml_factors.index)
                
                # ä¸å¸‚åœºæ•°æ®å¯¹é½
                market_data = data_result['data']
                print(f"å¸‚åœºæ•°æ®å½¢çŠ¶: {market_data.shape}")
                print(f"å¸‚åœºæ•°æ®ç´¢å¼•èŒƒå›´: {market_data.index.min()} åˆ° {market_data.index.max()}")
                
                # æ‰¾åˆ°å…±åŒçš„ç´¢å¼•
                common_index = ml_factors.index.intersection(market_data.index)
                print(f"å…±åŒç´¢å¼•æ•°é‡: {len(common_index)}")
                
                if len(common_index) < 100:
                    raise Exception(f"å› å­æ•°æ®ä¸å¸‚åœºæ•°æ®å¯¹é½åæ ·æœ¬å¤ªå°‘: {len(common_index)} < 100")
                
                # å¯¹é½æ•°æ®
                ml_factors_aligned = ml_factors.loc[common_index]
                market_data_aligned = market_data.loc[common_index]
                
                print(f"å¯¹é½åMLå› å­å½¢çŠ¶: {ml_factors_aligned.shape}")
                print(f"å¯¹é½åå¸‚åœºæ•°æ®å½¢çŠ¶: {market_data_aligned.shape}")
                
                # æ£€æŸ¥æ•°æ®è´¨é‡
                print(f"MLå› å­ç¼ºå¤±å€¼ç»Ÿè®¡:")
                for col in ml_factors_aligned.columns:
                    missing_count = ml_factors_aligned[col].isna().sum()
                    missing_ratio = missing_count / len(ml_factors_aligned)
                    print(f"  {col}: {missing_count} ({missing_ratio:.2%})")
                
                # å¡«å……ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨å‰å‘å¡«å……ï¼‰
                ml_factors_aligned = ml_factors_aligned.fillna(method='ffill').fillna(0)
                print(f"ç¼ºå¤±å€¼å¡«å……å®Œæˆ")
                
                # æ›´æ–°æ•°æ®ç»“æœ
                data_result['data'] = market_data_aligned
                
                factors_result = {
                    'success': True,
                    'factors': ml_factors_aligned,
                    'info': {
                        'total_factors': len(ml_factors_aligned.columns),
                        'factor_names': list(ml_factors_aligned.columns),
                        'factor_types': ['ml'],
                        'ml_params': ml_config['ml_params'],
                        'data_alignment': {
                            'original_shape': ml_factors.shape,
                            'aligned_shape': ml_factors_aligned.shape,
                            'common_index_count': len(common_index),
                            'missing_values_filled': True
                        }
                    }
                }
                
                update_session_progress(session_id, 'factor_building', 100, f'MLå› å­æ„å»ºå®Œæˆï¼Œå…±{len(ml_factors.columns)}ä¸ªå› å­')
                
                # æ­¥éª¤3: å› å­è¯„ä¼°
                step_start_time = time.time()
                mining_sessions[session_id]['progress_info']['factor_evaluation']['start_time'] = step_start_time
                mining_sessions[session_id]['progress_info']['factor_evaluation']['current_step_start'] = step_start_time
                
                update_session_progress(session_id, 'factor_evaluation', 0, 'å¼€å§‹è¯„ä¼°MLå› å­...')
                
                # æ‰§è¡ŒMLå› å­è¯„ä¼°
                evaluation_result = api.evaluate_factors(
                    factors_result['factors'], 
                    data_result['data'], 
                    ml_config
                )
                
                if not evaluation_result['success']:
                    raise Exception(f"MLå› å­è¯„ä¼°å¤±è´¥: {evaluation_result['error']}")
                
                update_session_progress(session_id, 'factor_evaluation', 100, 'MLå› å­è¯„ä¼°å®Œæˆ')
                
                # æ­¥éª¤4: å› å­ä¼˜åŒ–
                step_start_time = time.time()
                mining_sessions[session_id]['progress_info']['factor_optimization']['start_time'] = step_start_time
                mining_sessions[session_id]['progress_info']['factor_optimization']['current_step_start'] = step_start_time
                
                update_session_progress(session_id, 'factor_optimization', 0, 'å¼€å§‹ä¼˜åŒ–MLå› å­ç»„åˆ...')
                
                # æ‰§è¡ŒMLå› å­ä¼˜åŒ–
                optimization_result = api.optimize_factor_combination(
                    factors_result['factors'], 
                    data_result['data'],
                    ml_config
                )
                
                if not optimization_result['success']:
                    raise Exception(f"MLå› å­ä¼˜åŒ–å¤±è´¥: {optimization_result['error']}")
                
                update_session_progress(session_id, 'factor_optimization', 100, 'MLå› å­ä¼˜åŒ–å®Œæˆ')
                
                # æ­¥éª¤5: ç»“æœä¿å­˜
                step_start_time = time.time()
                mining_sessions[session_id]['progress_info']['result_saving']['start_time'] = step_start_time
                mining_sessions[session_id]['progress_info']['result_saving']['current_step_start'] = step_start_time
                
                update_session_progress(session_id, 'result_saving', 0, 'å¼€å§‹ä¿å­˜MLæŒ–æ˜ç»“æœ...')
                
                # ä¿å­˜ç»“æœ
                results = {
                    'success': True,
                    'session_id': session_id,
                    'mining_type': 'ml',
                    'data_info': data_result.get('info', {}),
                    'factors_info': factors_result.get('info', {}),
                    'evaluation': evaluation_result.get('evaluation', {}),
                    'optimization': optimization_result,
                    'ml_config': ml_config,
                    'output_path': '',
                    'report': f"MLå› å­æŒ–æ˜å®Œæˆï¼Œå…±ç”Ÿæˆ {factors_result.get('info', {}).get('total_factors', 0)} ä¸ªMLå› å­"
                }
                
                # ä¿å­˜æœ¬æ¬¡æŒ–æ˜å› å­åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œä¾›åç»­ç”¨æˆ·é€‰æ‹©ä¿å­˜ï¼ˆå…¼å®¹æ— pyarrowç¯å¢ƒï¼‰
                try:
                    temp_dir = Path("factorlib") / "temp"
                    temp_dir.mkdir(parents=True, exist_ok=True)
                    df_tmp = factors_result['factors'].reset_index()
                    factors_file = temp_dir / f"factors_{session_id}.feather"
                    try:
                        df_tmp.to_feather(factors_file)
                        results['factors_file'] = str(factors_file)
                        results['factors_format'] = 'feather'
                        print(f"ä¸´æ—¶å› å­æ–‡ä»¶å·²ä¿å­˜(feather): {factors_file}")
                    except Exception as e_feather:
                        print(f"ä¿å­˜featherå¤±è´¥ï¼Œæ”¹ç”¨pickle: {e_feather}")
                        factors_file = temp_dir / f"factors_{session_id}.pkl"
                        df_tmp.to_pickle(factors_file)
                        results['factors_file'] = str(factors_file)
                        results['factors_format'] = 'pickle'
                        print(f"ä¸´æ—¶å› å­æ–‡ä»¶å·²ä¿å­˜(pickle): {factors_file}")
                except Exception as e:
                    print(f"ä¿å­˜ä¸´æ—¶å› å­æ–‡ä»¶å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()

                # å¯¹æ¯”æœ¬æ¬¡æŒ–æ˜å› å­ä¸åº“å†…å› å­ï¼Œä¾›ç”¨æˆ·å†³ç­–æ˜¯å¦ä¿å­˜
                try:
                    from factor_miner.core.factor_diff import compare_mined_factors_with_library
                    diff_report = compare_mined_factors_with_library(factors_result['factors'])
                    results['diff_report'] = diff_report
                    print(f"å› å­å¯¹æ¯”å®Œæˆï¼šæ–°å¢ {diff_report['summary']['new']}ï¼Œå·®å¼‚ {diff_report['summary']['different']}")
                except Exception as e:
                    print(f"å› å­å¯¹æ¯”å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                try:
                    print(f"å¼€å§‹ä¿å­˜MLæŒ–æ˜ç»“æœåˆ°æ–‡ä»¶...")
                    output_path = save_mining_results(session_id, results, data)
                    results['output_path'] = str(output_path)
                    print(f"MLæŒ–æ˜ç»“æœä¿å­˜æˆåŠŸ: {output_path}")
                except Exception as e:
                    print(f"ä¿å­˜MLç»“æœå¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                
                update_session_progress(session_id, 'result_saving', 100, 'MLæŒ–æ˜ç»“æœä¿å­˜å®Œæˆ')
                # æ ‡è®°å­é˜¶æ®µå®Œæˆï¼ˆä¾›å‰ç«¯å­è¿›åº¦ä½¿ç”¨ï¼‰
                mining_sessions[session_id].setdefault('progress_info', {}).setdefault('sub_progress', {})['ml'] = 100
                
                # æ›´æ–°ä¼šè¯çŠ¶æ€
                mining_sessions[session_id]['status'] = 'completed'
                mining_sessions[session_id]['results'] = results
                mining_sessions[session_id]['end_time'] = datetime.now().isoformat()
                
                print(f"MLæŒ–æ˜ä¼šè¯ {session_id} å®Œæˆ")
                
            except Exception as e:
                print(f"MLæŒ–æ˜ä¼šè¯ {session_id} å¤±è´¥: {e}")
                
                # æ›´æ–°å¤±è´¥çŠ¶æ€çš„è¿›åº¦
                if 'factor_building' in mining_sessions[session_id]['progress']:
                    update_session_progress(session_id, 'factor_building', 0, f'MLå› å­æ„å»ºå¤±è´¥: {str(e)}')
                elif 'factor_evaluation' in mining_sessions[session_id]['progress']:
                    update_session_progress(session_id, 'factor_evaluation', 0, f'MLå› å­è¯„ä¼°å¤±è´¥: {str(e)}')
                elif 'factor_optimization' in mining_sessions[session_id]['progress']:
                    update_session_progress(session_id, 'factor_optimization', 0, f'MLå› å­ä¼˜åŒ–å¤±è´¥: {str(e)}')
                
                mining_sessions[session_id]['status'] = 'error'
                mining_sessions[session_id]['error'] = str(e)
                mining_sessions[session_id]['end_time'] = datetime.now().isoformat()
        
        # å¯åŠ¨åå°çº¿ç¨‹
        thread = threading.Thread(target=run_ml_mining_background)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'session_id': session_id,
            'message': 'MLå› å­æŒ–æ˜å·²å¯åŠ¨ï¼Œè¯·ä½¿ç”¨session_idæŸ¥è¯¢è¿›åº¦',
            'estimated_time': total_estimated_time,
            'system_info': system_info,
            'mining_type': 'ml'
        })
        
    except Exception as e:
        print(f"å¯åŠ¨MLæŒ–æ˜å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'å¯åŠ¨MLæŒ–æ˜å¤±è´¥: {str(e)}'
        }), 500

def get_data_info(symbol, timeframe, start_date, end_date):
    """è·å–æ•°æ®ä¿¡æ¯ç”¨äºè¿›åº¦ä¼°ç®—"""
    try:
        # æ„å»ºæ•°æ®æ–‡ä»¶è·¯å¾„
        data_dir = Path("data/binance/futures")
        if not data_dir.exists():
            return {'data_size_mb': 100, 'record_count': 1000}  # é»˜è®¤å€¼
        
        # æŸ¥æ‰¾å¯¹åº”çš„æ•°æ®æ–‡ä»¶
        data_file = data_dir / f"{symbol}_USDT_USDT-{timeframe}-futures.feather"
        if data_file.exists():
            # è·å–æ–‡ä»¶å¤§å°
            file_size = data_file.stat().st_size
            data_size_mb = file_size / (1024 * 1024)
            
            # ä¼°ç®—è®°å½•æ•°ï¼ˆåŸºäºæ–‡ä»¶å¤§å°ï¼‰
            record_count = int(data_size_mb * 100)  # ç²—ç•¥ä¼°ç®—
            
            return {
                'data_size_mb': round(data_size_mb, 2),
                'record_count': record_count,
                'file_path': str(data_file)
            }
        else:
            return {'data_size_mb': 100, 'record_count': 1000}  # é»˜è®¤å€¼
            
    except Exception as e:
        print(f"è·å–æ•°æ®ä¿¡æ¯å¤±è´¥: {e}")
        return {'data_size_mb': 100, 'record_count': 1000}  # é»˜è®¤å€¼

@bp.route('/status/<session_id>', methods=['GET'])
def get_mining_status(session_id):
    """è·å–æŒ–æ˜çŠ¶æ€å’Œè¿›åº¦"""
    if session_id not in mining_sessions:
        return jsonify({
            'success': False,
            'error': 'æŒ–æ˜ä¼šè¯ä¸å­˜åœ¨'
        }), 404
    
    session = mining_sessions[session_id]
    
    # è®¡ç®—æ—¶é—´ä¿¡æ¯
    time_info = calculate_time_info(session)
    
    return jsonify({
        'success': True,
        'session_id': session_id,
        'status': session['status'],
        'progress': session['progress'],
        'current_step': session['current_step'],
        'messages': session.get('messages', []),
        'start_time': session['start_time'],
        'end_time': session.get('end_time'),
        'error': session.get('error'),
        'time_info': time_info,
        'progress_info': session.get('progress_info', {}),
        'system_info': session.get('system_info', {})
    })

def calculate_time_info(session):
    """è®¡ç®—æ—¶é—´ä¿¡æ¯"""
    try:
        current_time = time.time()
        start_time = datetime.fromisoformat(session['start_time']).timestamp()
        elapsed_time = current_time - start_time
        
        # è·å–å½“å‰æ­¥éª¤ä¿¡æ¯
        current_step = session.get('current_step', 'data_loading')
        progress_info = session.get('progress_info', {})
        
        if current_step not in progress_info:
            return {
                'elapsed_time': round(elapsed_time, 1),
                'estimated_remaining': 0,
                'estimated_total': session.get('total_estimated_time', 0),
                'current_step_progress': 0,
                'current_step_elapsed': 0,
                'current_step_remaining': 0
            }
        
        current_step_info = progress_info[current_step]
        current_step_start = current_step_info.get('current_step_start')
        
        if current_step_start:
            current_step_elapsed = current_time - current_step_start
            current_step_estimated = current_step_info.get('estimated_time', 10)
            
            # è®¡ç®—å½“å‰æ­¥éª¤çš„å‰©ä½™æ—¶é—´
            if current_step_elapsed < current_step_estimated:
                current_step_remaining = current_step_estimated - current_step_elapsed
            else:
                current_step_remaining = 0
            
            # è®¡ç®—æ€»ä½“å‰©ä½™æ—¶é—´
            total_estimated = session.get('total_estimated_time', 0)
            if total_estimated > elapsed_time:
                estimated_remaining = total_estimated - elapsed_time
            else:
                estimated_remaining = 0
            
            return {
                'elapsed_time': round(elapsed_time, 1),
                'estimated_remaining': round(estimated_remaining, 1),
                'estimated_total': total_estimated,
                'current_step_progress': session['progress'].get(current_step, 0),
                'current_step_elapsed': round(current_step_elapsed, 1),
                'current_step_remaining': round(current_step_remaining, 1),
                'current_step_estimated': current_step_estimated
            }
        else:
            return {
                'elapsed_time': round(elapsed_time, 1),
                'estimated_remaining': session.get('total_estimated_time', 0),
                'estimated_total': session.get('total_estimated_time', 0),
                'current_step_progress': 0,
                'current_step_elapsed': 0,
                'current_step_remaining': 0
            }
            
    except Exception as e:
        print(f"è®¡ç®—æ—¶é—´ä¿¡æ¯å¤±è´¥: {e}")
        return {
            'elapsed_time': 0,
            'estimated_remaining': 0,
            'estimated_total': 0,
            'current_step_progress': 0,
            'current_step_elapsed': 0,
            'current_step_remaining': 0
        }

@bp.route('/progress/<session_id>', methods=['GET'])
def get_mining_progress(session_id):
    """è·å–æŒ–æ˜è¿›åº¦ï¼ˆSSEæµï¼‰"""
    if session_id not in mining_sessions:
        return jsonify({
            'success': False,
            'error': 'æŒ–æ˜ä¼šè¯ä¸å­˜åœ¨'
        }), 404
    
    def generate_progress():
        session = mining_sessions[session_id]
        last_progress = None
        last_time_info = None
        last_sub_progress = None
        
        while session['status'] in ['pending', 'running']:
            current_progress = session['progress']
            current_time_info = calculate_time_info(session)
            current_sub_progress = session.get('progress_info', {}).get('sub_progress', {})
            
            # æ£€æŸ¥è¿›åº¦æˆ–æ—¶é—´ä¿¡æ¯æ˜¯å¦æœ‰å˜åŒ–
            progress_changed = current_progress != last_progress
            time_changed = current_time_info != last_time_info
            sub_changed = current_sub_progress != last_sub_progress
            
            if progress_changed or time_changed or sub_changed:
                data = {
                    'session_id': session_id,
                    'status': session['status'],
                    'progress': current_progress,
                    'current_step': session['current_step'],
                    'messages': session.get('messages', []),
                    'time_info': current_time_info,
                    'progress_info': session.get('progress_info', {}),
                    'system_info': session.get('system_info', {})
                }
                # ç¡®ä¿sub_progresså’Œsub_messagesç»“æ„å­˜åœ¨ï¼Œä½†ä¸å¼ºåˆ¶å›å¡«ä¸»è¿›åº¦
                try:
                    pi = data.setdefault('progress_info', {})
                    pi.setdefault('sub_progress', {})
                    pi.setdefault('sub_messages', {})
                except Exception:
                    pass
                
                yield f"data: {json.dumps(data)}\n\n"
                last_progress = current_progress.copy()
                last_time_info = current_time_info.copy()
                last_sub_progress = current_sub_progress.copy() if isinstance(current_sub_progress, dict) else current_sub_progress
            
            time.sleep(0.5)  # æ›´é¢‘ç¹çš„æ›´æ–°ï¼ˆæ¯0.5ç§’ï¼‰
        
        # å‘é€æœ€ç»ˆçŠ¶æ€
        final_data = {
            'session_id': session_id,
            'status': session['status'],
            'progress': session['progress'],
            'current_step': session['current_step'],
            'messages': session.get('messages', []),
            'results': session.get('results'),
            'error': session.get('error'),
            'time_info': calculate_time_info(session),
            'progress_info': session.get('progress_info', {}),
            'system_info': session.get('system_info', {})
        }
        
        yield f"data: {json.dumps(final_data)}\n\n"
    
    return Response(generate_progress(), mimetype='text/event-stream')

@bp.route('/diff/<session_id>', methods=['GET'])
def get_mining_diff(session_id: str):
    """è·å–æŸæ¬¡æŒ–æ˜ä¸åº“å†…å› å­çš„å¯¹æ¯”æŠ¥å‘Š"""
    try:
        if session_id not in mining_sessions:
            return jsonify({'success': False, 'message': 'ä¼šè¯ä¸å­˜åœ¨'}), 404
        results = mining_sessions[session_id].get('results') or {}
        diff_report = results.get('diff_report') or {}
        # å¦‚æœå†…å­˜ä¸­æ²¡æœ‰å¯¹æ¯”æŠ¥å‘Šï¼ŒåŸºäºä¸´æ—¶å› å­æ–‡ä»¶å³æ—¶ç”Ÿæˆä¸€æ¬¡
        if not diff_report or not diff_report.get('items'):
            factors_file = results.get('factors_file')
            factors_format = results.get('factors_format')
            try:
                if factors_file and Path(factors_file).exists():
                    import pandas as pd
                    if factors_format == 'pickle' or factors_file.endswith('.pkl'):
                        df = pd.read_pickle(factors_file).set_index('index')
                    else:
                        df = pd.read_feather(factors_file).set_index('index')
                    from factor_miner.core.factor_diff import compare_mined_factors_with_library
                    diff_report = compare_mined_factors_with_library(df)
                    # å›å†™åˆ°ä¼šè¯ï¼Œä¾¿äºåç»­è¯»å–
                    mining_sessions[session_id]['results']['diff_report'] = diff_report
                else:
                    # è¿›ä¸€æ­¥å…œåº•ï¼šè‹¥æ²¡æœ‰ä¸´æ—¶æ–‡ä»¶ï¼Œç”¨å› å­åç§°åˆ—è¡¨æ„é€ ç©ºDataFrameå¯¹æ¯”
                    factor_names = []
                    info = results.get('factors_info') or {}
                    if 'factor_names' in info:
                        factor_names = info.get('factor_names') or []
                    # è‹¥å†…å­˜ä¸­æ— ï¼Œåˆ™å°è¯•ä»å†å²æ–‡ä»¶åŠ è½½
                    if not factor_names:
                        try:
                            results_dir = Path("factorlib") / "mining_history"
                            result_file = results_dir / f"mining_results_{session_id}.json"
                            if result_file.exists():
                                import json as _json
                                with open(result_file, 'r', encoding='utf-8') as f:
                                    stored = _json.load(f)
                                fi = (stored or {}).get('factors_info') or {}
                                factor_names = fi.get('factor_names') or []
                        except Exception:
                            factor_names = []
                    if factor_names:
                        import pandas as pd
                        df = pd.DataFrame(index=pd.RangeIndex(0), data={name: [] for name in factor_names})
                        from factor_miner.core.factor_diff import compare_mined_factors_with_library
                        diff_report = compare_mined_factors_with_library(df)
                        mining_sessions[session_id]['results']['diff_report'] = diff_report
            except Exception as _:
                pass
        return jsonify({'success': True, 'session_id': session_id, 'diff_report': diff_report})
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)}), 500

@bp.route('/save_selected_factors', methods=['POST'])
def save_selected_factors():
    """æ ¹æ®å¯¹æ¯”ç»“æœé€‰æ‹©æ€§ä¿å­˜æŒ–æ˜å› å­å®šä¹‰ï¼ˆä¸è‡ªåŠ¨è¦†ç›–ï¼‰"""
    try:
        payload = request.get_json() or {}
        session_id = payload.get('session_id')
        selected = payload.get('factor_ids') or []
        if not session_id or not selected:
            return jsonify({'success': False, 'message': 'ç¼ºå°‘ session_id æˆ– factor_ids'}), 400

        if session_id not in mining_sessions:
            return jsonify({'success': False, 'message': 'ä¼šè¯ä¸å­˜åœ¨'}), 404

        session = mining_sessions[session_id]
        results = session.get('results') or {}
        factors_file = results.get('factors_file')
        if not factors_file or not Path(factors_file).exists():
            return jsonify({'success': False, 'message': 'ä¸´æ—¶å› å­æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°æŒ–æ˜'}), 400

        # åŠ è½½ä¸´æ—¶å› å­æ•°æ®
        df = pd.read_feather(factors_file).set_index('index')

        # è¿‡æ»¤å‡ºé€‰æ‹©çš„åˆ—
        missing = [c for c in selected if c not in df.columns]
        if missing:
            return jsonify({'success': False, 'message': f'æ‰€é€‰å› å­ä¸å­˜åœ¨: {missing}'}), 400

        subset = df[selected]

        # ä¿å­˜å®šä¹‰ï¼ˆè°ƒç”¨æ ¸å¿ƒæ„å»ºå™¨çš„ä¿å­˜é€»è¾‘ï¼Œä¼˜å…ˆml_modelï¼‰
        try:
            from factor_miner.core.factor_builder import FactorBuilder
            builder = FactorBuilder()
            built = {'ml': {col: subset[col] for col in subset.columns}}
            builder._save_factors_to_storage(built, subset)
        except Exception as e:
            return jsonify({'success': False, 'message': f'ä¿å­˜å®šä¹‰å¤±è´¥: {e}'}), 500

        # ä»…ä¿å­˜é€‰ä¸­å› å­çš„è¯„ä¼°ç»“æœï¼ˆå¦‚æœæœ¬æ¬¡ä¼šè¯æœ‰è¯„ä¼°æ•°æ®ï¼‰
        try:
            evaluation_map = (results.get('evaluation') or {})
            if evaluation_map:
                from factor_miner.core.evaluation_io import save_evaluation_results as core_save_eval
                for fid in selected:
                    if fid in evaluation_map:
                        core_save_eval(fid, evaluation_map[fid], {
                            'mining_session': True,
                            'session_id': session_id
                        })
        except Exception as e:
            return jsonify({'success': False, 'message': f'ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {e}'}), 500

        return jsonify({'success': True, 'saved_count': len(selected), 'saved_factor_ids': selected})
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)}), 500

@bp.route('/cancel/<session_id>', methods=['POST'])
def cancel_mining(session_id):
    """å–æ¶ˆæŒ–æ˜ä¼šè¯"""
    if session_id not in mining_sessions:
        return jsonify({
            'success': False,
            'error': 'æŒ–æ˜ä¼šè¯ä¸å­˜åœ¨'
        }), 404
    
    session = mining_sessions[session_id]
    if session['status'] in ['completed', 'error']:
        return jsonify({
            'success': False,
            'error': 'æŒ–æ˜ä¼šè¯å·²å®Œæˆï¼Œæ— æ³•å–æ¶ˆ'
        }), 400
    
    session['status'] = 'cancelled'
    session['end_time'] = datetime.now().isoformat()
    
    return jsonify({
        'success': True,
        'message': 'æŒ–æ˜ä¼šè¯å·²å–æ¶ˆ'
    })

@bp.route('/history', methods=['GET'])
def get_mining_history():
    """è·å–æŒ–æ˜å†å²"""
    try:
        print("=== å¼€å§‹è·å–æŒ–æ˜å†å² ===")
        
        # ä»å†…å­˜ä¸­è·å–æ´»è·ƒä¼šè¯
        active_sessions = []
        for session_id, session in mining_sessions.items():
            if session['status'] in ['pending', 'running']:
                active_sessions.append({
                    'session_id': session_id,
                    'timestamp': session['start_time'],
                    'status': session['status'],
                    'config': session['config'],
                    'progress': session['progress'],
                    'current_step': session['current_step']
                })
        
        print(f"å†…å­˜ä¸­çš„æ´»è·ƒä¼šè¯: {len(active_sessions)}")
        
        # ä»æ–‡ä»¶ç³»ç»ŸåŠ è½½å·²å®Œæˆçš„ä¼šè¯
        print("å¼€å§‹åŠ è½½å·²å®Œæˆçš„ä¼šè¯...")
        completed_sessions = load_completed_mining_sessions()
        print(f"ä»æ–‡ä»¶åŠ è½½çš„å·²å®Œæˆä¼šè¯: {len(completed_sessions)}")
        
        # åˆå¹¶æ‰€æœ‰ä¼šè¯
        all_sessions = active_sessions + completed_sessions
        print(f"æ€»ä¼šè¯æ•°: {len(all_sessions)}")
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        all_sessions.sort(key=lambda x: x['timestamp'], reverse=True)
        
        # æ‰“å°ç¬¬ä¸€ä¸ªä¼šè¯çš„é…ç½®ä¿¡æ¯ä½œä¸ºç¤ºä¾‹
        if all_sessions:
            first_session = all_sessions[0]
            print(f"ç¬¬ä¸€ä¸ªä¼šè¯é…ç½®ç¤ºä¾‹:")
            print(f"  session_id: {first_session.get('session_id')}")
            print(f"  config: {first_session.get('config')}")
            print(f"  configç±»å‹: {type(first_session.get('config'))}")
            print(f"  configé”®: {list(first_session.get('config', {}).keys()) if first_session.get('config') else 'None'}")
            print(f"  symbols: {first_session.get('config', {}).get('symbols', [])}")
            print(f"  symbolsç±»å‹: {type(first_session.get('config', {}).get('symbols', []))}")
            print(f"  timeframes: {first_session.get('config', {}).get('timeframes', [])}")
            print(f"  timeframesç±»å‹: {type(first_session.get('config', {}).get('timeframes', []))}")
            print(f"  factor_types: {first_session.get('config', {}).get('factor_types', [])}")
            print(f"  factor_typesç±»å‹: {type(first_session.get('config', {}).get('factor_types', []))}")
            
            # æ£€æŸ¥é…ç½®æ˜¯å¦è¢«ä¿®æ”¹
            if 'results' in first_session:
                print(f"  resultsä¸­çš„data_info: {first_session['results'].get('data_info', {})}")
                print(f"  resultsä¸­çš„factors_info: {first_session['results'].get('factors_info', {})}")
        
        return jsonify({
            'success': True,
            'sessions': all_sessions,
            'total': len(all_sessions),
            'active': len(active_sessions),
            'completed': len(completed_sessions)
        })
        
    except Exception as e:
        print(f"è·å–æŒ–æ˜å†å²å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'è·å–æŒ–æ˜å†å²å¤±è´¥: {str(e)}'
        }), 500

@bp.route('/result/<session_id>', methods=['GET'])
def get_mining_result(session_id):
    """è·å–æŒ–æ˜ç»“æœ"""
    try:
        # æ£€æŸ¥å†…å­˜ä¸­çš„ä¼šè¯
        if session_id in mining_sessions:
            session = mining_sessions[session_id]
            if session['status'] == 'completed' and 'results' in session:
                return jsonify(session['results'])
        
        # ä»æ–‡ä»¶åŠ è½½ç»“æœ
        try:
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            results_dir = Path("factorlib") / "mining_history"
            result_file = results_dir / f"mining_results_{session_id}.json"
            
            if not result_file.exists():
                return jsonify({
                    'success': False,
                    'error': f'æŒ–æ˜ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {result_file}'
                }), 404
            
            # ç›´æ¥è¯»å–æ–‡ä»¶
            with open(result_file, 'r', encoding='utf-8') as f:
                result = json.load(f)
            
            print(f"æˆåŠŸåŠ è½½æŒ–æ˜ç»“æœ: {result_file}")
            return jsonify(result)
            
        except Exception as e:
            print(f"ä»æ–‡ä»¶åŠ è½½æŒ–æ˜ç»“æœå¤±è´¥: {e}")
            return jsonify({
                'success': False,
                'error': f'åŠ è½½æŒ–æ˜ç»“æœå¤±è´¥: {str(e)}'
            }), 500
        
    except Exception as e:
        print(f"è·å–æŒ–æ˜ç»“æœå¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'è·å–æŒ–æ˜ç»“æœå¤±è´¥: {str(e)}'
        }), 500

@bp.route('/config', methods=['GET'])
def get_mining_config():
    """è·å–æŒ–æ˜é…ç½®é€‰é¡¹"""
    return jsonify({
        'factor_types': [
            {'value': 'technical', 'label': 'æŠ€æœ¯å› å­', 'description': 'åŸºäºä»·æ ¼å’Œæˆäº¤é‡çš„æŠ€æœ¯æŒ‡æ ‡'},
            {'value': 'statistical', 'label': 'ç»Ÿè®¡å› å­', 'description': 'åŸºäºç»Ÿè®¡å­¦çš„å› å­'},
            {'value': 'advanced', 'label': 'é«˜çº§å› å­', 'description': 'å¤æ‚çš„è¶‹åŠ¿å’ŒåŠ¨é‡å› å­'},
            {'value': 'ml', 'label': 'æœºå™¨å­¦ä¹ å› å­', 'description': 'ä½¿ç”¨MLç®—æ³•ç”Ÿæˆçš„å› å­'},
            {'value': 'crypto', 'label': 'åŠ å¯†å› å­', 'description': 'åŠ å¯†è´§å¸ç‰¹æœ‰å› å­'},
            {'value': 'pattern', 'label': 'å½¢æ€å› å­', 'description': 'ä»·æ ¼å½¢æ€è¯†åˆ«å› å­'},
            {'value': 'composite', 'label': 'å¤åˆå› å­', 'description': 'å¤šå› å­ç»„åˆ'},
            {'value': 'sentiment', 'label': 'æƒ…æ„Ÿå› å­', 'description': 'å¸‚åœºæƒ…ç»ªå› å­'}
        ],
        'optimization_methods': [
            {'value': 'greedy', 'label': 'è´ªå¿ƒç®—æ³•', 'description': 'å¿«é€Ÿé€‰æ‹©æœ€ä¼˜å› å­'},
            {'value': 'genetic', 'label': 'é—ä¼ ç®—æ³•', 'description': 'å…¨å±€ä¼˜åŒ–ï¼Œè€—æ—¶è¾ƒé•¿'},
            {'value': 'correlation', 'label': 'ç›¸å…³æ€§è¿‡æ»¤', 'description': 'å»é™¤é«˜ç›¸å…³æ€§å› å­'}
        ],
        'default_settings': {
            'max_factors': 15,
            'min_ic': 0.02,
            'min_ir': 0.1,
            'min_sample_size': 30
        }
    })

def update_session_progress(session_id, step, progress, message):
    """æ›´æ–°ä¼šè¯è¿›åº¦"""
    if session_id in mining_sessions:
        session = mining_sessions[session_id]
        session['progress'][step] = progress
        session['current_step'] = step
        
        if message:
            session['messages'].append({
                'timestamp': datetime.now().isoformat(),
                'step': step,
                'message': message,
                'progress': progress
            })
        
        # é™åˆ¶æ¶ˆæ¯æ•°é‡
        if len(session['messages']) > 100:
            session['messages'] = session['messages'][-100:]
        
        # åˆå§‹åŒ–å­è¿›åº¦ç»“æ„ï¼Œä½†ä¸å¼ºåˆ¶åŒæ­¥ä¸»è¿›åº¦åˆ°å­è¿›åº¦
        try:
            if step == 'factor_building':
                pi = session.setdefault('progress_info', {})
                sp = pi.setdefault('sub_progress', {})
                # ä»…åœ¨å­è¿›åº¦æœªåˆå§‹åŒ–æ—¶è®¾ç½®ä¸º0ï¼Œä¸å¼ºåˆ¶åŒæ­¥ä¸»è¿›åº¦
                sp.setdefault('ml', 0)
        except Exception:
            pass
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        print(f"ä¼šè¯ {session_id} æ­¥éª¤ {step} è¿›åº¦: {progress}% - {message}")

def load_completed_mining_sessions():
    """åŠ è½½å·²å®Œæˆçš„æŒ–æ˜ä¼šè¯"""
    try:
        print("  load_completed_mining_sessions: å¼€å§‹åŠ è½½")
        completed_sessions = []
        results_dir = Path("factorlib") / "mining_history"
        
        if not results_dir.exists():
            print(f"  load_completed_mining_sessions: ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
            return completed_sessions
        
        print(f"  load_completed_mining_sessions: ç»“æœç›®å½•å­˜åœ¨: {results_dir}")
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æŒ–æ˜ç»“æœæ–‡ä»¶
        result_files = []
        
        # 1. æŸ¥æ‰¾ mining_results_*.json æ–‡ä»¶
        factor_mining_dir = results_dir / "factor_mining"
        if factor_mining_dir.exists():
            result_files.extend(factor_mining_dir.glob("mining_results_*.json"))
        
        # 2. æŸ¥æ‰¾ factor_results.json æ–‡ä»¶
        factor_results_file = results_dir / "factor_results.json"
        if factor_results_file.exists():
            result_files.append(factor_results_file)
        
        # 3. æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æŒ–æ˜ç»“æœæ–‡ä»¶
        result_files.extend(results_dir.glob("*mining*.json"))
        result_files.extend(results_dir.glob("*factor*.json"))
        
        # å»é‡
        result_files = list(set(result_files))
        
        for result_file in result_files:
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)
                
                # å¤„ç†ä¸åŒçš„æ–‡ä»¶æ ¼å¼
                if 'factor_results' in result_data:
                    # factor_results.json æ ¼å¼
                    for factor in result_data['factor_results']:
                        session_id = factor.get('session_id', f"session_{factor.get('factor_id', 'unknown')}")
                        timestamp = factor.get('created_at', datetime.now().isoformat())
                        
                        completed_sessions.append({
                            'session_id': session_id,
                            'timestamp': timestamp,
                            'status': 'completed',
                            'config': {
                                'symbols': factor.get('data_requirements', {}).get('symbols', []),
                                'timeframes': factor.get('data_requirements', {}).get('timeframes', []),
                                'factor_types': [factor.get('category', 'unknown')]
                            },
                            'results': {
                                'total_factors': 1,
                                'selected_factors': [factor.get('name', 'Unknown Factor')],
                                'optimization_score': factor.get('performance', {}).get('ic', 0)
                            },
                            'factors_count': 1,
                            'start_time': timestamp,
                            'end_time': timestamp
                        })
                
                elif 'session_id' in result_data:
                    # mining_results_*.json æ ¼å¼
                    session_id = result_data.get('session_id', '')
                    if session_id:
                        # å°è¯•ä»æ–‡ä»¶ä¿®æ”¹æ—¶é—´è·å–æ—¶é—´æˆ³
                        try:
                            file_stat = result_file.stat()
                            timestamp = datetime.fromtimestamp(file_stat.st_mtime).isoformat()
                        except:
                            # å¦‚æœè·å–æ–‡ä»¶æ—¶é—´å¤±è´¥ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                            timestamp = datetime.now().isoformat()
                        
                        # æ„å»ºæ ‡å‡†åŒ–çš„é…ç½®ä¿¡æ¯
                        data_info = result_data.get('data_info', {})
                        factors_info = result_data.get('factors_info', {})
                        
                        config = {
                            'symbols': [data_info.get('symbol', '')] if data_info.get('symbol') else [],
                            'timeframes': [data_info.get('timeframe', '')] if data_info.get('timeframe') else [],
                            'factor_types': factors_info.get('factor_types', [])
                        }
                        
                        # æ·»åŠ è°ƒè¯•æ—¥å¿—
                        print(f"æ„å»ºé…ç½®ä¿¡æ¯: session_id={session_id}")
                        print(f"  data_info: {data_info}")
                        print(f"  factors_info: {factors_info}")
                        print(f"  æ„å»ºçš„config: {config}")
                        
                        completed_sessions.append({
                            'session_id': session_id,
                            'timestamp': timestamp,
                            'status': 'completed',
                            'config': config,
                            'results': result_data,
                            'factors_count': factors_info.get('total_factors', 0),
                            'start_time': timestamp,
                            'end_time': timestamp
                        })
                
                elif 'mining_sessions' in result_data:
                    # mining_sessions.json æ ¼å¼
                    for session in result_data.get('mining_sessions', []):
                        completed_sessions.append({
                            'session_id': session.get('session_id', ''),
                            'timestamp': session.get('timestamp', datetime.now().isoformat()),
                            'status': session.get('status', 'completed'),
                            'config': session.get('config', {}),
                            'results': session.get('results', {}),
                            'factors_count': session.get('results', {}).get('total_factors', 0),
                            'start_time': session.get('timestamp', datetime.now().isoformat()),
                            'end_time': session.get('timestamp', datetime.now().isoformat())
                        })
                
            except Exception as e:
                print(f"åŠ è½½æŒ–æ˜ç»“æœæ–‡ä»¶å¤±è´¥ {result_file}: {e}")
                continue
        
        # å»é‡ï¼ˆåŸºäºsession_idï¼‰
        seen_ids = set()
        unique_sessions = []
        for session in completed_sessions:
            if session['session_id'] not in seen_ids:
                seen_ids.add(session['session_id'])
                unique_sessions.append(session)
        
        print(f"  load_completed_mining_sessions: å»é‡åä¼šè¯æ•°: {len(unique_sessions)}")
        print(f"  load_completed_mining_sessions: åŠ è½½å®Œæˆ")
        
        return unique_sessions
        
    except Exception as e:
        print(f"åŠ è½½æŒ–æ˜ä¼šè¯å¤±è´¥: {e}")
        return []

def save_mining_results(session_id, results, config):
    """ä¿å­˜æŒ–æ˜ç»“æœ"""
    try:
        print(f"ä¿å­˜æŒ–æ˜ç»“æœ: session_id={session_id}")
        print(f"ç»“æœç»“æ„: {list(results.keys())}")
        print(f"é…ç½®ç»“æ„: {list(config.keys())}")
        
        # åˆ›å»ºç»“æœç›®å½•
        results_dir = Path("factorlib") / "mining_history"
        results_dir.mkdir(parents=True, exist_ok=True)
        print(f"ç»“æœç›®å½•: {results_dir}")
        
        # ä¿å­˜ç»“æœæ–‡ä»¶
        output_path = results_dir / f"mining_results_{session_id}.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        print(f"ç»“æœæ–‡ä»¶ä¿å­˜: {output_path}")
        
        # ä¿å­˜å› å­æ•°æ®ä¸ºCSV
        if 'evaluation' in results and results['evaluation']:
            csv_path = results_dir / f"factors_{session_id}.csv"
            factors_df = pd.DataFrame(results['evaluation'])
            factors_df.to_csv(csv_path, index=False)
            print(f"å› å­CSVä¿å­˜: {csv_path}")
        else:
            print(f"æ²¡æœ‰è¯„ä¼°ç»“æœï¼Œè·³è¿‡CSVä¿å­˜")
        
        # åŒæ—¶ä¿å­˜åˆ°ä¼šè¯å†å²æ–‡ä»¶
        try:
            print(f"å¼€å§‹ä¿å­˜ä¼šè¯å†å²...")
            save_session_to_history(session_id, results, config)
            print(f"ä¼šè¯å†å²ä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"ä¿å­˜ä¼šè¯å†å²å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        return output_path
        
    except Exception as e:
        print(f"ä¿å­˜æŒ–æ˜ç»“æœå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

def save_session_to_history(session_id, results, config):
    """ä¿å­˜ä¼šè¯åˆ°å†å²æ–‡ä»¶"""
    try:
        print(f"ä¿å­˜ä¼šè¯å†å²: session_id={session_id}")
        
        # åˆ›å»ºå†å²ç›®å½•
        history_dir = Path("factorlib") / "mining_history"
        history_dir.mkdir(parents=True, exist_ok=True)
        print(f"å†å²ç›®å½•: {history_dir}")
        
        # å†å²æ–‡ä»¶è·¯å¾„
        history_file = history_dir / "mining_sessions.json"
        print(f"å†å²æ–‡ä»¶: {history_file}")
        
        # è¯»å–ç°æœ‰å†å²
        if history_file.exists():
            with open(history_file, 'r', encoding='utf-8') as f:
                history_data = json.load(f)
            print(f"è¯»å–ç°æœ‰å†å²: {len(history_data.get('mining_sessions', []))} ä¸ªä¼šè¯")
        else:
            history_data = {"mining_sessions": [], "metadata": {"total_sessions": 0, "last_updated": "", "version": "1.0"}}
            print(f"åˆ›å»ºæ–°çš„å†å²æ•°æ®ç»“æ„")
        
        # åˆ›å»ºæ–°çš„ä¼šè¯è®°å½•
        session_record = {
            "session_id": session_id,
            "timestamp": datetime.now().isoformat(),
            "status": "completed",
            "config": {
                "symbols": config.get('symbols', []),
                "timeframes": config.get('timeframes', []),
                "factor_types": config.get('factor_types', []),
                "max_factors": config.get('max_factors', 15),
                "optimization_method": config.get('optimization_method', 'greedy'),
                "start_date": config.get('start_date', ''),
                "end_date": config.get('end_date', '')
            },
            "results": {
                "total_factors": results.get('factors_info', {}).get('total_factors', 0),
                "selected_factors": results.get('optimization', {}).get('selected_factors', []),
                "optimization_score": results.get('optimization', {}).get('score', 0),
                "execution_time": 0,  # å¯ä»¥æ·»åŠ å®é™…æ‰§è¡Œæ—¶é—´
                "output_path": str(results.get('output_path', ''))
            }
        }
        
        print(f"ä¼šè¯è®°å½•: {session_record}")
        
        # æ·»åŠ åˆ°å†å²åˆ—è¡¨
        history_data["mining_sessions"].append(session_record)
        
        # æ›´æ–°å…ƒæ•°æ®
        history_data["metadata"]["total_sessions"] = len(history_data["mining_sessions"])
        history_data["metadata"]["last_updated"] = datetime.now().isoformat()
        
        # ä¿å­˜å†å²æ–‡ä»¶
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(history_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ä¼šè¯ {session_id} å·²ä¿å­˜åˆ°å†å²æ–‡ä»¶")
        
    except Exception as e:
        print(f"ä¿å­˜ä¼šè¯å†å²å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
