"""
å› å­åº“ç›¸å…³è·¯ç”±ï¼ˆV3æ¶æ„ï¼‰
ä½¿ç”¨é€æ˜å› å­å­˜å‚¨ä¸ç»Ÿä¸€å¼•æ“
"""

import json
import pandas as pd
from datetime import datetime
from pathlib import Path
from flask import Blueprint, request, jsonify
import math
from factor_miner.core.factor_evaluator import FactorEvaluator
from factor_miner.core.factor_engine import get_global_engine
from factor_miner.core.evaluation_io import (
    save_evaluation_results as core_save_evaluation_results,
    load_evaluations as core_load_evaluations,
)
# from factor_miner.core.factor_storage import get_global_storage

bp = Blueprint('factors', __name__)

# å› å­åº“è·¯å¾„ï¼ˆV3 æ‰å¹³åŒ–ï¼‰
FACTOR_LIBRARY_DIR = Path(__file__).parent.parent.parent / "factorlib"

# Alpha101å› å­å…¬å¼æ˜ å°„
ALPHA101_FORMULAS = {
    'alpha001': '(rank(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2.), 5)) - 0.5)',
    'alpha002': '(-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))',
    'alpha003': '(-1 * correlation(rank(open), rank(volume), 10))',
    'alpha004': '(-1 * Ts_Rank(rank(low), 9))',
    'alpha005': '(rank((open - (sum(vwap, 10) / 10))) * (-1 * abs(rank((close - vwap)))))',
    'alpha006': '(-1 * correlation(open, volume, 10))',
    'alpha007': '((adv20 < volume) ? ((-1 * ts_rank(abs(delta(close, 7)), 60)) * sign(delta(close, 7))) : (-1))',
    'alpha008': '(-1 * rank(((sum(open, 5) * sum(returns, 5)) - delay((sum(open, 5) * sum(returns, 5)), 10))))',
    'alpha009': '((0 < ts_min(delta(close, 1), 5)) ? delta(close, 1) : ((ts_max(delta(close, 1), 5) < 0) ? delta(close, 1) : (-1 * delta(close, 1))))',
    'alpha010': 'rank(((0 < ts_min(delta(close, 1), 4)) ? delta(close, 1) : ((ts_max(delta(close, 1), 4) < 0) ? delta(close, 1) : (-1 * delta(close, 1)))))',
    # å¯ä»¥ç»§ç»­æ·»åŠ æ›´å¤šAlpha101å› å­å…¬å¼...
}

# é¡µé¢è·¯ç”± - å·²è¿ç§»åˆ°main.py

# APIè·¯ç”±
@bp.route('/list')
def list_factors():
    """è·å–å› å­åˆ—è¡¨ï¼ˆV3ï¼šä» definitions è¯»å–ï¼‰"""
    try:
        definitions_dir = FACTOR_LIBRARY_DIR / "definitions"
        factors = []
        if definitions_dir.exists():
            for file in definitions_dir.glob("*.json"):
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    comp = data.get('computation_data', {})
                    formula_preview = None
                    if data.get('computation_type') == 'formula':
                        formula_preview = (comp.get('formula') or '').split('\n')[0][:120]
                    # èšåˆè¯„ä¼°å‡å€¼ï¼ˆæ ¸å¿ƒIOï¼‰
                    evaluated = False
                    eval_count = 0
                    avg_metrics = {}
                    last_evaluated_at = None
                    try:
                        eval_payload = core_load_evaluations(data.get('factor_id'))
                        evaluations = (eval_payload or {}).get('evaluations') or []
                        eval_count = len(evaluations)
                        if eval_count > 0:
                            evaluated = True
                            keys = ['ic_pearson', 'ic_spearman', 'win_rate', 'sharpe_ratio', 'long_short_return']
                            sums = {k: 0.0 for k in keys}
                            counts = {k: 0 for k in keys}
                            for ev in evaluations:
                                res = (ev or {}).get('results') or {}
                                for k in keys:
                                    v = res.get(k)
                                    if isinstance(v, (int, float)):
                                        sums[k] += float(v)
                                        counts[k] += 1
                                last_evaluated_at = (ev or {}).get('evaluated_at') or last_evaluated_at
                            for k in keys:
                                avg_metrics[k] = (sums[k] / counts[k]) if counts[k] > 0 else None
                    except Exception:
                        pass
                    # æ•°å€¼å®‰å…¨å¤„ç†ï¼Œé¿å…NaNè¿›å…¥JSON
                    def _safe_num(x):
                        return float(x) if isinstance(x, (int, float)) and math.isfinite(x) else None
                    avg_metrics_clean = {}
                    for k, v in (avg_metrics or {}).items():
                        avg_metrics_clean[k] = _safe_num(v)
                    factors.append({
                        'id': data.get('factor_id'),
                        'name': data.get('name'),
                        'description': data.get('description'),
                        'type': data.get('category'),
                        'created_at': data.get('metadata', {}).get('created_at'),
                        'computation_type': data.get('computation_type'),
                        'formula': formula_preview,
                        'evaluated': evaluated,
                        'evaluations_count': eval_count,
                        'avg_metrics': avg_metrics_clean,
                        'last_evaluated_at': last_evaluated_at,
                        'ic': _safe_num((avg_metrics or {}).get('ic_pearson')),
                        'ir': _safe_num((avg_metrics or {}).get('sharpe_ratio')),
                    })
                except Exception:
                    continue
        return jsonify({'success': True, 'factors': factors, 'total': len(factors)})
    except Exception as e:
        return jsonify({'success': False, 'message': f'è·å–å› å­åˆ—è¡¨å¤±è´¥: {str(e)}'})

def get_traditional_factor_formula(_):
    """V3ä¸å†ä»æ—§CSVæ¨å¯¼ä¼ ç»ŸæŒ‡æ ‡å…¬å¼ï¼Œä¿ç•™å ä½ã€‚"""
    return None

@bp.route('/evaluate', methods=['POST'])
def evaluate_factor():
    """è¯„ä¼°å› å­"""
    try:
        data = request.get_json()
        
        factor_id = data.get('factor_id')
        symbol = data.get('symbol')
        timeframe = data.get('timeframe')
        start_date = data.get('start_date')
        end_date = data.get('end_date')
        exchange = data.get('exchange', 'binance')
        trade_type = data.get('trade_type', 'futures')
        
        if not all([factor_id, symbol, timeframe, start_date, end_date]):
            return jsonify({
                'success': False,
                'message': 'ç¼ºå°‘å¿…è¦å‚æ•°'
            })
        
        # ä½¿ç”¨V3å¼•æ“ç›´æ¥è®¡ç®—
        engine = get_global_engine()
        # åŠ è½½æœ¬åœ°æ•°æ®
        market_data = load_local_market_data(symbol, timeframe, start_date, end_date, exchange, trade_type)
        
        if market_data is None or market_data.empty:
            return jsonify({
                'success': False,
                'message': 'æ— æ³•åŠ è½½å¸‚åœºæ•°æ®'
            })
        
        # è®¡ç®—å› å­å€¼ï¼ˆV3ï¼‰
        factor_values = engine.compute_single_factor(factor_id, market_data)
        
        if factor_values is None:
            return jsonify({
                'success': False,
                'message': 'å› å­è®¡ç®—å¤±è´¥'
            })
        
        # è¯„ä¼°å› å­
        evaluator = FactorEvaluator()
        
        # ç®€åŒ–å¯¹é½ç­–ç•¥ï¼šå¼ºåˆ¶ä»¥ market_data.index ä¸ºå‡†
        market_data = market_data.sort_index()
        market_data['returns'] = market_data['close'].pct_change()
        
        # ç¡®ä¿å› å­ä¸ºSeries
        if hasattr(factor_values, 'columns'):
            try:
                factor_values = factor_values.iloc[:, 0]
            except Exception:
                factor_values = factor_values.squeeze()

        # ç›´æ¥æŒ‰å¸‚åœºæ•°æ®ç´¢å¼•é‡å»ºï¼Œçª—å£æœŸäº§ç”Ÿçš„NaNåç»­ä¸€èµ·è¿‡æ»¤
        factor_values = factor_values.reindex(market_data.index)
        returns = market_data['returns']

        # åŒæ­¥è¿‡æ»¤éç©ºå¹¶ä¿è¯æ ·æœ¬æ•°é‡
        mask = factor_values.notna() & returns.notna()
        factor_values = factor_values[mask]
        returns = returns[mask]

        if len(factor_values) < 30:
            return jsonify({
                'success': False,
                'message': f'æ•°æ®ä¸è¶³ï¼šæ ·æœ¬æ•° {len(factor_values)} < 30ï¼Œè¯·æ‰©å¤§æ—¶é—´èŒƒå›´æˆ–é€‰æ‹©æ›´ä½é¢‘æ—¶é—´æ¡†æ¶'
            })
        
        # è¯„ä¼°å› å­
        evaluation_results = evaluator.evaluate_single_factor(
            factor=factor_values,
            returns=returns,
            factor_name=factor_id
        )
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        save_evaluation_results(factor_id, evaluation_results, {
            'symbol': symbol,
            'timeframe': timeframe,
            'start_date': start_date,
            'end_date': end_date
        })
        
        return jsonify({
            'success': True,
            'message': 'å› å­è¯„ä¼°å®Œæˆ',
            'results': evaluation_results
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'å› å­è¯„ä¼°å¤±è´¥: {str(e)}'
        })

@bp.route('/detail/<factor_id>')
def get_factor_detail(factor_id):
    """è·å–å› å­è¯¦æƒ…"""
    try:
        factor_info = parse_factor_id(factor_id)
        if not factor_info:
            return jsonify({
                'success': False,
                'message': 'æ— æ•ˆçš„å› å­ID'
            })
        
        # è·å–å› å­è¯¦ç»†ä¿¡æ¯
        factor_detail = get_factor_detail_info(factor_info)
        
        return jsonify({
            'success': True,
            'factor': factor_detail
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'è·å–å› å­è¯¦æƒ…å¤±è´¥: {str(e)}'
        })

@bp.route('/export/<factor_id>')
def export_factor(factor_id):
    """å¯¼å‡ºå› å­"""
    try:
        # V3å¯¼å‡ºï¼šä»…å¯¼å‡ºå®šä¹‰
        export_dir = FACTOR_LIBRARY_DIR / "exports"
        export_dir.mkdir(parents=True, exist_ok=True)
        definition_file = FACTOR_LIBRARY_DIR / "definitions" / f"{factor_id}.json"
        if not definition_file.exists():
            return jsonify({'success': False, 'message': 'å› å­å®šä¹‰ä¸å­˜åœ¨'})
        export_path = export_dir / f"{factor_id}_definition.json"
        with open(definition_file, 'r', encoding='utf-8') as src, open(export_path, 'w', encoding='utf-8') as dst:
            dst.write(src.read())
        
        return jsonify({'success': True, 'message': 'å› å­å¯¼å‡ºæˆåŠŸ', 'export_path': str(export_path)})
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'å› å­å¯¼å‡ºå¤±è´¥: {str(e)}'
        })

@bp.route('/evaluations/<factor_id>')
def get_evaluations(factor_id: str):
    """è·å–æŸå› å­çš„å†å²è¯„ä¼°è®°å½•ï¼ˆå¤šç»“æœç»“æ„ï¼‰"""
    try:
        payload = core_load_evaluations(factor_id)
        return jsonify({'success': True, 'factor_id': payload.get('factor_id', factor_id), 'evaluations': payload.get('evaluations', [])})
    except Exception as e:
        return jsonify({'success': False, 'message': f'è·å–è¯„ä¼°å†å²å¤±è´¥: {str(e)}'})

@bp.route('/batch_evaluate', methods=['POST'])
def batch_evaluate():
    """æ‰¹é‡è¯„ä¼°ï¼šå¤šä¸ªå› å­*å¤šä¸ªäº¤æ˜“å¯¹*å¤šä¸ªæ—¶é—´æ¡†æ¶"""
    try:
        payload = request.get_json() or {}
        factor_ids = payload.get('factor_ids') or []
        symbols = payload.get('symbols') or []
        timeframes = payload.get('timeframes') or []
        start_date = payload.get('start_date')
        end_date = payload.get('end_date')
        exchange = payload.get('exchange', 'binance')
        trade_type = payload.get('trade_type', 'futures')

        if not factor_ids or not symbols or not timeframes or not start_date or not end_date:
            return jsonify({'success': False, 'message': 'ç¼ºå°‘å¿…è¦å‚æ•°ï¼ˆfactor_ids/symbols/timeframes/start_date/end_dateï¼‰'})

        engine = get_global_engine()
        evaluator = FactorEvaluator()
        results = []

        for factor_id in factor_ids:
            for symbol in symbols:
                for timeframe in timeframes:
                    market_data = load_local_market_data(symbol, timeframe, start_date, end_date, exchange, trade_type)
                    if market_data is None or market_data.empty:
                        results.append({'factor_id': factor_id, 'symbol': symbol, 'timeframe': timeframe, 'success': False, 'message': 'æ— æ³•åŠ è½½å¸‚åœºæ•°æ®'})
                        continue
                    try:
                        factor_values = engine.compute_single_factor(factor_id, market_data)
                        if factor_values is None:
                            results.append({'factor_id': factor_id, 'symbol': symbol, 'timeframe': timeframe, 'success': False, 'message': 'å› å­è®¡ç®—å¤±è´¥'})
                            continue
                        market_data = market_data.sort_index()
                        market_data['returns'] = market_data['close'].pct_change()
                        if hasattr(factor_values, 'columns'):
                            try:
                                factor_values = factor_values.iloc[:, 0]
                            except Exception:
                                factor_values = factor_values.squeeze()
                        factor_values = factor_values.reindex(market_data.index)
                        returns = market_data['returns']
                        mask = factor_values.notna() & returns.notna()
                        factor_values = factor_values[mask]
                        returns = returns[mask]
                        if len(factor_values) < 30:
                            results.append({'factor_id': factor_id, 'symbol': symbol, 'timeframe': timeframe, 'success': False, 'message': f'æ•°æ®ä¸è¶³ï¼šæ ·æœ¬æ•° {len(factor_values)} < 30'})
                            continue
                        eval_res = evaluator.evaluate_single_factor(factor=factor_values, returns=returns, factor_name=factor_id)
                        results.append({'factor_id': factor_id, 'symbol': symbol, 'timeframe': timeframe, 'success': True, 'results': eval_res})
                    except Exception as ex:
                        results.append({'factor_id': factor_id, 'symbol': symbol, 'timeframe': timeframe, 'success': False, 'message': str(ex)})
        return jsonify({'success': True, 'results': results})
    except Exception as e:
        return jsonify({'success': False, 'message': f'æ‰¹é‡è¯„ä¼°å¤±è´¥: {str(e)}'})

def parse_alpha101_filename(filename):
    """è§£æAlpha101æ–‡ä»¶å"""
    # æ ¼å¼: alpha101_results_SYMBOL_TIMEFRAME.pkl
    parts = filename.replace('.pkl', '').split('_')
    if len(parts) >= 4:
        symbol = parts[2]
        timeframe = parts[3]
        return symbol, timeframe
    return 'Unknown', 'Unknown'

def clean_factor_name(factor_name, factor_type=''):
    """æ¸…ç†å› å­åç§°ï¼Œç§»é™¤ä¸åˆç†çš„å¸ç§åç¼€"""
    # éœ€è¦ç§»é™¤çš„å¸ç§åç¼€åˆ—è¡¨
    crypto_symbols = ['BTC', 'ETH', 'BNB', 'SOL', 'ADA', 'DOGE', 'LINK', 'LPT', 'MOVR', 'PEOPLE', 'SUI', 'FIL']
    
    # ç§»é™¤å› å­åç§°æœ«å°¾çš„å¸ç§åç¼€
    for symbol in crypto_symbols:
        # ç§»é™¤ "_SYMBOL" æ ¼å¼çš„åç¼€
        if factor_name.endswith(f'_{symbol}'):
            factor_name = factor_name[:-len(symbol)-1]
        # ç§»é™¤ "_SYMBOL_USDT" æ ¼å¼çš„åç¼€  
        if factor_name.endswith(f'_{symbol}_USDT'):
            factor_name = factor_name[:-len(symbol)-6]
        # ç§»é™¤ "_SYMBOL_timeframe" æ ¼å¼çš„åç¼€
        for tf in ['1h', '4h', '1m', '5m', '15m', '1d']:
            if factor_name.endswith(f'_{symbol}_{tf}'):
                factor_name = factor_name[:-len(symbol)-len(tf)-2]
    
    return factor_name

def parse_factor_id(factor_id):
    """è§£æå› å­ID"""
    parts = factor_id.split('_', 2)
    if len(parts) >= 2:
        return {
            'type': parts[0],
            'subtype': parts[1] if len(parts) > 2 else None,
            'identifier': parts[2] if len(parts) > 2 else parts[1]
        }
    return None

def calculate_factor_values(factor_info, market_data):
    """è®¡ç®—å› å­å€¼"""
    # V3å·²ä¸å†ä½¿ç”¨è¯¥å‡½æ•°ï¼Œä¿ç•™å ä½
    return None

def save_evaluation_results(factor_id, results, metadata):
    """è½¬è°ƒæ ¸å¿ƒå±‚çš„è¯„ä¼°ç»“æœä¿å­˜"""
    try:
        core_save_evaluation_results(factor_id, results, metadata)
    except Exception as e:
        print(f"ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {e}")

def get_factor_detail_info(factor_info):
    """è·å–å› å­è¯¦ç»†ä¿¡æ¯"""
    # è¿™é‡Œå®ç°è·å–å› å­è¯¦ç»†ä¿¡æ¯çš„é€»è¾‘
    return {
        'id': factor_info.get('identifier'),
        'type': factor_info.get('type'),
        'description': 'å› å­è¯¦ç»†ä¿¡æ¯',
        'formula': 'å› å­è®¡ç®—å…¬å¼',
        'parameters': {},
        'evaluation_history': []
    }

def export_factor_data(factor_info):
    """å¯¼å‡ºå› å­æ•°æ®"""
    export_dir = FACTOR_LIBRARY_DIR / "exports"
    export_dir.mkdir(parents=True, exist_ok=True)
    
    identifier = factor_info.get('identifier') or factor_info.get('id') or 'factor'
    export_file = export_dir / f"{identifier}_export.csv"
    
    # åˆ›å»ºç¤ºä¾‹å¯¼å‡ºæ–‡ä»¶
    pd.DataFrame({
        'factor_name': [factor_info['identifier']],
        'type': [factor_info['type']],
        'exported_at': [datetime.now().isoformat()]
    }).to_csv(export_file, index=False)
    
    return str(export_file)

def load_local_market_data(symbol, timeframe, start_date, end_date, exchange='binance', trade_type='futures'):
    """ä»æœ¬åœ°åŠ è½½å¸‚åœºæ•°æ®"""
    try:
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        data_dir = Path(__file__).parent.parent.parent / "data" / exchange / trade_type
        
        # è§£æäº¤æ˜“å¯¹æ ¼å¼
        # APIè¿”å›çš„äº¤æ˜“å¯¹æ ¼å¼æ˜¯ BTC_USDTï¼Œä½†å®é™…æ–‡ä»¶åéœ€è¦ BTC_USDT_USDT
        if '_' in symbol:
            parts = symbol.split('_')
            if len(parts) >= 2:
                # å¦‚æœè¾“å…¥æ˜¯ BTC_USDTï¼Œæˆ‘ä»¬éœ€è¦æ„å»º BTC_USDT_USDT
                base_symbol = parts[0]  # BTC
                filename = f"{base_symbol}_USDT_USDT-{timeframe}-{trade_type}.feather"
            else:
                base_symbol = symbol
                filename = f"{base_symbol}_USDT_USDT-{timeframe}-{trade_type}.feather"
        else:
            base_symbol = symbol
            filename = f"{base_symbol}_USDT_USDT-{timeframe}-{trade_type}.feather"
            
        print(f"è§£æäº¤æ˜“å¯¹: {symbol} -> {base_symbol}")
        print(f"æ—¶é—´æ¡†æ¶å‚æ•°: {timeframe}")
        print(f"æ„å»ºæ–‡ä»¶å: {filename}")
        file_path = data_dir / filename
        
        if not file_path.exists():
            print(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
        
        # è¯»å–featheræ–‡ä»¶
        data = pd.read_feather(file_path)
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {data.shape}")
        print(f"åŸå§‹æ•°æ®åˆ—å: {list(data.columns)}")
        print(f"åŸå§‹æ•°æ®ç´¢å¼•: {data.index[:5] if len(data) > 0 else 'ç©º'}")
        
        # ç¡®ä¿æ•°æ®æœ‰å¿…è¦çš„åˆ—
        required_columns = ['open', 'high', 'low', 'close', 'volume']
        if not all(col in data.columns for col in required_columns):
            print(f"æ•°æ®æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {required_columns}")
            return None
        
        # ç»Ÿä¸€çš„æ—¶é—´åˆ—è§£æï¼ˆè‡ªåŠ¨è¯†åˆ«ç§’/æ¯«ç§’/å­—ç¬¦ä¸²ï¼‰
        def parse_time_series(series):
            try:
                if pd.api.types.is_datetime64_any_dtype(series):
                    return series
                if pd.api.types.is_numeric_dtype(series):
                    s = series.dropna()
                    unit = 's'
                    if len(s) > 0:
                        sample = s.iloc[0]
                        unit = 'ms' if sample > 10_000_000_000 else 's'
                    return pd.to_datetime(series, errors='coerce', unit=unit, utc=True)
                return pd.to_datetime(series, errors='coerce', utc=True)
            except Exception:
                return pd.to_datetime(series, errors='coerce', utc=True)
        
        # å¦‚æœæœ‰æ—¶é—´åˆ—ï¼Œè®¾ç½®ä¸ºç´¢å¼•
        time_col = None
        for cand in ['timestamp', 'datetime', 'time', 'date']:
            if cand in data.columns:
                time_col = cand
                break
        
        if time_col is not None:
            # æ‰¾åˆ°æ—¶é—´åˆ—ï¼Œæ­£ç¡®è®¾ç½®ç´¢å¼•
            print(f"æ‰¾åˆ°æ—¶é—´åˆ—: {time_col}")
            data[time_col] = parse_time_series(data[time_col])
            data.set_index(time_col, inplace=True)
        else:
            # æ²¡æœ‰æ‰¾åˆ°æ—¶é—´åˆ—ï¼Œæ£€æŸ¥æ•°æ®æ˜¯å¦å·²ç»æœ‰æ­£ç¡®çš„ç´¢å¼•
            if isinstance(data.index, pd.DatetimeIndex):
                print("æ•°æ®å·²æœ‰æ­£ç¡®çš„æ—¶é—´ç´¢å¼•ï¼Œæ— éœ€è®¾ç½®")
            else:
                # å°è¯•ä»æ–‡ä»¶åæ¨æ–­æ—¶é—´ä¿¡æ¯
                print("æ²¡æœ‰æ‰¾åˆ°æ—¶é—´åˆ—ï¼Œä¸”æ•°æ®æ²¡æœ‰æ—¶é—´ç´¢å¼•")
                print("å°è¯•ä»æ–‡ä»¶åæ¨æ–­æ—¶é—´ä¿¡æ¯...")
                
                # æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ç»æ˜¯æ­£ç¡®çš„æ ¼å¼ï¼ˆfeatheræ–‡ä»¶é€šå¸¸ä¿æŒç´¢å¼•ï¼‰
                if len(data) > 0:
                    print(f"æ•°æ®è¡Œæ•°: {len(data)}")
                    print(f"æ•°æ®åˆ—: {list(data.columns)}")
                    print(f"å½“å‰ç´¢å¼•ç±»å‹: {type(data.index)}")
                    
                    # å¦‚æœæ•°æ®é‡å¾ˆå¤§ä¸”æ²¡æœ‰æ—¶é—´ç´¢å¼•ï¼Œå¯èƒ½æ˜¯ç´¢å¼•ä¸¢å¤±äº†
                    if len(data) > 1000 and not isinstance(data.index, pd.DatetimeIndex):
                        print("è­¦å‘Šï¼šæ•°æ®é‡å¤§ä½†ç¼ºå°‘æ—¶é—´ç´¢å¼•ï¼Œå¯èƒ½æ— æ³•æ­£ç¡®åŠ è½½")
                        return None
                else:
                    print("æ•°æ®ä¸ºç©ºï¼Œæ— æ³•å¤„ç†")
                    return None
        
        print(f"è®¾ç½®ç´¢å¼•åçš„æ•°æ®å½¢çŠ¶: {data.shape}")
        print(f"è®¾ç½®ç´¢å¼•åçš„ç´¢å¼•ç±»å‹: {type(data.index)}")
        print(f"è®¾ç½®ç´¢å¼•åçš„å‰å‡ è¡Œç´¢å¼•: {data.index[:5] if len(data) > 0 else 'ç©º'}")
        
        # è¿‡æ»¤æ—¥æœŸèŒƒå›´
        if start_date and end_date:
            try:
                # ç»Ÿä¸€åˆ°UTC
                start_dt = pd.to_datetime(start_date, utc=True)
                end_dt = pd.to_datetime(end_date, utc=True)
                
                # ç¡®ä¿ç´¢å¼•æ˜¯datetimeç±»å‹
                if not isinstance(data.index, pd.DatetimeIndex):
                    print("è­¦å‘Šï¼šæ•°æ®ç´¢å¼•ä¸æ˜¯datetimeç±»å‹ï¼Œå°è¯•è½¬æ¢...")
                    try:
                        data.index = pd.to_datetime(data.index, utc=True)
                    except Exception as conv_err:
                        print(f"ç´¢å¼•è½¬æ¢å¤±è´¥: {conv_err}")
                        return None
                
                # ç»Ÿä¸€ç´¢å¼•åˆ°UTCï¼ˆè‹¥æ— tzåˆ™æœ¬åœ°åŒ–ä¸ºUTCï¼‰
                try:
                    if data.index.tz is None:
                        data.index = data.index.tz_localize('UTC')
                    else:
                        data.index = data.index.tz_convert('UTC')
                except Exception as tz_err:
                    print(f"æ—¶åŒºç»Ÿä¸€å¤±è´¥: {tz_err}")
                    # å¦‚æœæ—¶åŒºå¤„ç†å¤±è´¥ï¼Œå°è¯•ç§»é™¤æ—¶åŒºä¿¡æ¯
                    try:
                        data.index = data.index.tz_localize(None)
                        print("å·²ç§»é™¤æ—¶åŒºä¿¡æ¯ï¼Œä½¿ç”¨æœ¬åœ°æ—¶é—´")
                    except Exception:
                        print("æ—¶åŒºå¤„ç†å®Œå…¨å¤±è´¥")
                        return None
                
                # æ‰§è¡Œæ—¥æœŸè¿‡æ»¤
                original_count = len(data)
                data = data[(data.index >= start_dt) & (data.index <= end_dt)]
                filtered_count = len(data)
                print(f"æ—¥æœŸè¿‡æ»¤: {original_count} -> {filtered_count} æ¡è®°å½•")
                
                if filtered_count == 0:
                    print("è­¦å‘Šï¼šè¿‡æ»¤åæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ—¥æœŸèŒƒå›´")
                    print(f"æ•°æ®æ—¶é—´èŒƒå›´: {data.index.min()} åˆ° {data.index.max()}")
                    print(f"è¯·æ±‚æ—¶é—´èŒƒå›´: {start_dt} åˆ° {end_dt}")
                    
            except Exception as e:
                print(f"æ—¥æœŸè¿‡æ»¤å¤±è´¥: {e}")
                # å¦‚æœæ—¥æœŸè¿‡æ»¤å¤±è´¥ï¼Œè¿”å›Noneè€Œä¸æ˜¯ç»§ç»­ä½¿ç”¨åŸå§‹æ•°æ®
                return None
        
        # ç¡®ä¿æ•°æ®æŒ‰æ—¶é—´æ’åº
        data.sort_index(inplace=True)
        
        # æœ€ç»ˆæ•°æ®éªŒè¯
        if len(data) == 0:
            print("âŒ æœ€ç»ˆæ•°æ®ä¸ºç©ºï¼Œæ— æ³•ä½¿ç”¨")
            return None
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        required_columns = ['open', 'high', 'low', 'close', 'volume']
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            print(f"âŒ æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            return None
        
        # éªŒè¯æ—¶é—´ç´¢å¼•
        if not isinstance(data.index, pd.DatetimeIndex):
            print("âŒ æ•°æ®ç´¢å¼•ä¸æ˜¯æ—¶é—´ç±»å‹")
            return None
        
        # æ£€æŸ¥æ—¶é—´èŒƒå›´
        time_range = data.index.max() - data.index.min()
        if time_range.total_seconds() < 60:  # å°‘äº1åˆ†é’Ÿ
            print("âŒ æ•°æ®æ—¶é—´èŒƒå›´è¿‡çŸ­")
            return None
        
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {len(data)} æ¡è®°å½•")
        print(f"   æ—¶é—´èŒƒå›´: {data.index.min()} åˆ° {data.index.max()}")
        print(f"   æ•°æ®åˆ—: {list(data.columns)}")
        return data
        
    except Exception as e:
        print(f"åŠ è½½æœ¬åœ°æ•°æ®å¤±è´¥: {e}")
        return None 

# æ–°å¢ï¼šå› å­è®¡ç®—API
@bp.route('/calculate', methods=['POST'])
def calculate_factor():
    """è®¡ç®—å› å­å€¼"""
    try:
        data = request.get_json()
        factor_id = data.get('factor_id')
        market_data = data.get('data', {})
        parameters = data.get('parameters', {})
        
        print(f"ğŸ” æ”¶åˆ°å› å­è®¡ç®—è¯·æ±‚: {factor_id}")
        print(f"ğŸ” å¸‚åœºæ•°æ®é•¿åº¦: {len(market_data.get('close', []))}")
        print(f"ğŸ” å‚æ•°: {parameters}")
        
        if not factor_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘factor_idå‚æ•°'})
        
        # æŸ¥æ‰¾å› å­å®šä¹‰
        factor_file = FACTOR_LIBRARY_DIR / "definitions" / f"{factor_id}.json"
        if not factor_file.exists():
            print(f"âŒ å› å­å®šä¹‰æ–‡ä»¶ä¸å­˜åœ¨: {factor_file}")
            return jsonify({'success': False, 'error': f'å› å­ {factor_id} ä¸å­˜åœ¨'})
        
        with open(factor_file, 'r', encoding='utf-8') as f:
            factor_info = json.load(f)
        
        print(f"ğŸ” å› å­ä¿¡æ¯: {factor_info.get('name', factor_id)}")
        
        # æ£€æŸ¥å› å­ç±»å‹
        computation_type = factor_info.get('computation_type')
        
        if computation_type == 'formula':
            # å…¬å¼å› å­
            factor_values = calculate_formula_factor(factor_info, market_data, parameters)
        elif computation_type == 'ml':
            # MLå› å­
            factor_values = calculate_ml_factor(factor_info, market_data, parameters)
        else:
            # é»˜è®¤ä½¿ç”¨å‡½æ•°è®¡ç®—
            factor_values = calculate_function_factor(factor_info, market_data, parameters)
        
        if factor_values is not None:
            print(f"âœ… å› å­è®¡ç®—æˆåŠŸï¼Œè¿”å› {len(factor_values)} ä¸ªå€¼")
            return jsonify({
                'success': True,
                'factor_values': factor_values,
                'factor_name': factor_info.get('name', factor_id)
            })
        else:
            print(f"âŒ å› å­è®¡ç®—å¤±è´¥")
            return jsonify({'success': False, 'error': 'å› å­è®¡ç®—å¤±è´¥'})
            
    except Exception as e:
        print(f"âŒ å› å­è®¡ç®—APIå¼‚å¸¸: {e}")
        return jsonify({'success': False, 'error': str(e)})

def calculate_formula_factor(factor_info, market_data, parameters):
    """è®¡ç®—å…¬å¼å› å­"""
    try:
        print(f"ğŸ” è®¡ç®—å…¬å¼å› å­: {factor_info.get('name')}")
        # è¿™é‡Œå¯ä»¥å®ç°å…¬å¼è§£æå’Œè®¡ç®—
        # æš‚æ—¶è¿”å›ç®€å•çš„ç§»åŠ¨å¹³å‡çº¿ä½œä¸ºç¤ºä¾‹
        close_prices = market_data.get('close', [])
        if not close_prices:
            print("âŒ æ²¡æœ‰æ”¶ç›˜ä»·æ•°æ®")
            return None
        
        period = parameters.get('period', 20)
        if len(close_prices) < period:
            print(f"âŒ æ•°æ®é•¿åº¦ {len(close_prices)} å°äºå‘¨æœŸ {period}")
            return None
        
        # è®¡ç®—ç®€å•ç§»åŠ¨å¹³å‡çº¿
        factor_values = []
        for i in range(len(close_prices)):
            if i < period - 1:
                factor_values.append(None)
            else:
                window = close_prices[i-period+1:i+1]
                avg = sum(window) / len(window)
                factor_values.append(avg)
        
        print(f"âœ… å…¬å¼å› å­è®¡ç®—å®Œæˆï¼Œè¿”å› {len(factor_values)} ä¸ªå€¼")
        return factor_values
    except Exception as e:
        print(f"âŒ å…¬å¼å› å­è®¡ç®—å¤±è´¥: {e}")
        return None

def calculate_ml_factor(factor_info, market_data, parameters):
    """è®¡ç®—MLå› å­"""
    try:
        print(f"ğŸ” è®¡ç®—MLå› å­: {factor_info.get('name')}")
        # è¿™é‡Œå¯ä»¥å®ç°MLæ¨¡å‹é¢„æµ‹
        # æš‚æ—¶è¿”å›éšæœºå€¼ä½œä¸ºç¤ºä¾‹
        close_prices = market_data.get('close', [])
        if not close_prices:
            print("âŒ æ²¡æœ‰æ”¶ç›˜ä»·æ•°æ®")
            return None
        
        import random
        factor_values = [random.uniform(-1, 1) for _ in range(len(close_prices))]
        print(f"âœ… MLå› å­è®¡ç®—å®Œæˆï¼Œè¿”å› {len(factor_values)} ä¸ªå€¼")
        return factor_values
    except Exception as e:
        print(f"âŒ MLå› å­è®¡ç®—å¤±è´¥: {e}")
        return None

def calculate_function_factor(factor_info, market_data, parameters):
    """è®¡ç®—å‡½æ•°å› å­"""
    try:
        print(f"ğŸ” è®¡ç®—å‡½æ•°å› å­: {factor_info.get('name')}")
        # å°è¯•å¯¼å…¥å¹¶è°ƒç”¨å› å­å‡½æ•°
        factor_name = factor_info.get('factor_id', '')
        if not factor_name:
            print("âŒ å› å­IDä¸ºç©º")
            return None
        
        # æ„å»ºå‡½æ•°æ–‡ä»¶è·¯å¾„
        function_file = FACTOR_LIBRARY_DIR / "functions" / f"{factor_name}.py"
        if not function_file.exists():
            print(f"âŒ å› å­å‡½æ•°æ–‡ä»¶ä¸å­˜åœ¨: {function_file}")
            return None
        
        print(f"ğŸ” æ‰¾åˆ°å› å­å‡½æ•°æ–‡ä»¶: {function_file}")
        
        # åŠ¨æ€å¯¼å…¥å› å­å‡½æ•°
        import importlib.util
        spec = importlib.util.spec_from_file_location(factor_name, function_file)
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        
        # å‡†å¤‡æ•°æ®
        df_data = pd.DataFrame({
            'open': market_data.get('open', []),
            'high': market_data.get('high', []),
            'low': market_data.get('low', []),
            'close': market_data.get('close', []),
            'volume': market_data.get('volume', [])
        })
        
        print(f"ğŸ” å‡†å¤‡æ•°æ®DataFrame: {df_data.shape}")
        
        # è°ƒç”¨calculateå‡½æ•°
        if hasattr(module, 'calculate'):
            print(f"ğŸ” è°ƒç”¨å› å­å‡½æ•°: {factor_name}.calculate()")
            factor_values = module.calculate(df_data, **parameters)
            
            if isinstance(factor_values, pd.Series):
                result = factor_values.tolist()
            elif isinstance(factor_values, (list, tuple)):
                result = list(factor_values)
            else:
                print(f"âŒ å› å­å‡½æ•°è¿”å›ç±»å‹ä¸æ”¯æŒ: {type(factor_values)}")
                return None
            
            print(f"âœ… å‡½æ•°å› å­è®¡ç®—å®Œæˆï¼Œè¿”å› {len(result)} ä¸ªå€¼")
            return result
        else:
            print(f"âŒ å› å­å‡½æ•° {factor_name} æ²¡æœ‰calculateå‡½æ•°")
            return None
            
    except Exception as e:
        print(f"âŒ å‡½æ•°å› å­è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None 