"""
æ•°æ®ç®¡ç†APIè·¯ç”±
"""

from flask import Blueprint, request, jsonify, current_app, session
import os
from pathlib import Path
import pandas as pd
from datetime import datetime, timedelta
import json
import ccxt  # æ·»åŠ  CCXT åº“
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

bp = Blueprint('data_api', __name__)

# å…¨å±€ä¸‹è½½ä»»åŠ¡å­˜å‚¨ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
DOWNLOADS = {}
DOWNLOADS_LOCK = threading.Lock()

@bp.route('/exchanges', methods=['GET'])
def get_exchanges():
    """è·å–æ”¯æŒçš„äº¤æ˜“æ‰€åˆ—è¡¨"""
    exchanges = [
        {
            'id': 'binance',
            'name': 'Binance',
            'type': 'cryptocurrency',
            'description': 'å…¨çƒæœ€å¤§çš„åŠ å¯†è´§å¸äº¤æ˜“æ‰€'
        },
        {
            'id': 'okx',
            'name': 'OKX',
            'type': 'cryptocurrency',
            'description': 'é¢†å…ˆçš„åŠ å¯†è´§å¸äº¤æ˜“å¹³å°'
        },
        {
            'id': 'bybit',
            'name': 'Bybit',
            'type': 'cryptocurrency',
            'description': 'ä¸“ä¸šçš„åŠ å¯†è´§å¸è¡ç”Ÿå“äº¤æ˜“æ‰€'
        }
    ]
    return jsonify({'success': True, 'data': exchanges})

def get_exchange_instance(exchange_id, is_futures=False):
    """è·å–äº¤æ˜“æ‰€å®ä¾‹"""
    exchange_class = getattr(ccxt, exchange_id)
    
    # åŸºç¡€é…ç½® - æ£€æµ‹ç¯å¢ƒå˜é‡ä¸­çš„ä»£ç†é…ç½®
    http_proxy = os.getenv('HTTP_PROXY')
    https_proxy = os.getenv('HTTPS_PROXY')
    proxies = {}
    if http_proxy:
        proxies['http'] = http_proxy
    if https_proxy:
        proxies['https'] = https_proxy

    options = {
        'enableRateLimit': True,
        'timeout': 30000,
        'proxies': proxies if proxies else None
    }
    
    # ä¸ºä¸åŒäº¤æ˜“æ‰€é…ç½®æœŸè´§å¸‚åœºé€‰é¡¹
    if is_futures:
        if exchange_id == 'binance':
            options.update({
                'defaultType': 'future',
                'urls': {
                    'api': {
                        'public': 'https://fapi.binance.com/fapi/v1',
                        'private': 'https://fapi.binance.com/fapi/v1',
                    }
                }
            })
        elif exchange_id == 'okx':
            options['defaultType'] = 'swap'
        elif exchange_id == 'bybit':
            options['defaultType'] = 'linear'
    else:
        if exchange_id == 'binance':
            options.update({
                'defaultType': 'spot',
                'urls': {
                    'api': {
                        'public': 'https://api.binance.com/api/v3',
                        'private': 'https://api.binance.com/api/v3',
                    }
                }
            })
        else:
            options['defaultType'] = 'spot'
    
    exchange = exchange_class(options)
    
    # è®¾ç½®è¯·æ±‚å¤´
    exchange.headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'
    }
    
    return exchange

def format_symbol(market, market_type='spot', exchange_id='binance'):
    """æ ¼å¼åŒ–äº¤æ˜“å¯¹ä¿¡æ¯"""
    base = market['base']
    quote = market['quote']
    
    # è·å–åˆçº¦åˆ°æœŸæ—¥ï¼ˆå¦‚æœæœ‰ï¼‰
    contract_type = market.get('info', {}).get('contractType', '')
    delivery_date = market.get('info', {}).get('deliveryDate', '')
    
    # æ ¹æ®ä¸åŒäº¤æ˜“æ‰€æ ¼å¼åŒ–äº¤æ˜“å¯¹åç§°
    if exchange_id == 'binance':
        # å¯¹äºæ°¸ç»­åˆçº¦ï¼Œä½¿ç”¨åŸºç¡€åç§°
        if contract_type == 'PERPETUAL':
            symbol = f"{base}_{quote}"
        # å¯¹äºäº¤å‰²åˆçº¦ï¼Œæ·»åŠ åˆ°æœŸæ—¥
        elif delivery_date:
            symbol = f"{base}_{quote}_{delivery_date}"
        else:
            symbol = f"{base}_{quote}"
    elif exchange_id == 'okx':
        symbol = f"{base}-{quote}"
    elif exchange_id == 'bybit':
        symbol = f"{base}{quote}"
    else:
        symbol = f"{base}_{quote}"
    
    result = {
        'symbol': symbol,
        'name': f"{base}/{quote}",
        'type': market_type,
        'base': base,
        'quote': quote,
        'active': market.get('active', True)
    }
    
    # æ·»åŠ åˆçº¦ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if contract_type:
        result['contract_type'] = contract_type
    if delivery_date:
        result['delivery_date'] = delivery_date
    
    return result

@bp.route('/symbols/<exchange>', methods=['GET'])
def get_symbols(exchange):
    """è·å–æŒ‡å®šäº¤æ˜“æ‰€çš„äº¤æ˜“å¯¹åˆ—è¡¨"""
    try:
        spot_markets = []
        perpetual_markets = []
        delivery_markets = []
        
        # è·å–ç°è´§å¸‚åœº
        try:
            print(f"\nå¼€å§‹è·å– {exchange} ç°è´§å¸‚åœºæ•°æ®...")
            spot_instance = get_exchange_instance(exchange, is_futures=False)
            spot_markets_data = spot_instance.load_markets()
            print(f"æˆåŠŸè·å–ç°è´§å¸‚åœºæ•°æ®ï¼Œå…± {len(spot_markets_data)} ä¸ªäº¤æ˜“å¯¹")
            
            # ç”¨äºå»é‡çš„é›†åˆ
            seen_symbols = set()
            
            for symbol, market in spot_markets_data.items():
                try:
                    # è°ƒè¯•æ¯ä¸ªå¸‚åœºçš„æ•°æ®ç»“æ„
                    print(f"\nå¤„ç†ç°è´§äº¤æ˜“å¯¹ {symbol}:")
                    print(f"  base: {market.get('base', 'N/A')}")
                    print(f"  quote: {market.get('quote', 'N/A')}")
                    print(f"  active: {market.get('active', 'N/A')}")
                    
                    if market.get('quote') == 'USDT' and market.get('active', True):
                        formatted = format_symbol(market, 'spot', exchange)
                        
                        # æ£€æŸ¥æ˜¯å¦å·²ç»æ·»åŠ è¿‡è¿™ä¸ªäº¤æ˜“å¯¹
                        if formatted['symbol'] not in seen_symbols:
                            seen_symbols.add(formatted['symbol'])
                            spot_markets.append(formatted)
                            print(f"  âœ… æ·»åŠ ç°è´§äº¤æ˜“å¯¹: {formatted['symbol']}")
                        else:
                            print(f"  âš ï¸ è·³è¿‡é‡å¤çš„ç°è´§äº¤æ˜“å¯¹: {formatted['symbol']}")
                    else:
                        print(f"  âŒ è·³è¿‡ç°è´§äº¤æ˜“å¯¹: quote={market.get('quote')}, active={market.get('active')}")
                except Exception as e:
                    print(f"  âŒ å¤„ç†ç°è´§äº¤æ˜“å¯¹ {symbol} æ—¶å‡ºé”™: {str(e)}")
                    continue
                    
        except Exception as e:
            print(f"è·å–ç°è´§å¸‚åœºå¤±è´¥: {str(e)}")
            print(f"é”™è¯¯ç±»å‹: {type(e)}")
            import traceback
            print(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        
        # è·å–æœŸè´§å¸‚åœº
        try:
            print(f"\nå¼€å§‹è·å– {exchange} æœŸè´§å¸‚åœºæ•°æ®...")
            futures_instance = get_exchange_instance(exchange, is_futures=True)
            futures_markets_data = futures_instance.load_markets()
            print(f"æˆåŠŸè·å–æœŸè´§å¸‚åœºæ•°æ®ï¼Œå…± {len(futures_markets_data)} ä¸ªäº¤æ˜“å¯¹")
            
            # åªè·å–æ°¸ç»­åˆçº¦ï¼Œè·³è¿‡äº¤å‰²åˆçº¦
            seen_perpetual_symbols = set()
            
            for symbol, market in futures_markets_data.items():
                try:
                    # è°ƒè¯•æ¯ä¸ªå¸‚åœºçš„æ•°æ®ç»“æ„
                    print(f"\nå¤„ç†æœŸè´§äº¤æ˜“å¯¹ {symbol}:")
                    print(f"  base: {market.get('base', 'N/A')}")
                    print(f"  quote: {market.get('quote', 'N/A')}")
                    print(f"  active: {market.get('active', 'N/A')}")
                    print(f"  contract_type: {market.get('info', {}).get('contractType', 'N/A')}")
                    
                    if market.get('quote') == 'USDT' and market.get('active', True):
                        contract_type = market.get('info', {}).get('contractType', '')
                        
                        # åªå¤„ç†æ°¸ç»­åˆçº¦ï¼Œè·³è¿‡äº¤å‰²åˆçº¦
                        if contract_type == 'PERPETUAL':
                            formatted = format_symbol(market, 'futures', exchange)
                            if formatted['symbol'] not in seen_perpetual_symbols:
                                seen_perpetual_symbols.add(formatted['symbol'])
                                perpetual_markets.append(formatted)
                                print(f"  âœ… æ·»åŠ æ°¸ç»­åˆçº¦: {formatted['symbol']}")
                            else:
                                print(f"  âš ï¸ è·³è¿‡é‡å¤çš„æ°¸ç»­åˆçº¦: {formatted['symbol']}")
                        else:
                            print(f"  â­ï¸ è·³è¿‡äº¤å‰²åˆçº¦: {symbol} (contract_type: {contract_type})")
                    else:
                        print(f"  âŒ è·³è¿‡æœŸè´§äº¤æ˜“å¯¹: quote={market.get('quote')}, active={market.get('active')}")
                except Exception as e:
                    print(f"  âŒ å¤„ç†æœŸè´§äº¤æ˜“å¯¹ {symbol} æ—¶å‡ºé”™: {str(e)}")
                    continue
                    
        except Exception as e:
            print(f"è·å–æœŸè´§å¸‚åœºå¤±è´¥: {str(e)}")
            print(f"é”™è¯¯ç±»å‹: {type(e)}")
            import traceback
            print(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        
        # æŒ‰äº¤æ˜“å¯¹åç§°æ’åº
        spot_markets.sort(key=lambda x: x['symbol'])
        perpetual_markets.sort(key=lambda x: x['symbol'])
        
        print(f"\næœ€ç»ˆç»“æœ:")
        print(f"âœ… è·å–åˆ° {len(spot_markets)} ä¸ªç°è´§äº¤æ˜“å¯¹")
        print(f"âœ… è·å–åˆ° {len(perpetual_markets)} ä¸ªæ°¸ç»­åˆçº¦")
        
        return jsonify({
            'success': True,
            'data': {
                'spot': spot_markets,
                'futures': perpetual_markets  # æœŸè´§ç›´æ¥è¿”å›æ°¸ç»­åˆçº¦
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'è·å–äº¤æ˜“å¯¹å¤±è´¥: {str(e)}'
        }), 500

@bp.route('/timeframes', methods=['GET'])
def get_timeframes():
    """è·å–æ”¯æŒçš„æ—¶é—´æ¡†æ¶"""
    timeframes = [
        {'value': '1m', 'name': '1åˆ†é’Ÿ', 'description': '1åˆ†é’ŸKçº¿æ•°æ®'},
        {'value': '3m', 'name': '3åˆ†é’Ÿ', 'description': '3åˆ†é’ŸKçº¿æ•°æ®'},
        {'value': '5m', 'name': '5åˆ†é’Ÿ', 'description': '5åˆ†é’ŸKçº¿æ•°æ®'},
        {'value': '15m', 'name': '15åˆ†é’Ÿ', 'description': '15åˆ†é’ŸKçº¿æ•°æ®'},
        {'value': '1h', 'name': '1å°æ—¶', 'description': '1å°æ—¶Kçº¿æ•°æ®'},
        {'value': '2h', 'name': '2å°æ—¶', 'description': '2å°æ—¶Kçº¿æ•°æ®'},
        {'value': '4h', 'name': '4å°æ—¶', 'description': '4å°æ—¶Kçº¿æ•°æ®'},
        {'value': '6h', 'name': '6å°æ—¶', 'description': '6å°æ—¶Kçº¿æ•°æ®'},
        {'value': '8h', 'name': '8å°æ—¶', 'description': '8å°æ—¶Kçº¿æ•°æ®'},
        {'value': '12h', 'name': '12å°æ—¶', 'description': '12å°æ—¶Kçº¿æ•°æ®'},
        {'value': '1d', 'name': '1å¤©', 'description': '1å¤©Kçº¿æ•°æ®'}
    ]
    return jsonify({'success': True, 'data': timeframes})

@bp.route('/local-data', methods=['GET'])
def get_local_data():
    """è·å–æœ¬åœ°å­˜å‚¨çš„æ•°æ®ä¿¡æ¯"""
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        exchange = request.args.get('exchange', 'binance')
        trade_type = request.args.get('trade_type', '')  # ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºæ‰€æœ‰ç±»å‹
        
        # æ„å»ºæ•°æ®ç›®å½•è·¯å¾„
        configured_data_dir = current_app.config.get('DATA_DIR', 'data')
        print(f"é…ç½®çš„DATA_DIR: {configured_data_dir}")
        
        # å¦‚æœé…ç½®çš„è·¯å¾„å·²ç»æŒ‡å‘å…·ä½“ç›®å½•ï¼Œåˆ™ä½¿ç”¨å…¶çˆ¶ç›®å½•
        if 'binance' in str(configured_data_dir) and ('futures' in str(configured_data_dir) or 'spot' in str(configured_data_dir)):
            base_data_dir = Path(configured_data_dir).parent.parent
        else:
            base_data_dir = Path(configured_data_dir)
        
        local_data = []
        
        # å¦‚æœæŒ‡å®šäº†ç‰¹å®šç±»å‹ï¼Œåªæ‰«æè¯¥ç±»å‹ç›®å½•
        if trade_type:
            search_dirs = [base_data_dir / exchange / trade_type]
        else:
            # æ‰«ææ‰€æœ‰ç±»å‹ç›®å½•
            search_dirs = [
                base_data_dir / exchange / 'futures',
                base_data_dir / exchange / 'spot',
                base_data_dir / exchange / 'perpetual',
                base_data_dir / exchange / 'delivery'
            ]
        
        print(f"åŸºç¡€æ•°æ®ç›®å½•: {base_data_dir}")
        print(f"æ‰«æç›®å½•: {[str(d) for d in search_dirs]}")
        
        for data_dir in search_dirs:
            if not data_dir.exists():
                print(f"ç›®å½•ä¸å­˜åœ¨: {data_dir}")
                continue
                
            print(f"æ‰«æç›®å½•: {data_dir}")
            print(f"ç›®å½•å†…å®¹: {list(data_dir.glob('*.feather'))}")
            
            for file_path in data_dir.glob('*.feather'):
                try:
                    # è§£ææ–‡ä»¶åè·å–ä¿¡æ¯
                    filename = file_path.stem
                    
                    # æ£€æµ‹æ•°æ®ç±»å‹å’Œè§£ææ–‡ä»¶å
                    data_type = 'unknown'
                    base_name = filename
                    timeframe_part = 'unknown'
                    
                    # å¤„ç†ä¸åŒçš„æ–‡ä»¶åæ ¼å¼
                    if filename.endswith('-futures'):
                        data_type = 'futures'
                        base_name = filename[:-8]
                    elif filename.endswith('-spot'):
                        data_type = 'spot'
                        base_name = filename[:-5]
                    elif filename.endswith('-perpetual'):
                        data_type = 'perpetual'
                        base_name = filename[:-10]
                    elif filename.endswith('-delivery'):
                        data_type = 'delivery'
                        base_name = filename[:-9]
                    
                    # æŸ¥æ‰¾æœ€åä¸€ä¸ªè¿å­—ç¬¦çš„ä½ç½®ï¼ˆåˆ†éš”äº¤æ˜“å¯¹å’Œæ—¶é—´æ¡†æ¶ï¼‰
                    last_hyphen = base_name.rfind('-')
                    if last_hyphen != -1:
                        symbol_part = base_name[:last_hyphen]
                        timeframe_part = base_name[last_hyphen + 1:]
                        
                        print(f"è§£ææ–‡ä»¶å: {filename}")
                        print(f"  data_type: {data_type}")
                        print(f"  base_name: {base_name}")
                        print(f"  symbol_part: {symbol_part}")
                        print(f"  timeframe_part: {timeframe_part}")
                        
                        # è§£æäº¤æ˜“å¯¹ (ä¾‹å¦‚: BTC_USDT_USDT -> BTC_USDT)
                        symbol_parts = symbol_part.split('_')
                        if len(symbol_parts) >= 2:
                            symbol = f"{symbol_parts[0]}_{symbol_parts[1]}"
                            
                            print(f"  æœ€ç»ˆè§£æç»“æœ: symbol={symbol}, timeframe={timeframe_part}, type={data_type}")
                            
                            # è¯»å–æ•°æ®è·å–åŸºæœ¬ä¿¡æ¯
                            df = pd.read_feather(file_path)
                            print(f"æ–‡ä»¶ {filename} çš„åˆ—å: {list(df.columns)}")
                            
                            # æ¨æ–­æ—¶é—´èŒƒå›´ï¼šä¼˜å…ˆåˆ—ï¼Œå…¶æ¬¡ç´¢å¼•
                            def to_datetime_series(series):
                                try:
                                    if pd.api.types.is_datetime64_any_dtype(series):
                                        return series
                                    if pd.api.types.is_numeric_dtype(series):
                                        # åˆ¤æ–­æ¯«ç§’/ç§’çº§
                                        s = series.dropna()
                                        if len(s) == 0:
                                            return pd.to_datetime(series, errors='coerce', unit='s')
                                        sample = s.iloc[0]
                                        unit = 'ms' if sample > 10_000_000_000 else 's'
                                        return pd.to_datetime(series, errors='coerce', unit=unit)
                                    # å­—ç¬¦ä¸²
                                    return pd.to_datetime(series, errors='coerce')
                                except Exception:
                                    return pd.to_datetime(series, errors='coerce')

                            start_ts = None
                            end_ts = None
                            cols_lower = {c.lower(): c for c in df.columns}
                            # å¸¸è§åˆ—å
                            open_cols = [name for key, name in cols_lower.items() if key in ['open_time', 'opentime', 'start_time', 'time', 'timestamp', 'datetime', 'date']]
                            close_cols = [name for key, name in cols_lower.items() if key in ['close_time', 'closetime', 'end_time', 'time', 'timestamp', 'datetime', 'date']]
                            cand_open = open_cols[0] if open_cols else None
                            cand_close = close_cols[0] if close_cols else None
                            if cand_open is not None:
                                s = to_datetime_series(df[cand_open])
                                if s.notna().any():
                                    start_ts = s.min()
                            if cand_close is not None:
                                s = to_datetime_series(df[cand_close])
                                if s.notna().any():
                                    end_ts = s.max()
                            # è‹¥ä»ä¸ºç©ºï¼Œå°è¯•ç´¢å¼•
                            if (start_ts is None or pd.isna(start_ts)) and hasattr(df.index, 'min'):
                                idx = df.index
                                try:
                                    if not pd.api.types.is_datetime64_any_dtype(idx):
                                        idx = to_datetime_series(pd.Series(idx))
                                    start_ts = idx.min()
                                except Exception:
                                    start_ts = None
                            if (end_ts is None or pd.isna(end_ts)) and hasattr(df.index, 'max'):
                                idx = df.index
                                try:
                                    if not pd.api.types.is_datetime64_any_dtype(idx):
                                        idx = to_datetime_series(pd.Series(idx))
                                    end_ts = idx.max()
                                except Exception:
                                    end_ts = None

                            # æ ¼å¼åŒ–ä¸º ISO å­—ç¬¦ä¸²
                            if start_ts is not None and not pd.isna(start_ts):
                                try:
                                    start_str = pd.to_datetime(start_ts).strftime('%Y-%m-%d')
                                except Exception:
                                    start_str = str(start_ts)
                            else:
                                start_str = ""
                            if end_ts is not None and not pd.isna(end_ts):
                                try:
                                    end_str = pd.to_datetime(end_ts).strftime('%Y-%m-%d')
                                except Exception:
                                    end_str = str(end_ts)
                            else:
                                end_str = ""
                            
                            data_info = {
                                'exchange': exchange,
                                'symbol': symbol,
                                'timeframe': timeframe_part,
                                'data_type': data_type,
                                'file_path': str(file_path),
                                'file_size': f"{file_path.stat().st_size / 1024 / 1024:.2f} MB",
                                'data_points': len(df),
                                'date_range': {
                                    'start': start_str,
                                    'end': end_str
                                },
                                'last_modified': datetime.fromtimestamp(file_path.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                            }
                            local_data.append(data_info)
                except Exception as e:
                    print(f"Error reading file {file_path}: {e}")
                    continue
        
        print(f"è¿”å›æ•°æ®æ¡æ•°: {len(local_data)}")
        print(f"è¿”å›æ•°æ®ç»“æ„ç¤ºä¾‹: {local_data[0] if local_data else 'No data'}")
        return jsonify({'success': True, 'data': local_data})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@bp.route('/view-data', methods=['POST'])
def view_data():
    """æŸ¥çœ‹æ•°æ®æ–‡ä»¶å†…å®¹"""
    print("ğŸ” view_data API å¼€å§‹æ‰§è¡Œ")
    try:
        data = request.get_json()
        print(f"ğŸ” æ¥æ”¶åˆ°çš„è¯·æ±‚æ•°æ®: {data}")
        
        file_path = data.get('file_path')
        print(f"ğŸ” æ–‡ä»¶è·¯å¾„: {file_path}")
        
        if not file_path:
            print("âŒ æ–‡ä»¶è·¯å¾„ä¸ºç©º")
            return jsonify({'success': False, 'error': 'æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º'})
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        print(f"ğŸ” æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(file_path)}")
        if not os.path.exists(file_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'})
        
        # è¯»å–Featheræ–‡ä»¶
        print("ğŸ” å¼€å§‹è¯»å–Featheræ–‡ä»¶...")
        df = pd.read_feather(file_path)
        print(f"ğŸ” æ–‡ä»¶è¯»å–æˆåŠŸï¼Œæ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"ğŸ” åˆ—å: {list(df.columns)}")
        print(f"ğŸ” æ•°æ®ç±»å‹: {df.dtypes.to_dict()}")
        
        # è§£ææ–‡ä»¶åè·å–åŸºæœ¬ä¿¡æ¯
        filename = Path(file_path).stem
        
        # è§£æäº¤æ˜“å¯¹å’Œæ—¶é—´æ¡†æ¶
        if filename.endswith('-futures'):
            base_name = filename[:-8]
            last_hyphen = base_name.rfind('-')
            if last_hyphen != -1:
                symbol_part = base_name[:last_hyphen]
                timeframe_part = base_name[last_hyphen + 1:]
                symbol_parts = symbol_part.split('_')
                if len(symbol_parts) >= 2:
                    symbol = f"{symbol_parts[0]}_{symbol_parts[1]}"
                    timeframe = timeframe_part
                else:
                    symbol = filename
                    timeframe = 'unknown'
            else:
                symbol = filename
                timeframe = 'unknown'
        else:
            symbol = filename
            timeframe = 'unknown'
        
        # å‡†å¤‡OHLCVæ•°æ®
        print("ğŸ” å¼€å§‹å‡†å¤‡OHLCVæ•°æ®...")
        ohlcv_data = []
        
        # ç¡®å®šæ—¶é—´åˆ—å’ŒOHLCVåˆ—
        time_col = None
        ohlcv_cols = {}
        
        # æŸ¥æ‰¾æ—¶é—´åˆ—
        print("ğŸ” å¼€å§‹æŸ¥æ‰¾æ—¶é—´åˆ—...")
        for col in df.columns:
            col_lower = col.lower()
            print(f"ğŸ” æ£€æŸ¥åˆ—: {col} (å°å†™: {col_lower})")
            if any(keyword in col_lower for keyword in ['time', 'date', 'timestamp', 'datetime']):
                time_col = col
                print(f"âœ… æ‰¾åˆ°æ—¶é—´åˆ—: {col}")
                break
        
        print(f"ğŸ” æœ€ç»ˆç¡®å®šçš„æ—¶é—´åˆ—: {time_col}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¶é—´åˆ—ï¼Œå°è¯•ä½¿ç”¨ç´¢å¼•
        if time_col is None and df.index.name:
            time_col = df.index.name
            df = df.reset_index()
        
        # æŸ¥æ‰¾OHLCVåˆ—
        print("ğŸ” å¼€å§‹æŸ¥æ‰¾OHLCVåˆ—...")
        for col in df.columns:
            col_lower = col.lower()
            print(f"ğŸ” æ£€æŸ¥åˆ—: {col} (å°å†™: {col_lower})")
            if 'open' in col_lower:
                ohlcv_cols['open'] = col
                print(f"âœ… æ‰¾åˆ°å¼€ç›˜ä»·åˆ—: {col}")
            elif 'high' in col_lower:
                ohlcv_cols['high'] = col
                print(f"âœ… æ‰¾åˆ°æœ€é«˜ä»·åˆ—: {col}")
            elif 'low' in col_lower:
                ohlcv_cols['low'] = col
                print(f"âœ… æ‰¾åˆ°æœ€ä½ä»·åˆ—: {col}")
            elif 'close' in col_lower:
                ohlcv_cols['close'] = col
                print(f"âœ… æ‰¾åˆ°æ”¶ç›˜ä»·åˆ—: {col}")
            elif 'volume' in col_lower:
                ohlcv_cols['volume'] = col
                print(f"âœ… æ‰¾åˆ°æˆäº¤é‡åˆ—: {col}")
        
        print(f"ğŸ” æ‰¾åˆ°çš„OHLCVåˆ—æ˜ å°„: {ohlcv_cols}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†åˆ—åï¼Œå°è¯•å…¶ä»–å¸¸è§åˆ—å
        if not ohlcv_cols.get('open'):
            for col in df.columns:
                if any(keyword in col.lower() for keyword in ['o', 'op', 'open']):
                    ohlcv_cols['open'] = col
                    break
        
        if not ohlcv_cols.get('high'):
            for col in df.columns:
                if any(keyword in col.lower() for keyword in ['h', 'hi', 'high']):
                    ohlcv_cols['high'] = col
                    break
        
        if not ohlcv_cols.get('low'):
            for col in df.columns:
                if any(keyword in col.lower() for keyword in ['l', 'lo', 'low']):
                    ohlcv_cols['low'] = col
                    break
        
        if not ohlcv_cols.get('close'):
            for col in df.columns:
                if any(keyword in col.lower() for keyword in ['c', 'cl', 'close']):
                    ohlcv_cols['close'] = col
                    break
        
        if not ohlcv_cols.get('volume'):
            for col in df.columns:
                if any(keyword in col.lower() for keyword in ['v', 'vol', 'volume']):
                    ohlcv_cols['volume'] = col
                    break
        
        # æ„å»ºOHLCVæ•°æ®
        print("ğŸ” å¼€å§‹æ„å»ºOHLCVæ•°æ®...")
        print(f"ğŸ” æ•°æ®è¡Œæ•°: {len(df)}")
        print(f"ğŸ” æ—¶é—´åˆ—: {time_col}")
        print(f"ğŸ” OHLCVåˆ—æ˜ å°„: {ohlcv_cols}")
        
        for idx, row in df.iterrows():
            if idx < 5:  # åªæ‰“å°å‰5è¡Œçš„è°ƒè¯•ä¿¡æ¯
                print(f"ğŸ” å¤„ç†ç¬¬{idx}è¡Œæ•°æ®...")
            
            # å¤„ç†æ—¶é—´
            timestamp = None
            if time_col and time_col in df.columns:
                try:
                    if pd.api.types.is_datetime64_any_dtype(df[time_col]):
                        timestamp = df[time_col].iloc[idx]
                        if idx < 5:
                            print(f"ğŸ” ç¬¬{idx}è¡Œæ—¶é—´(åŸå§‹): {df[time_col].iloc[idx]}")
                    elif pd.api.types.is_numeric_dtype(df[time_col]):
                        # åˆ¤æ–­æ˜¯ç§’è¿˜æ˜¯æ¯«ç§’
                        sample = df[time_col].iloc[0]
                        unit = 'ms' if sample > 10_000_000_000 else 's'
                        timestamp = pd.to_datetime(df[time_col].iloc[idx], unit=unit)
                        if idx < 5:
                            print(f"ğŸ” ç¬¬{idx}è¡Œæ—¶é—´(è½¬æ¢): {timestamp}, å•ä½: {unit}")
                    else:
                        timestamp = pd.to_datetime(df[time_col].iloc[idx])
                        if idx < 5:
                            print(f"ğŸ” ç¬¬{idx}è¡Œæ—¶é—´(å­—ç¬¦ä¸²): {timestamp}")
                except Exception as e:
                    print(f"âŒ ç¬¬{idx}è¡Œæ—¶é—´å¤„ç†å¤±è´¥: {e}")
                    timestamp = pd.Timestamp.now()
            else:
                timestamp = pd.Timestamp.now()
                if idx < 5:
                    print(f"ğŸ” ç¬¬{idx}è¡Œä½¿ç”¨é»˜è®¤æ—¶é—´: {timestamp}")
            
            # å¤„ç†OHLCVæ•°æ®
            ohlcv_item = {}
            
            # å¼€ç›˜ä»·
            if ohlcv_cols.get('open') and ohlcv_cols['open'] in df.columns:
                try:
                    ohlcv_item['open'] = float(row[ohlcv_cols['open']])
                    if idx < 5:
                        print(f"ğŸ” ç¬¬{idx}è¡Œå¼€ç›˜ä»·: {ohlcv_item['open']}")
                except:
                    ohlcv_item['open'] = None
            else:
                ohlcv_item['open'] = None
            
            # æœ€é«˜ä»·
            if ohlcv_cols.get('high') and ohlcv_cols['high'] in df.columns:
                try:
                    ohlcv_item['high'] = float(row[ohlcv_cols['high']])
                    if idx < 5:
                        print(f"ğŸ” ç¬¬{idx}è¡Œæœ€é«˜ä»·: {ohlcv_item['high']}")
                except:
                    ohlcv_item['high'] = None
            else:
                ohlcv_item['high'] = None
            
            # æœ€ä½ä»·
            if ohlcv_cols.get('low') and ohlcv_cols['low'] in df.columns:
                try:
                    ohlcv_item['low'] = float(row[ohlcv_cols['low']])
                    if idx < 5:
                        print(f"ğŸ” ç¬¬{idx}è¡Œæœ€ä½ä»·: {ohlcv_item['low']}")
                except:
                    ohlcv_item['low'] = None
            else:
                ohlcv_item['low'] = None
            
            # æ”¶ç›˜ä»·
            if ohlcv_cols.get('close') and ohlcv_cols['close'] in df.columns:
                try:
                    ohlcv_item['close'] = float(row[ohlcv_cols['close']])
                    if idx < 5:
                        print(f"ğŸ” ç¬¬{idx}è¡Œæ”¶ç›˜ä»·: {ohlcv_item['close']}")
                except:
                    ohlcv_item['close'] = None
            else:
                ohlcv_item['close'] = None
            
            # æˆäº¤é‡
            if ohlcv_cols.get('volume') and ohlcv_cols['volume'] in df.columns:
                try:
                    ohlcv_item['volume'] = float(row[ohlcv_cols['volume']])
                    if idx < 5:
                        print(f"ğŸ” ç¬¬{idx}è¡Œæˆäº¤é‡: {ohlcv_item['volume']}")
                except:
                    ohlcv_item['volume'] = None
            else:
                ohlcv_item['volume'] = None
            
            # æ·»åŠ æ—¶é—´æˆ³ - ç¡®ä¿ä½¿ç”¨UTCæ—¶åŒº
            if timestamp:
                # å¦‚æœæ—¶é—´æœ‰æ—¶åŒºä¿¡æ¯ï¼Œè½¬æ¢ä¸ºUTC
                if timestamp.tz is not None:
                    timestamp_utc = timestamp.tz_convert('UTC')
                else:
                    # å¦‚æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œå‡è®¾æ˜¯UTC
                    timestamp_utc = timestamp.tz_localize('UTC')
                
                ohlcv_item['timestamp'] = timestamp_utc.isoformat()
                if idx < 5:
                    print(f"ğŸ” ç¬¬{idx}è¡Œæ—¶é—´(UTC): {ohlcv_item['timestamp']}")
            else:
                ohlcv_item['timestamp'] = None
            
            if idx < 5:
                print(f"ğŸ” ç¬¬{idx}è¡Œå®Œæ•´OHLCVæ•°æ®: {ohlcv_item}")
            
            ohlcv_data.append(ohlcv_item)
        
        print(f"ğŸ” æ„å»ºå®Œæˆï¼Œå…±{len(ohlcv_data)}æ¡OHLCVæ•°æ®")
        if len(ohlcv_data) > 0:
            print(f"ğŸ” ç¬¬ä¸€æ¡æ•°æ®ç¤ºä¾‹: {ohlcv_data[0]}")
            print(f"ğŸ” æœ€åä¸€æ¡æ•°æ®ç¤ºä¾‹: {ohlcv_data[-1]}")
        
        # æŒ‰æ—¶é—´æ’åº
        ohlcv_data.sort(key=lambda x: x['timestamp'] if x['timestamp'] else '')
        
        # å‡†å¤‡è¿”å›æ•°æ®
        print("ğŸ” å‡†å¤‡è¿”å›æ•°æ®...")
        result_data = {
            'symbol': symbol,
            'timeframe': timeframe,
            'file_path': file_path,
            'file_size': f"{Path(file_path).stat().st_size / 1024 / 1024:.2f} MB",
            'last_modified': datetime.fromtimestamp(Path(file_path).stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
            'ohlcv_data': ohlcv_data,
            'columns_found': list(df.columns),
            'ohlcv_columns_mapped': ohlcv_cols
        }
        
        print(f"ğŸ” è¿”å›æ•°æ®æ‘˜è¦:")
        print(f"  - äº¤æ˜“å¯¹: {result_data['symbol']}")
        print(f"  - æ—¶é—´æ¡†æ¶: {result_data['timeframe']}")
        print(f"  - OHLCVæ•°æ®æ¡æ•°: {len(result_data['ohlcv_data'])}")
        print(f"  - æ‰¾åˆ°çš„åˆ—: {result_data['columns_found']}")
        print(f"  - OHLCVåˆ—æ˜ å°„: {result_data['ohlcv_columns_mapped']}")
        
        print("âœ… view_data API æ‰§è¡ŒæˆåŠŸï¼Œå‡†å¤‡è¿”å›æ•°æ®")
        return jsonify({'success': True, 'data': result_data})
        
    except Exception as e:
        import traceback
        print(f"æŸ¥çœ‹æ•°æ®å¤±è´¥: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return jsonify({'success': False, 'error': f'æŸ¥çœ‹æ•°æ®å¤±è´¥: {str(e)}'})

@bp.route('/download', methods=['POST'])
def start_download():
    """å¼€å§‹ä¸‹è½½æ•°æ®"""
    try:
        data = request.get_json()
        exchange = data.get('exchange')
        symbol = data.get('symbol')
        timeframe = data.get('timeframe')
        start_date = data.get('start_date')
        end_date = data.get('end_date')
        trade_type = data.get('trade_type', 'spot')  # é»˜è®¤ä¸ºç°è´§
        
        # ç”Ÿæˆä¸‹è½½ID
        download_id = f"{exchange}_{symbol}_{timeframe}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # åˆå§‹åŒ–ä¸‹è½½çŠ¶æ€åˆ°å…¨å±€å­˜å‚¨
        with DOWNLOADS_LOCK:
            DOWNLOADS[download_id] = {
                'id': download_id,
            'exchange': exchange,
            'symbol': symbol,
            'timeframe': timeframe,
            'start_date': start_date,
            'end_date': end_date,
                'trade_type': trade_type,
                'status': 'starting',
                'progress': 0,
                'message': 'æ­£åœ¨åˆå§‹åŒ–ä¸‹è½½...',
                'start_time': datetime.now().isoformat()
            }
        
        # å¯åŠ¨åå°ä¸‹è½½ä»»åŠ¡
        def download_task():
            try:
                from factor_miner.core.batch_downloader import batch_downloader
                
                # æ›´æ–°çŠ¶æ€
                with DOWNLOADS_LOCK:
                    if download_id in DOWNLOADS:
                        DOWNLOADS[download_id]['status'] = 'downloading'
                        DOWNLOADS[download_id]['message'] = 'æ­£åœ¨åˆå§‹åŒ–åˆ†æ‰¹ä¸‹è½½...'
                
                # è®¾ç½®äº¤æ˜“ç±»å‹
                batch_downloader.trade_type = trade_type
                
                # å¼€å§‹åˆ†æ‰¹ä¸‹è½½ - ä½¿ç”¨æ™ºèƒ½åˆ†æ‰¹ä¸‹è½½å™¨
                # ä¿®å¤äº¤æ˜“å¯¹æ ¼å¼ï¼šBTC_USDT -> BTC/USDT
                formatted_symbol = symbol.replace('_', '/')
                
                # è®¡ç®—åˆ†æ‰¹ä¿¡æ¯
                from datetime import datetime
                start_dt = datetime.strptime(start_date, '%Y-%m-%d')
                end_dt = datetime.strptime(end_date, '%Y-%m-%d')
                total_days = (end_dt - start_dt).days
                
                with DOWNLOADS_LOCK:
                    if download_id in DOWNLOADS:
                        DOWNLOADS[download_id]['message'] = f'å¼€å§‹åˆ†æ‰¹ä¸‹è½½ï¼Œæ€»å¤©æ•°: {total_days} å¤©'
                
                result = batch_downloader.download_ohlcv_batch(
                    config_id=None,  # ä¸ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼Œç›´æ¥åˆ›å»ºå®ä¾‹
                    symbol=formatted_symbol,
                    timeframe=timeframe,
                    start_date=start_date,
                    end_date=end_date,
                    trade_type=trade_type,
                    progress_callback=lambda progress, message: update_download_progress(download_id, progress, message)
                )
                
                if result.get('success'):
                    with DOWNLOADS_LOCK:
                        if download_id in DOWNLOADS:
                            DOWNLOADS[download_id]['status'] = 'completed'
                            DOWNLOADS[download_id]['progress'] = 100
                            DOWNLOADS[download_id]['message'] = f'ä¸‹è½½å®Œæˆï¼å…± {result.get("total_records", 0)} æ¡æ•°æ®'
                            DOWNLOADS[download_id]['file_path'] = result.get('file_path', '')
                else:
                    with DOWNLOADS_LOCK:
                        if download_id in DOWNLOADS:
                            DOWNLOADS[download_id]['status'] = 'failed'
                            DOWNLOADS[download_id]['message'] = f'ä¸‹è½½å¤±è´¥: {result.get("error", "æœªçŸ¥é”™è¯¯")}'
                    
            except Exception as e:
                with DOWNLOADS_LOCK:
                    if download_id in DOWNLOADS:
                        DOWNLOADS[download_id]['status'] = 'failed'
                        DOWNLOADS[download_id]['message'] = f'ä¸‹è½½å¼‚å¸¸: {str(e)}'
                print(f"ä¸‹è½½ä»»åŠ¡å¼‚å¸¸: {e}")
        
        # å¯åŠ¨åå°çº¿ç¨‹
        thread = threading.Thread(target=download_task)
        thread.daemon = True
        thread.start()
        
        with DOWNLOADS_LOCK:
            return jsonify({'success': True, 'data': DOWNLOADS[download_id]})
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@bp.route('/downloads', methods=['GET'])
def get_all_downloads():
    """è·å–æ‰€æœ‰ä¸‹è½½ä»»åŠ¡çš„çŠ¶æ€"""
    try:
        with DOWNLOADS_LOCK:
            # è¿”å›æ‰€æœ‰ä¸‹è½½ä»»åŠ¡çš„çŠ¶æ€
            downloads = list(DOWNLOADS.values())
            return jsonify({'success': True, 'data': downloads})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@bp.route('/batch-download', methods=['POST'])
def start_batch_download():
    """å¼€å§‹æ‰¹é‡ä¸‹è½½æ•°æ®"""
    try:
        data = request.get_json()
        exchange = data.get('exchange')
        symbols = data.get('symbols', [])  # äº¤æ˜“å¯¹åˆ—è¡¨
        timeframes = data.get('timeframes', [])  # æ—¶é—´æ¡†æ¶åˆ—è¡¨
        start_date = data.get('start_date')
        end_date = data.get('end_date')
        trade_type = data.get('trade_type', 'spot')  # é»˜è®¤ä¸ºç°è´§
        
        if not symbols or not timeframes:
            return jsonify({'success': False, 'error': 'è¯·é€‰æ‹©äº¤æ˜“å¯¹å’Œæ—¶é—´æ¡†æ¶'})
        
        # è®¡ç®—æ€»ä»»åŠ¡æ•°
        total_tasks = len(symbols) * len(timeframes)
        print(f"å¼€å§‹æ‰¹é‡ä¸‹è½½: {len(symbols)} ä¸ªäº¤æ˜“å¯¹ Ã— {len(timeframes)} ä¸ªæ—¶é—´æ¡†æ¶ = {total_tasks} ä¸ªä»»åŠ¡")
        
        # åˆ›å»ºæ‰¹é‡ä¸‹è½½ä»»åŠ¡ID
        batch_id = f"batch_{exchange}_{trade_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # åˆå§‹åŒ–æ‰¹é‡ä¸‹è½½çŠ¶æ€
        with DOWNLOADS_LOCK:
            DOWNLOADS[batch_id] = {
                'id': batch_id,
                'type': 'batch',
                'exchange': exchange,
                'symbols': symbols,
                'timeframes': timeframes,
                'start_date': start_date,
                'end_date': end_date,
                'trade_type': trade_type,
                'status': 'starting',
            'progress': 0,
                'message': f'æ­£åœ¨åˆå§‹åŒ–æ‰¹é‡ä¸‹è½½ï¼Œå…± {total_tasks} ä¸ªä»»åŠ¡...',
                'start_time': datetime.now().isoformat(),
                'total_tasks': total_tasks,
                'completed_tasks': 0,
                'failed_tasks': 0,
                'task_results': []
            }
        
        # å¯åŠ¨åå°æ‰¹é‡ä¸‹è½½ä»»åŠ¡
        def batch_download_task():
            try:
                from factor_miner.core.batch_downloader import batch_downloader
                
                # æ›´æ–°çŠ¶æ€ä¸ºä¸‹è½½ä¸­
                with DOWNLOADS_LOCK:
                    if batch_id in DOWNLOADS:
                        DOWNLOADS[batch_id]['status'] = 'downloading'
                        DOWNLOADS[batch_id]['message'] = f'å¼€å§‹æ‰¹é‡ä¸‹è½½ï¼Œå…± {total_tasks} ä¸ªä»»åŠ¡'
                        print(f"æ‰¹é‡ä¸‹è½½çŠ¶æ€æ›´æ–°: {batch_id} -> downloading")
                
                # è®¾ç½®äº¤æ˜“ç±»å‹
                batch_downloader.trade_type = trade_type
                
                completed_count = 0
                failed_count = 0
                
                # éå†æ‰€æœ‰äº¤æ˜“å¯¹å’Œæ—¶é—´æ¡†æ¶ç»„åˆ
                for symbol in symbols:
                    for timeframe in timeframes:
                        task_id = f"{symbol}_{timeframe}"
                        
                        try:
                            # æ›´æ–°å½“å‰ä»»åŠ¡çŠ¶æ€
                            with DOWNLOADS_LOCK:
                                if batch_id in DOWNLOADS:
                                    current_task = completed_count + failed_count + 1
                                    progress = int((current_task - 1) / total_tasks * 100)
                                    DOWNLOADS[batch_id]['progress'] = progress
                                    DOWNLOADS[batch_id]['message'] = f'æ­£åœ¨ä¸‹è½½ {symbol} {timeframe} ({current_task}/{total_tasks})'
                                    print(f"æ‰¹é‡ä¸‹è½½è¿›åº¦æ›´æ–°: {batch_id} -> {progress}% - {symbol} {timeframe}")
                            
                            # ä¿®å¤äº¤æ˜“å¯¹æ ¼å¼ï¼šBTC_USDT -> BTC/USDT
                            formatted_symbol = symbol.replace('_', '/')
                            
                            # ä½¿ç”¨ç°æœ‰çš„ä¸‹è½½é€»è¾‘
                            result = batch_downloader.download_ohlcv_batch(
                                config_id=None,
                                symbol=formatted_symbol,
                                timeframe=timeframe,
                                start_date=start_date,
                                end_date=end_date,
                                trade_type=trade_type,
                                progress_callback=None  # æ‰¹é‡ä¸‹è½½æ—¶ä¸ä½¿ç”¨è¿›åº¦å›è°ƒ
                            )
                            
                            # è®°å½•ä»»åŠ¡ç»“æœ
                            task_result = {
                                'task_id': task_id,
                                'symbol': symbol,
                                'timeframe': timeframe,
                                'success': result.get('success', False),
                                'message': result.get('message', 'æœªçŸ¥çŠ¶æ€'),
                                'records': result.get('total_records', 0),
                                'file_path': result.get('file_path', '')
                            }
                            
                            if result.get('success'):
                                completed_count += 1
                                task_result['status'] = 'completed'
                                print(f"âœ… ä»»åŠ¡å®Œæˆ: {symbol} {timeframe}")
                            else:
                                failed_count += 1
                                task_result['status'] = 'failed'
                                print(f"âŒ ä»»åŠ¡å¤±è´¥: {symbol} {timeframe} - {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                            
                            # æ›´æ–°æ‰¹é‡ä¸‹è½½çŠ¶æ€
                            with DOWNLOADS_LOCK:
                                if batch_id in DOWNLOADS:
                                    DOWNLOADS[batch_id]['task_results'].append(task_result)
                                    DOWNLOADS[batch_id]['completed_tasks'] = completed_count
                                    DOWNLOADS[batch_id]['failed_tasks'] = failed_count
                                    progress = int((completed_count + failed_count) / total_tasks * 100)
                                    DOWNLOADS[batch_id]['progress'] = progress
                                    print(f"æ‰¹é‡ä¸‹è½½ä»»åŠ¡å®Œæˆ: {batch_id} -> è¿›åº¦ {progress}% - å®Œæˆ {completed_count}, å¤±è´¥ {failed_count}")
                            
                        except Exception as e:
                            failed_count += 1
                            print(f"âŒ ä»»åŠ¡å¼‚å¸¸: {symbol} {timeframe} - {str(e)}")
                            
                            # è®°å½•å¤±è´¥çš„ä»»åŠ¡
                            task_result = {
                                'task_id': task_id,
                                'symbol': symbol,
                                'timeframe': timeframe,
                                'success': False,
                                'message': f'ä»»åŠ¡å¼‚å¸¸: {str(e)}',
                                'records': 0,
                                'file_path': '',
                                'status': 'failed'
                            }
                            
                            with DOWNLOADS_LOCK:
                                if batch_id in DOWNLOADS:
                                    DOWNLOADS[batch_id]['task_results'].append(task_result)
                                    DOWNLOADS[batch_id]['failed_tasks'] = failed_count
                                    progress = int((completed_count + failed_count) / total_tasks * 100)
                                    DOWNLOADS[batch_id]['progress'] = progress
                                    print(f"æ‰¹é‡ä¸‹è½½ä»»åŠ¡å¼‚å¸¸: {batch_id} -> è¿›åº¦ {progress}% - å®Œæˆ {completed_count}, å¤±è´¥ {failed_count}")
                
                # æ‰¹é‡ä¸‹è½½å®Œæˆ
                with DOWNLOADS_LOCK:
                    if batch_id in DOWNLOADS:
                        DOWNLOADS[batch_id]['status'] = 'completed'
                        DOWNLOADS[batch_id]['progress'] = 100
                        DOWNLOADS[batch_id]['message'] = f'æ‰¹é‡ä¸‹è½½å®Œæˆï¼æˆåŠŸ: {completed_count}, å¤±è´¥: {failed_count}'
                        print(f"æ‰¹é‡ä¸‹è½½æœ€ç»ˆå®Œæˆ: {batch_id} -> completed - æˆåŠŸ {completed_count}, å¤±è´¥ {failed_count}")
                
                print(f"æ‰¹é‡ä¸‹è½½å®Œæˆ: æˆåŠŸ {completed_count}, å¤±è´¥ {failed_count}")
                
            except Exception as e:
                with DOWNLOADS_LOCK:
                    if batch_id in DOWNLOADS:
                        DOWNLOADS[batch_id]['status'] = 'failed'
                        DOWNLOADS[batch_id]['message'] = f'æ‰¹é‡ä¸‹è½½å¼‚å¸¸: {str(e)}'
                print(f"æ‰¹é‡ä¸‹è½½ä»»åŠ¡å¼‚å¸¸: {e}")
        
        # å¯åŠ¨åå°çº¿ç¨‹
        thread = threading.Thread(target=batch_download_task)
        thread.daemon = True
        thread.start()
        
        with DOWNLOADS_LOCK:
            return jsonify({'success': True, 'data': DOWNLOADS[batch_id]})
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

def update_download_progress(download_id, progress, message):
    """æ›´æ–°ä¸‹è½½è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    with DOWNLOADS_LOCK:
        if download_id in DOWNLOADS:
            DOWNLOADS[download_id]['progress'] = progress
            DOWNLOADS[download_id]['message'] = message

@bp.route('/download-status/<download_id>', methods=['GET'])
def get_download_status(download_id):
    """è·å–ä¸‹è½½çŠ¶æ€"""
    try:
        # ä»å…¨å±€å­˜å‚¨è·å–ä¸‹è½½çŠ¶æ€
        with DOWNLOADS_LOCK:
            if download_id not in DOWNLOADS:
                return jsonify({
                    'success': False,
                    'error': 'ä¸‹è½½ä»»åŠ¡ä¸å­˜åœ¨'
                }), 404
            download_info = dict(DOWNLOADS[download_id])
        
        # è®¡ç®—ä¸‹è½½é€Ÿåº¦ï¼ˆå¦‚æœæ­£åœ¨ä¸‹è½½ï¼‰
        if download_info['status'] == 'downloading' and 'start_time' in download_info:
            start_time = datetime.fromisoformat(download_info['start_time'])
            elapsed = (datetime.now() - start_time).total_seconds()
            if elapsed > 0:
                speed = f"{download_info['progress'] / elapsed * 100:.1f} %/s"
            else:
                speed = "è®¡ç®—ä¸­..."
        else:
            speed = "N/A"
        
        status_info = {
            'id': download_id,
            'progress': download_info['progress'],
            'status': download_info['status'],
            'message': download_info['message'],
            'exchange': download_info.get('exchange', ''),
            'symbol': download_info.get('symbol', ''),
            'timeframe': download_info.get('timeframe', ''),
            'start_date': download_info.get('start_date', ''),
            'end_date': download_info.get('end_date', ''),
            'trade_type': download_info.get('trade_type', ''),
            'file_path': download_info.get('file_path', ''),
            'download_speed': speed
        }
        
        return jsonify({'success': True, 'data': status_info})
        
    except Exception as e:
        return jsonify({
            'success': False, 
            'error': f'è·å–ä¸‹è½½çŠ¶æ€å¤±è´¥: {str(e)}'
        }), 500

@bp.route('/download-suggestions', methods=['POST'])
def get_download_suggestions():
    """è·å–ä¸‹è½½å»ºè®®"""
    try:
        data = request.get_json()
        exchange = data.get('exchange')
        trade_type = data.get('trade_type')
        symbol = data.get('symbol')
        timeframe = data.get('timeframe')
        
        # è·å–æœ¬åœ°æ•°æ®
        local_data = []
        
        # æ„å»ºæ•°æ®ç›®å½•è·¯å¾„
        search_dirs = []
        
        if trade_type == 'futures':
            # æœŸè´§ç±»å‹ï¼šæ£€æŸ¥ DATA_DIR æ˜¯å¦å·²ç»æ˜¯ futures ç›®å½•
            base_dir = Path(current_app.config.get('DATA_DIR', 'data'))
            print(f"DATA_DIR é…ç½®: {base_dir}")
            
            # å¦‚æœ DATA_DIR å·²ç»æ˜¯ futures ç›®å½•ï¼Œç›´æ¥ä½¿ç”¨
            if base_dir.name == 'futures':
                search_dirs.append(base_dir)
                print(f"æœŸè´§ç±»å‹ï¼šDATA_DIR å·²æ˜¯ futures ç›®å½•ï¼Œä½¿ç”¨ {base_dir}")
            else:
                # å¦åˆ™æ„å»ºæ ‡å‡†è·¯å¾„
                futures_dir = base_dir / exchange / 'futures'
                search_dirs.append(futures_dir)
                print(f"æœŸè´§ç±»å‹ï¼šæ„å»ºæ ‡å‡†è·¯å¾„ {futures_dir}")
        elif trade_type in ['perpetual', 'delivery']:
            # æ°¸ç»­æˆ–äº¤å‰²åˆçº¦ï¼šå…ˆåœ¨æ–°ç›®å½•ä¸­æŸ¥æ‰¾ï¼Œå†åœ¨æ—§çš„ futures ç›®å½•ä¸­æŸ¥æ‰¾
            data_dir = Path(current_app.config.get('DATA_DIR', 'data')) / exchange / trade_type
            search_dirs.append(data_dir)
            
            futures_dir = Path(current_app.config.get('DATA_DIR', 'data')) / exchange / 'futures'
            if futures_dir.exists():
                search_dirs.append(futures_dir)
                print(f"å‘åå…¼å®¹ï¼šåŒæ—¶åœ¨ {futures_dir} ç›®å½•ä¸­æŸ¥æ‰¾æ•°æ®")
        else:
            # ç°è´§ç­‰å…¶ä»–ç±»å‹
            data_dir = Path(current_app.config.get('DATA_DIR', 'data')) / exchange / trade_type
            search_dirs.append(data_dir)
        
        # åœ¨æ‰€æœ‰ç›¸å…³ç›®å½•ä¸­æŸ¥æ‰¾æ•°æ®
        for search_dir in search_dirs:
            if search_dir.exists():
                print(f"åœ¨ç›®å½• {search_dir} ä¸­æŸ¥æ‰¾æ•°æ®æ–‡ä»¶...")
                # è°ƒè¯•ï¼šæ‰“å°åŒ¹é…æ¨¡å¼
                glob_pattern = f"{symbol}*{timeframe}*.feather"
                print(f"ä½¿ç”¨åŒ¹é…æ¨¡å¼: {glob_pattern}")
                
                # ä½¿ç”¨æ›´ç²¾ç¡®çš„åŒ¹é…æ¨¡å¼ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…
                # ä¾‹å¦‚ï¼š2h ä¸åº”è¯¥åŒ¹é…åˆ° 12h
                for file_path in search_dir.glob(f"{symbol}*{timeframe}*.feather"):
                    # éªŒè¯æ–‡ä»¶åæ˜¯å¦çœŸæ­£åŒ¹é…æŒ‡å®šçš„ timeframe
                    filename = file_path.name
                    if timeframe not in filename or f"-{timeframe}-" not in filename:
                        # è·³è¿‡ä¸åŒ¹é…çš„æ–‡ä»¶
                        print(f"è·³è¿‡ä¸åŒ¹é…çš„æ–‡ä»¶: {filename} (æœŸæœ›: {timeframe})")
                        continue
                    try:
                        print(f"æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {file_path}")
                        df = pd.read_feather(file_path)
                        # è·å–æ—¶é—´èŒƒå›´ - æ”¯æŒå¤šç§æ—¶é—´åˆ—å
                        time_col = None
                        for col in df.columns:
                            if col.lower() in ['date', 'time', 'datetime', 'timestamp']:
                                time_col = col
                                break
                        
                        if time_col:
                            start_date = pd.to_datetime(df[time_col].min()).strftime('%Y-%m-%d')
                            end_date = pd.to_datetime(df[time_col].max()).strftime('%Y-%m-%d')
                            local_data.append({
                                'data_type': trade_type,
                                'start_date': start_date,
                                'end_date': end_date,
                                'data_points': len(df),
                                'file_size': f"{file_path.stat().st_size / 1024 / 1024:.2f} MB",
                                'file_path': str(file_path)
                            })
                            print(f"æˆåŠŸè¯»å–æ•°æ®æ–‡ä»¶: {file_path}, æ•°æ®ç‚¹æ•°: {len(df)}")
                    except Exception as e:
                        print(f"è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
            else:
                print(f"ç›®å½•ä¸å­˜åœ¨: {search_dir}")
        
        print(f"æ€»å…±æ‰¾åˆ° {len(local_data)} ä¸ªæ•°æ®æ–‡ä»¶")
        
        # ç”Ÿæˆä¸‹è½½å»ºè®®
        recommended_downloads = []
        
        # å¦‚æœæ²¡æœ‰æœ¬åœ°æ•°æ®ï¼Œå»ºè®®ä¸‹è½½æœ€è¿‘ä¸€ä¸ªæœˆçš„æ•°æ®
        if not local_data:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=30)
            recommended_downloads.append({
                'data_type': trade_type,
                'start_date': start_date.strftime('%Y-%m-%d'),
                'end_date': end_date.strftime('%Y-%m-%d'),
                'reason': 'å»ºè®®ä»æœ€è¿‘ä¸€ä¸ªæœˆå¼€å§‹ä¸‹è½½æ•°æ®'
            })
        else:
            # æ£€æŸ¥æ•°æ®æ˜¯å¦éœ€è¦æ›´æ–°åˆ°æœ€æ–°
            latest_data = max(local_data, key=lambda x: x['end_date'])
            latest_date = datetime.strptime(latest_data['end_date'], '%Y-%m-%d')
            if (datetime.now() - latest_date).days > 1:
                recommended_downloads.append({
                    'data_type': trade_type,
                    'start_date': latest_data['end_date'],
                    'end_date': datetime.now().strftime('%Y-%m-%d'),
                    'reason': 'æ›´æ–°æ•°æ®è‡³æœ€æ–°'
                })
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰ç©ºç¼º
            sorted_data = sorted(local_data, key=lambda x: x['start_date'])
            for i in range(len(sorted_data) - 1):
                current_end = datetime.strptime(sorted_data[i]['end_date'], '%Y-%m-%d')
                next_start = datetime.strptime(sorted_data[i + 1]['start_date'], '%Y-%m-%d')
                if (next_start - current_end).days > 1:
                    recommended_downloads.append({
                        'data_type': trade_type,
                        'start_date': sorted_data[i]['end_date'],
                        'end_date': sorted_data[i + 1]['start_date'],
                        'reason': 'è¡¥å……æ•°æ®ç©ºç¼º'
                    })
        
        return jsonify({
            'success': True,
            'data': {
                'exchange': exchange,
                'symbol': symbol,
                'timeframe': timeframe,
                'trade_type': trade_type,
                'existing_data': local_data,
                'recommended_downloads': recommended_downloads
            }
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'è·å–ä¸‹è½½å»ºè®®å¤±è´¥: {str(e)}'
        }), 500

@bp.route('/data-health', methods=['POST'])
def check_data_health():
    """æ£€æŸ¥æ•°æ®å¥åº·åº¦"""
    try:
        data = request.get_json()
        file_path = data.get('file_path')
        symbol = data.get('symbol')
        timeframe = data.get('timeframe')
        
        if not file_path or not os.path.exists(file_path):
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'})
        
        # å¯¼å…¥å¥åº·åº¦æ£€æŸ¥å™¨
        from factor_miner.core.data_health_checker import health_checker
        
        # è¯»å–æ•°æ®æ–‡ä»¶
        df = pd.read_feather(file_path)
        
        # æ£€æŸ¥æ•°æ®å¥åº·åº¦
        health_report = health_checker.check_data_health(df, timeframe, symbol)
        
        return jsonify({
            'success': True,
            'data': health_report
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@bp.route('/auto-fill-gaps', methods=['POST'])
def auto_fill_gaps():
    """è‡ªåŠ¨è¡¥å…¨æ•°æ®æ–­å±‚"""
    try:
        data = request.get_json()
        symbol = data.get('symbol')
        timeframe = data.get('timeframe')
        trade_type = data.get('trade_type', 'futures')
        data_dir = data.get('data_dir')
        
        if not symbol or not timeframe:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'})
        
        # å¯¼å…¥æ–­å±‚è¡¥å…¨å™¨
        from factor_miner.core.data_gap_filler import gap_filler
        
        # æ‰§è¡Œè‡ªåŠ¨è¡¥å…¨
        result = gap_filler.auto_fill_gaps(symbol, timeframe, trade_type, data_dir)
        
        return jsonify({
            'success': True,
            'data': result
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@bp.route('/scan-gaps', methods=['POST'])
def scan_data_gaps():
    """æ‰«ææ•°æ®æ–­å±‚"""
    try:
        data = request.get_json()
        data_dir = data.get('data_dir', 'data/binance/futures')
        symbol = data.get('symbol')
        timeframe = data.get('timeframe')
        
        # å¯¼å…¥æ–­å±‚è¡¥å…¨å™¨
        from factor_miner.core.data_gap_filler import gap_filler
        
        # æ‰«ææ–­å±‚
        gaps = gap_filler.scan_for_gaps(data_dir, symbol, timeframe)
        
        return jsonify({
            'success': True,
            'data': {
                'gaps': gaps,
                'total_gaps': len(gaps),
                'data_dir': data_dir
            }
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@bp.route('/delete-data', methods=['POST'])
def delete_data():
    """åˆ é™¤æœ¬åœ°æ•°æ®"""
    try:
        data = request.get_json()
        file_path = data.get('file_path')
        
        if file_path and os.path.exists(file_path):
            os.remove(file_path)
            return jsonify({'success': True, 'message': 'æ•°æ®åˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}) 