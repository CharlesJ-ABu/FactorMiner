"""
çœŸå®æ•°æ®ä¸‹è½½æ¨¡å—
ä½¿ç”¨CCXTæ¥å£ä¸‹è½½äº¤æ˜“æ‰€æ•°æ®
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Union
from pathlib import Path
import ccxt
from datetime import datetime, timedelta
import time
import logging
import os

from config.user_config import config_manager
from .data_health_checker import health_checker
from .data_processor import data_processor


class DataDownloader:
    """æ•°æ®ä¸‹è½½å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®ä¸‹è½½å™¨"""
        self.logger = logging.getLogger(__name__)

    def get_exchange_instance(self, config_id: int = None) -> Optional[ccxt.Exchange]:
        """
        è·å–äº¤æ˜“æ‰€å®ä¾‹
        
        Args:
            config_id: é…ç½®IDï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            
        Returns:
            äº¤æ˜“æ‰€å®ä¾‹
        """
        try:
            if config_id is None:
                # ä½¿ç”¨é»˜è®¤çš„ Binance é…ç½®
                exchange_class = getattr(ccxt, 'binance')
                # æ£€æµ‹ç¯å¢ƒå˜é‡ä¸­çš„ä»£ç†é…ç½®
                http_proxy = os.getenv('HTTP_PROXY')
                https_proxy = os.getenv('HTTPS_PROXY')
                proxies = {}
                if http_proxy:
                    proxies['http'] = http_proxy
                if https_proxy:
                    proxies['https'] = https_proxy

                exchange = exchange_class({
                    'enableRateLimit': True,
                    'timeout': 30000,
                    'proxies': proxies if proxies else None,
                    'headers': {
                        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                    }
                })
                return exchange

            # ä½¿ç”¨é…ç½®æ–‡ä»¶
            config = config_manager.get_exchange_config(config_id)
            if not config:
                self.logger.error(f"é…ç½®ä¸å­˜åœ¨: {config_id}")
                return None

            exchange_class = getattr(ccxt, config['exchange_id'])
            exchange = exchange_class({
                'apiKey': config['api_key'],
                'secret': config['secret'],
                'password': config['password'],
                'sandbox': config['sandbox'],
                'enableRateLimit': True
            })

            return exchange
        except Exception as e:
            self.logger.error(f"åˆ›å»ºäº¤æ˜“æ‰€å®ä¾‹å¤±è´¥: {e}")
            return None

    def download_ohlcv(self, config_id: int = None, symbol: str = None, timeframe: str = None,
                      start_date: str = None, end_date: str = None, trade_type: str = None, progress_callback=None) -> Dict:
        """
        ä¸‹è½½OHLCVæ•°æ® - é‡æ„ç‰ˆæœ¬ï¼šç¡®ä¿æ•°æ®å®Œæ•´æ€§
        
        Args:
            config_id: äº¤æ˜“æ‰€é…ç½®ID
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´æ¡†æ¶
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            trade_type: äº¤æ˜“ç±»å‹ (spot, futures, perpetual, delivery)
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä¸‹è½½ç»“æœ
        """
        try:
            # è®¾ç½®äº¤æ˜“ç±»å‹å±æ€§
            self.trade_type = trade_type

            exchange = self.get_exchange_instance(config_id)
            if not exchange:
                return {'success': False, 'error': 'æ— æ³•åˆ›å»ºäº¤æ˜“æ‰€å®ä¾‹'}

            # è½¬æ¢æ—¥æœŸæ ¼å¼
            start_dt = datetime.strptime(start_date, '%Y-%m-%d')
            end_dt = datetime.strptime(end_date, '%Y-%m-%d')

            # è·å–æ—¶é—´æ¡†æ¶çš„æ¯«ç§’æ•°
            timeframe_ms = exchange.parse_timeframe(timeframe) * 1000

            # è®¡ç®—éœ€è¦ä¸‹è½½çš„æ•°æ®ç‚¹æ•°é‡
            total_ms = (end_dt - start_dt).total_seconds() * 1000
            total_candles = int(total_ms / timeframe_ms)

            if progress_callback:
                progress_callback(0, f"å¼€å§‹ä¸‹è½½ {symbol} {timeframe} æ•°æ®...")

            # åˆ†æ‰¹ä¸‹è½½æ•°æ®
            all_data = []
            current_dt = start_dt

            while current_dt < end_dt:
                try:
                    # è®¡ç®—æœ¬æ¬¡ä¸‹è½½çš„ç»“æŸæ—¶é—´
                    batch_end = min(current_dt + timedelta(days=30), end_dt)

                    # ä¸‹è½½æ•°æ®
                    ohlcv = exchange.fetch_ohlcv(
                        symbol,
                        timeframe,
                        int(current_dt.timestamp() * 1000),
                        limit=1000
                    )

                    if ohlcv:
                        all_data.extend(ohlcv)

                    # æ›´æ–°è¿›åº¦
                    progress = min(100, int((current_dt - start_dt).total_seconds() / (end_dt - start_dt).total_seconds() * 100))
                    if progress_callback:
                        progress_callback(progress, f"å·²ä¸‹è½½ {len(all_data)} æ¡æ•°æ®...")

                    # ç§»åŠ¨åˆ°ä¸‹ä¸€æ‰¹
                    current_dt = batch_end

                    # é™é€Ÿ
                    time.sleep(exchange.rateLimit / 1000)

                except Exception as e:
                    self.logger.error(f"ä¸‹è½½æ‰¹æ¬¡å¤±è´¥: {e}")
                    break

            if not all_data:
                return {'success': False, 'error': 'æ²¡æœ‰ä¸‹è½½åˆ°æ•°æ®'}

            # è½¬æ¢ä¸ºDataFrame - ç›´æ¥å‘½åä¸º dateï¼Œé¿å…åç»­å¤æ‚æ“ä½œ
            df = pd.DataFrame(all_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['date'] = pd.to_datetime(df['timestamp'], unit='ms')
            df.set_index('date', inplace=True)  # è®¾ç½® date ä¸ºç´¢å¼•
            df.drop('timestamp', axis=1, inplace=True)

            # åŸºç¡€å»é‡å’Œæ’åº
            df = df[~df.index.duplicated(keep='last')].sort_index()
            print(f"åŸå§‹ä¸‹è½½æ•°æ®: {len(df)} æ¡")

            # è¿‡æ»¤æ—¥æœŸèŒƒå›´
            df = df[(df.index >= start_date) & (df.index <= end_date)]
            print(f"è¿‡æ»¤åæ•°æ®: {len(df)} æ¡")

            # é˜¶æ®µ1: ä¸ç°æœ‰æ•°æ®åˆå¹¶å’ŒéªŒè¯
            df_merged = self._merge_with_existing_data(df, symbol, timeframe, start_date, end_date)

            # é˜¶æ®µ2: æ£€æµ‹å’Œè¡¥å…¨æ•°æ®é—´æ–­
            df_complete = self._fill_data_gaps(df_merged, symbol, timeframe, start_date, end_date, progress_callback)

            # é˜¶æ®µ3: æœ€ç»ˆéªŒè¯ - åªæœ‰100åˆ†æ‰èƒ½ä¿å­˜
            if not self._final_validation(df_complete, timeframe, symbol):
                print("âš ï¸ æ•°æ®éªŒè¯å¤±è´¥ï¼Œå¼€å§‹è‡ªåŠ¨ä¿®å¤...")
                df_complete = self._auto_fix_data_issues(df_complete, timeframe, symbol, max_retries=20)

                # å†æ¬¡éªŒè¯
                if not self._final_validation(df_complete, timeframe, symbol):
                    return {'success': False, 'error': 'æ•°æ®éªŒè¯å¤±è´¥ï¼Œè‡ªåŠ¨ä¿®å¤åä»æ— æ³•è¾¾åˆ°100åˆ†æ ‡å‡†'}

                print("ğŸ‰ è‡ªåŠ¨ä¿®å¤æˆåŠŸï¼Œæ•°æ®è¾¾åˆ°100åˆ†æ ‡å‡†ï¼")

            # å‡†å¤‡ä¿å­˜æ•°æ®
            print("=== ä¿å­˜å‰æ•°æ®æ£€æŸ¥ ===")
            print(f"df_complete ç´¢å¼•å: {df_complete.index.name}")
            print(f"df_complete åˆ—å: {df_complete.columns.tolist()}")
            print(f"df_complete å½¢çŠ¶: {df_complete.shape}")
            print(f"df_complete æ•°æ®ç±»å‹:")
            print(df_complete.dtypes)
            print("========================")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®ç´¢å¼•
            # if df_complete.index.name == 'date' and 'date' in df_complete.columns:
            #     print("âš ï¸ æ£€æµ‹åˆ° date ç´¢å¼•å’Œ date åˆ—å†²çªï¼Œéœ€è¦é‡ç½®ç´¢å¼•")
            #     df_save = df_complete.reset_index()
            # else:
            #     print("âœ… æ²¡æœ‰ç´¢å¼•å’Œåˆ—å†²çªï¼Œç›´æ¥ä½¿ç”¨åŸæ•°æ®")
            #     df_save = df_complete.copy()
            df_save = df_complete.copy()



            print(f"æœ€ç»ˆéªŒè¯é€šè¿‡ï¼Œå‡†å¤‡ä¿å­˜: {len(df_save)} æ¡æ•°æ®")

            # ä¿å­˜æ•°æ® - ä½¿ç”¨ä¸ç°æœ‰æ–‡ä»¶ä¸€è‡´çš„å‘½åæ ¼å¼
            # ä¾‹å¦‚ï¼šBTC_USDT_USDT-2h-futures.feather
            if hasattr(self, 'trade_type') and self.trade_type:
                if self.trade_type == 'futures':
                    filename = f"{symbol.replace('/', '_')}_USDT-{timeframe}-futures.feather"
                elif self.trade_type == 'spot':
                    filename = f"{symbol.replace('/', '_')}_USDT-{timeframe}-spot.feather"
                elif self.trade_type in ['perpetual', 'delivery']:
                    filename = f"{symbol.replace('/', '_')}_USDT-{timeframe}-{self.trade_type}.feather"
                else:
                    filename = f"{symbol}_{timeframe}_{start_date}_{end_date}.feather"
            else:
                filename = f"{symbol}_{timeframe}_{start_date}_{end_date}.feather"

            # æ ¹æ®äº¤æ˜“ç±»å‹ç¡®å®šå­˜å‚¨ç›®å½•
            if hasattr(self, 'trade_type') and self.trade_type:
                if self.trade_type == 'futures':
                    # ç¡®ä¿ä¸åˆ›å»ºå­ç›®å½•ï¼Œç›´æ¥å­˜å‚¨åˆ° futures ç›®å½•
                    save_path = Path("data/binance/futures") / filename
                elif self.trade_type == 'spot':
                    save_path = Path("data/binance/spot") / filename
                elif self.trade_type in ['perpetual', 'delivery']:
                    save_path = Path(f"data/binance/{self.trade_type}") / filename
                else:
                    save_path = Path("data/binance") / filename
            else:
                # é»˜è®¤å­˜å‚¨åˆ° binance ç›®å½•
                save_path = Path("data/binance") / filename

            # æ£€æŸ¥ç°æœ‰æ–‡ä»¶ä»¥ç¡®å®šç›®æ ‡æ—¶åŒº
            target_tz = None
            if save_path.exists():
                try:
                    existing_df = pd.read_feather(save_path)
                    if 'date' in existing_df.columns and existing_df['date'].dt.tz is not None:
                        target_tz = existing_df['date'].dt.tz
                        print(f"æ£€æµ‹åˆ°ç°æœ‰æ–‡ä»¶æ—¶åŒº: {target_tz}")
                except Exception as e:
                    print(f"è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ—¶åŒº: {e}")

            # ç»Ÿä¸€æ—¶åŒº
            if target_tz is not None:
                if df_save['date'].dt.tz is None:
                    # æ–°æ•°æ®æ— æ—¶åŒºï¼Œæ·»åŠ æ—¶åŒº
                    df_save['date'] = df_save['date'].dt.tz_localize(target_tz)
                    print(f"ä¸ºæ–°æ•°æ®æ·»åŠ æ—¶åŒº: {target_tz}")
                elif df_save['date'].dt.tz != target_tz:
                    # æ–°æ•°æ®æ—¶åŒºä¸åŒï¼Œè½¬æ¢ä¸ºç›¸åŒæ—¶åŒº
                    df_save['date'] = df_save['date'].dt.tz_convert(target_tz)
                    print(f"è½¬æ¢æ–°æ•°æ®æ—¶åŒºåˆ°: {target_tz}")
            else:
                # æ²¡æœ‰ç°æœ‰æ–‡ä»¶æˆ–ç°æœ‰æ–‡ä»¶æ— æ—¶åŒºï¼Œç¡®ä¿æ–°æ•°æ®ä¹Ÿæ— æ—¶åŒº
                if df_save['date'].dt.tz is not None:
                    df_save['date'] = df_save['date'].dt.tz_localize(None)
                    print("ç§»é™¤æ–°æ•°æ®æ—¶åŒº")

            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŒåæ–‡ä»¶ï¼Œå¦‚æœå­˜åœ¨åˆ™åˆå¹¶æ•°æ®ï¼ˆå¤±è´¥æ—¶ä¿ç•™æºæ–‡ä»¶ä¸å˜ï¼‰
            merge_attempted = False
            merge_successful = False
            alt_saved = False
            alt_path = None
            if save_path.exists():
                merge_attempted = True
                try:
                    print(f"å‘ç°åŒåæ–‡ä»¶ï¼Œå°è¯•åˆå¹¶æ•°æ®: {save_path}")
                    existing_df = pd.read_feather(save_path)

                    # ç¡®ä¿ä¸¤ä¸ªæ•°æ®æ¡†çš„ date åˆ—éƒ½æ˜¯ datetime ç±»å‹ï¼Œå¹¶ç»Ÿä¸€æ—¶åŒº
                    target_tz = None
                    if 'date' in existing_df.columns:
                        if existing_df['date'].dtype == 'int64':
                            existing_df['date'] = pd.to_datetime(existing_df['date'], unit='ms')
                        elif existing_df['date'].dtype == 'int32':
                            existing_df['date'] = pd.to_datetime(existing_df['date'], unit='s')

                        # ç»Ÿä¸€æ—¶åŒºï¼šå¦‚æœç°æœ‰æ•°æ®æœ‰æ—¶åŒºï¼Œæ–°æ•°æ®ä¹Ÿä½¿ç”¨ç›¸åŒæ—¶åŒº
                        if hasattr(existing_df['date'], 'dt') and existing_df['date'].dt.tz is not None:
                            target_tz = existing_df['date'].dt.tz
                            print(f"ç°æœ‰æ•°æ®æ—¶åŒº: {target_tz}")
                        else:
                            print("ç°æœ‰æ•°æ®æ— æ—¶åŒº")

                    if 'date' in df_save.columns:
                        if df_save['date'].dtype == 'int64':
                            df_save['date'] = pd.to_datetime(df_save['date'], unit='ms')
                        elif df_save['date'].dtype == 'int32':
                            df_save['date'] = pd.to_datetime(df_save['date'], unit='s')

                        # ç»Ÿä¸€æ—¶åŒº
                        if target_tz is not None:
                            if df_save['date'].dt.tz is None:
                                # æ–°æ•°æ®æ— æ—¶åŒºï¼Œæ·»åŠ æ—¶åŒº
                                df_save['date'] = df_save['date'].dt.tz_localize(target_tz)
                                print(f"ä¸ºæ–°æ•°æ®æ·»åŠ æ—¶åŒº: {target_tz}")
                            elif df_save['date'].dt.tz != target_tz:
                                # æ–°æ•°æ®æ—¶åŒºä¸åŒï¼Œè½¬æ¢ä¸ºç›¸åŒæ—¶åŒº
                                df_save['date'] = df_save['date'].dt.tz_convert(target_tz)
                                print(f"è½¬æ¢æ–°æ•°æ®æ—¶åŒºåˆ°: {target_tz}")
                        else:
                            # ç°æœ‰æ•°æ®æ— æ—¶åŒºï¼Œç¡®ä¿æ–°æ•°æ®ä¹Ÿæ— æ—¶åŒº
                            if df_save['date'].dt.tz is not None:
                                df_save['date'] = df_save['date'].dt.tz_localize(None)
                                print("ç§»é™¤æ–°æ•°æ®æ—¶åŒºä»¥åŒ¹é…ç°æœ‰æ•°æ®")

                    # åˆå¹¶æ•°æ®ï¼ŒæŒ‰ date å»é‡ï¼Œä¿ç•™æœ€æ–°çš„æ•°æ®
                    combined_df = pd.concat([existing_df, df_save], ignore_index=True)
                    combined_df = combined_df.drop_duplicates(subset=['date'], keep='last').sort_values('date')

                    print(f"åˆå¹¶å®Œæˆï¼šåŸæ•°æ® {len(existing_df)} è¡Œï¼Œæ–°æ•°æ® {len(df_save)} è¡Œï¼Œåˆå¹¶å {len(combined_df)} è¡Œ")
                    df_save = combined_df
                    merge_successful = True
                except Exception as e:
                    merge_successful = False
                    print(f"åˆå¹¶æ•°æ®å¤±è´¥ï¼Œæºæ–‡ä»¶å°†ä¿æŒä¸å˜ï¼Œæ–°æ•°æ®å°†å¦å­˜ä¸ºæ–°æ–‡ä»¶: {e}")

            save_path.parent.mkdir(parents=True, exist_ok=True)

            # å®‰å…¨å†™å…¥ï¼š
            # 1) è‹¥åˆå¹¶æˆåŠŸï¼Œå†™å…¥ä¸´æ—¶æ–‡ä»¶å¹¶åŸå­æ›¿æ¢æºæ–‡ä»¶
            # 2) è‹¥åˆå¹¶å¤±è´¥ä¸”å­˜åœ¨æºæ–‡ä»¶ï¼Œåˆ™æºæ–‡ä»¶ä¸å˜ï¼Œå¦å­˜æ–°æ–‡ä»¶
            # 3) è‹¥ä¸å­˜åœ¨æºæ–‡ä»¶ï¼Œç›´æ¥å†™å…¥ç›®æ ‡æ–‡ä»¶
            complete_message = None
            actual_path = save_path
            if save_path.exists() and merge_attempted and merge_successful:
                tmp_path = save_path.with_suffix('.tmp')
                df_save.to_feather(tmp_path)
                tmp_path.replace(save_path)
                complete_message = f"ä¸‹è½½å®Œæˆå¹¶åˆå¹¶ï¼å…± {len(df_save)} æ¡æ•°æ®"
            elif save_path.exists() and merge_attempted and not merge_successful:
                # ç”Ÿæˆå¦å­˜æ–‡ä»¶å
                ts = datetime.now().strftime('%Y%m%d_%H%M%S')
                alt_path = save_path.with_name(f"{save_path.stem}-new-{ts}{save_path.suffix}")
                df_save.to_feather(alt_path)
                actual_path = alt_path
                alt_saved = True
                complete_message = f"ä¸‹è½½å®Œæˆï¼Œä½†åˆå¹¶å¤±è´¥ï¼Œå·²å¦å­˜ä¸ºæ–°æ–‡ä»¶ï¼ˆä¸å½±å“åŸæ–‡ä»¶ï¼‰ã€‚å…± {len(df_save)} æ¡æ•°æ®"
            else:
                # ä¸å­˜åœ¨æºæ–‡ä»¶ï¼Œç›´æ¥å†™å…¥
                df_save.to_feather(save_path)
                complete_message = f"ä¸‹è½½å®Œæˆï¼å…± {len(df_save)} æ¡æ•°æ®"

            if progress_callback and complete_message:
                progress_callback(100, complete_message)

            # ç¡®ä¿è¿”å›çš„æ—¥æœŸèŒƒå›´ä½¿ç”¨æ­£ç¡®çš„æ—¥æœŸåˆ—
            # ä½¿ç”¨åˆå¹¶åçš„æ•°æ®è®¡ç®—å®é™…çš„æ—¥æœŸèŒƒå›´
            start_date_str = end_date_str = "æœªçŸ¥"
            if 'date' in df_save.columns:
                try:
                    if pd.api.types.is_datetime64_any_dtype(df_save['date']):
                        start_date_str = df_save['date'].min().strftime('%Y-%m-%d')
                        end_date_str = df_save['date'].max().strftime('%Y-%m-%d')
                    else:
                        df_save['date'] = pd.to_datetime(df_save['date'])
                        start_date_str = df_save['date'].min().strftime('%Y-%m-%d')
                        end_date_str = df_save['date'].max().strftime('%Y-%m-%d')
                except Exception as e:
                    print(f"å¤„ç†æ—¥æœŸèŒƒå›´æ—¶å‡ºé”™: {e}")
                    start_date_str = end_date_str = "å¤„ç†å¤±è´¥"

            return {
                'success': True,
                'data': df_save,
                'filename': filename,
                'file_path': str(actual_path),
                'data_points': len(df_save),
                'date_range': {
                    'start': start_date_str,
                    'end': end_date_str
                }
            }

        except Exception as e:
            self.logger.error(f"ä¸‹è½½æ•°æ®å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}

    def _merge_with_existing_data(self, new_df: pd.DataFrame, symbol: str, timeframe: str,
                                 start_date: str, end_date: str) -> pd.DataFrame:
        """
        ä¸ç°æœ‰æ•°æ®åˆå¹¶å’ŒéªŒè¯
        
        Args:
            new_df: æ–°ä¸‹è½½çš„æ•°æ®
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´æ¡†æ¶
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            åˆå¹¶åçš„æ•°æ®
        """
        try:
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            if hasattr(self, 'trade_type') and self.trade_type:
                if self.trade_type == 'futures':
                    filename = f"{symbol.replace('/', '_')}_USDT-{timeframe}-futures.feather"
                    save_path = Path("data/binance/futures") / filename
                elif self.trade_type == 'spot':
                    filename = f"{symbol.replace('/', '_')}_USDT-{timeframe}-spot.feather"
                    save_path = Path("data/binance/spot") / filename
                else:
                    filename = f"{symbol}_{timeframe}_{start_date}_{end_date}.feather"
                    save_path = Path("data/binance") / filename
            else:
                filename = f"{symbol}_{timeframe}_{start_date}_{end_date}.feather"
                save_path = Path("data/binance") / filename

            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æœ‰æ–‡ä»¶
            if not save_path.exists():
                print(f"æ²¡æœ‰ç°æœ‰æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨æ–°æ•°æ®")
                return new_df

            print(f"å‘ç°ç°æœ‰æ–‡ä»¶: {save_path}")

            try:
                # è¯»å–ç°æœ‰æ•°æ®
                existing_df = pd.read_feather(save_path)
                print(f"ç°æœ‰æ•°æ®: {len(existing_df)} æ¡")

                # ç¡®ä¿ç°æœ‰æ•°æ®æœ‰dateåˆ—
                if 'date' not in existing_df.columns:
                    print("ç°æœ‰æ•°æ®æ²¡æœ‰dateåˆ—ï¼Œè·³è¿‡åˆå¹¶")
                    return new_df

                # è½¬æ¢æ—¶é—´åˆ—
                existing_df['date'] = pd.to_datetime(existing_df['date'])
                existing_df = existing_df.set_index('date')

                # åˆå¹¶æ•°æ®
                combined_df = pd.concat([existing_df, new_df], ignore_index=False)

                # å»é‡ï¼ˆä¿ç•™æœ€æ–°çš„æ•°æ®ï¼‰- æŒ‰ indexï¼ˆæ—¶é—´ç‚¹ï¼‰å»é‡
                combined_df = combined_df[~combined_df.index.duplicated(keep='last')].sort_index()

                print(f"åˆå¹¶å®Œæˆ: ç°æœ‰ {len(existing_df)} æ¡ + æ–° {len(new_df)} æ¡ = åˆå¹¶å {len(combined_df)} æ¡")

                return combined_df

            except Exception as e:
                print(f"è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}ï¼Œè·³è¿‡åˆå¹¶")
                return new_df

        except Exception as e:
            print(f"åˆå¹¶ç°æœ‰æ•°æ®å¤±è´¥: {e}")
            return new_df

    def _fill_data_gaps(self, df: pd.DataFrame, symbol: str, timeframe: str,
                        start_date: str, end_date: str, progress_callback=None) -> pd.DataFrame:
        """
        æ£€æµ‹å’Œè¡¥å…¨æ•°æ®é—´æ–­
        
        Args:
            df: è¾“å…¥æ•°æ®
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´æ¡†æ¶
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            progress_callback: è¿›åº¦å›è°ƒ
            
        Returns:
            è¡¥å…¨åçš„æ•°æ®
        """
        try:
            if progress_callback:
                progress_callback(85, "æ£€æµ‹æ•°æ®é—´æ–­...")

            # æ£€æµ‹é—´æ–­
            gaps = self._detect_data_gaps(df, timeframe)

            if not gaps:
                print("æ²¡æœ‰å‘ç°æ•°æ®é—´æ–­")
                if progress_callback:
                    progress_callback(90, "æ•°æ®å®Œæ•´ï¼Œæ— éœ€è¡¥å…¨")
                return df

            print(f"å‘ç° {len(gaps)} ä¸ªæ•°æ®é—´æ–­ï¼Œå¼€å§‹è¡¥å…¨...")

            if progress_callback:
                progress_callback(87, f"å‘ç° {len(gaps)} ä¸ªé—´æ–­ï¼Œå¼€å§‹è¡¥å…¨...")

            # è¡¥å…¨é—´æ–­
            df_complete = self._download_missing_data(df, gaps, symbol, timeframe, progress_callback)

            if progress_callback:
                progress_callback(95, "æ•°æ®é—´æ–­è¡¥å…¨å®Œæˆ")

            return df_complete

        except Exception as e:
            print(f"è¡¥å…¨æ•°æ®é—´æ–­å¤±è´¥: {e}")
            return df

    def _detect_data_gaps(self, df: pd.DataFrame, timeframe: str) -> List[Dict]:
        """
        æ£€æµ‹æ•°æ®ä¸­çš„æ—¶é—´é—´æ–­
        
        Args:
            df: æ•°æ®DataFrame
            timeframe: æ—¶é—´æ¡†æ¶
            
        Returns:
            é—´æ–­ä¿¡æ¯åˆ—è¡¨
        """
        try:
            gaps = []

            # ç¡®ä¿æ•°æ®æ ¼å¼æ ‡å‡†åŒ–
            df_work = self._ensure_data_format(df)

            # ç¡®ä¿æ—¶åŒºä¸€è‡´ - ç§»é™¤æ—¶åŒºä¿¡æ¯
            if df_work.index.tz is not None:
                df_work.index = df_work.index.tz_localize(None)

            # è®¡ç®—é¢„æœŸæ—¶é—´é—´éš”
            timeframe_intervals = {
                '1m': pd.Timedelta('1 minute'),
                '3m': pd.Timedelta('3 minutes'),
                '5m': pd.Timedelta('5 minutes'),
                '15m': pd.Timedelta('15 minutes'),
                '30m': pd.Timedelta('30 minutes'),
                '1h': pd.Timedelta('1 hour'),
                '2h': pd.Timedelta('2 hours'),
                '4h': pd.Timedelta('4 hours'),
                '6h': pd.Timedelta('6 hours'),
                '8h': pd.Timedelta('8 hours'),
                '12h': pd.Timedelta('12 hours'),
                '1d': pd.Timedelta('1 day'),
            }

            expected_interval = timeframe_intervals.get(timeframe, pd.Timedelta('1 minute'))

            # è®¡ç®—æ—¶é—´å·®
            time_diff = df_work.index.to_series().diff()
            # print('time_diff å®Œæ•´å†…å®¹:')
            # print(time_diff.to_string())

            # æ£€æµ‹å¤§æ–­å±‚ï¼ˆè¶…è¿‡é¢„æœŸé—´éš”çš„2.5å€ï¼‰
            gap_threshold = expected_interval * 1.5
            large_gaps = time_diff[time_diff > gap_threshold]

            for idx, gap in large_gaps.items():
                gap_start = idx - gap
                gap_end = idx

                gaps.append({
                    'start_time': gap_start,
                    'end_time': gap_end,
                    'duration': gap,
                    'expected_interval': expected_interval,
                    'missing_intervals': int(gap.total_seconds() / expected_interval.total_seconds())
                })

            print(f"æ£€æµ‹åˆ° {len(gaps)} ä¸ªæ•°æ®é—´æ–­")
            return gaps

        except Exception as e:
            print(f"æ£€æµ‹æ•°æ®é—´æ–­å¤±è´¥: {e}")
            return []

    def _download_missing_data(self, df: pd.DataFrame, gaps: List[Dict], symbol: str,
                              timeframe: str, progress_callback=None) -> pd.DataFrame:
        """
        ä¸‹è½½ç¼ºå¤±çš„æ•°æ®
        
        Args:
            df: åŸå§‹æ•°æ®
            gaps: é—´æ–­ä¿¡æ¯
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´æ¡†æ¶
            progress_callback: è¿›åº¦å›è°ƒ
            
        Returns:
            è¡¥å…¨åçš„æ•°æ®
        """
        try:
            if not gaps:
                return df

            exchange = self.get_exchange_instance()
            if not exchange:
                print("æ— æ³•åˆ›å»ºäº¤æ˜“æ‰€å®ä¾‹ï¼Œè·³è¿‡æ•°æ®è¡¥å…¨")
                return df

            df_complete = df.copy()
            total_gaps = len(gaps)

            for i, gap in enumerate(gaps):
                if progress_callback:
                    progress = 87 + int((i + 1) / total_gaps * 8)
                    progress_callback(progress, f"è¡¥å…¨é—´æ–­ {i+1}/{total_gaps}...")

                try:
                    # ä¸‹è½½ç¼ºå¤±æ•°æ® - ç¡®ä¿æ—¶åŒºä¸€è‡´æ€§
                    start_time = gap['start_time']
                    end_time = gap['end_time']

                    # ç§»é™¤æ—¶åŒºä¿¡æ¯ä»¥é¿å…æ¯”è¾ƒé”™è¯¯
                    if hasattr(start_time, 'tz') and start_time.tz is not None:
                        start_time = start_time.tz_localize(None)
                    if hasattr(end_time, 'tz') and end_time.tz is not None:
                        end_time = end_time.tz_localize(None)

                    start_timestamp = int(start_time.timestamp() * 1000)
                    end_timestamp = int(end_time.timestamp() * 1000)

                    # åˆ†æ‰¹ä¸‹è½½ç¼ºå¤±æ•°æ®
                    missing_data = []
                    current_ts = start_timestamp

                    while current_ts < end_timestamp:
                        # è®¡ç®—å‰©ä½™æ—¶é—´åŒºé—´
                        remaining_time = end_timestamp - current_ts
                        timeframe_ms = exchange.parse_timeframe(timeframe) * 1000

                        # è®¡ç®—ç†è®ºä¸Šå‰©ä½™çš„æ•°æ®æ¡æ•°
                        remaining_candles = remaining_time // timeframe_ms

                        # åŠ¨æ€è®¾ç½® limitï¼Œä½†ä¸è¶…è¿‡1000
                        dynamic_limit = min(remaining_candles, 1000)
                        #print(f"dynamic_limit: {dynamic_limit}")

                        ohlcv = exchange.fetch_ohlcv(
                            symbol,
                            timeframe,
                            current_ts,
                            limit=dynamic_limit
                        )

                        if not ohlcv:
                            print(f"æ²¡æœ‰æ•°æ®ï¼Œç»§ç»­ç”¨å½“å‰çš„current_tså°è¯•ä¸‹è½½")
                            # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œç»§ç»­ç”¨å½“å‰çš„current_tså°è¯•ä¸‹è½½
                            # ä¸è¦æ¨è¿›æ—¶é—´ï¼Œå› ä¸ºå¯èƒ½è¿™ä¸ªæ—¶é—´ç‚¹ç¡®å®æ²¡æœ‰æ•°æ®
                            continue
                        else:
                            # å¦‚æœæœ‰æ•°æ®ï¼Œä½¿ç”¨æœ€åä¸€æ¡æ•°æ®çš„æ—¶é—´æˆ³æ¨è¿›
                            missing_data.extend(ohlcv)
                            last_timestamp = ohlcv[-1][0]  # æœ€åä¸€æ¡æ•°æ®çš„æ—¶é—´æˆ³
                            #print(f"last_timestamp: {last_timestamp}")
                            current_ts = last_timestamp + timeframe_ms

                        # é™é€Ÿ
                        time.sleep(exchange.rateLimit / 1000)

                    if missing_data:
                        # è½¬æ¢ä¸ºDataFrame
                        missing_df = pd.DataFrame(missing_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
                        missing_df['date'] = pd.to_datetime(missing_df['timestamp'], unit='ms')
                        missing_df = missing_df.set_index('date').drop('timestamp', axis=1)

                        # åœ¨åˆå¹¶ä¹‹å‰ç¡®ä¿æ—¶åŒºä¸€è‡´
                        # å¦‚æœåŸå§‹æ•°æ®æœ‰æ—¶åŒºï¼Œå…ˆç§»é™¤æ—¶åŒº
                        if hasattr(df_complete.index, 'tz') and df_complete.index.tz is not None:
                            print('ç§»é™¤åŸå§‹æ•°æ®æ—¶åŒºä¿¡æ¯')
                            df_complete.index = df_complete.index.tz_localize(None)

                        # ç¡®ä¿æ–°æ•°æ®ä¹Ÿæ— æ—¶åŒº
                        if hasattr(missing_df.index, 'tz') and missing_df.index.tz is not None:
                            print('ç§»é™¤æ–°æ•°æ®æ—¶åŒºä¿¡æ¯')
                            missing_df.index = missing_df.index.tz_localize(None)

                        # åˆå¹¶æ•°æ®
                        df_complete = pd.concat([df_complete, missing_df], ignore_index=False)
                        print('df_complete åˆå¹¶æˆåŠŸ')

                        # ç¡®ä¿æ²¡æœ‰é‡å¤çš„ date åˆ—
                        if 'date' in df_complete.columns and df_complete.index.name == 'date':
                            print('æ£€æµ‹åˆ°é‡å¤çš„ date åˆ—ï¼Œç§»é™¤åˆ—ä¸­çš„ date')
                            df_complete = df_complete.drop('date', axis=1)

                        # å»é‡å‰çš„è¯¦ç»†ç»Ÿè®¡
                        #print(f"å»é‡å‰æ•°æ®: {len(df_complete)} æ¡")
                        print(f"å»é‡å‰é‡å¤æ—¶é—´ç‚¹æ•°é‡: {df_complete.index.duplicated().sum()} æ¡")
                        #print(f"å»é‡å‰æ—¶é—´èŒƒå›´: {df_complete.index.min()} åˆ° {df_complete.index.max()}")

                        # è®°å½•å»é‡å‰çš„æ•°æ®é‡
                        before_count = len(df_complete)

                        # æ‰§è¡Œå»é‡ - æŒ‰ indexï¼ˆæ—¶é—´ç‚¹ï¼‰å»é‡ï¼Œä¸æ˜¯æŒ‰åˆ—å»é‡
                        df_complete = df_complete[~df_complete.index.duplicated(keep='last')].sort_index()

                        # å»é‡åçš„è¯¦ç»†ç»Ÿè®¡
                        #print(f"å»é‡åæ•°æ®: {len(df_complete)} æ¡")
                        print(f"å»é‡åé‡å¤æ—¶é—´ç‚¹æ•°é‡: {df_complete.index.duplicated().sum()} æ¡")
                        #print(f"å»é‡åæ—¶é—´èŒƒå›´: {df_complete.index.min()} åˆ° {df_complete.index.max()}")

                        # è®¡ç®—å®é™…ç§»é™¤çš„æ•°é‡
                        removed_count = before_count - len(df_complete)
                        if removed_count > 0:
                            print(f"âœ… æˆåŠŸç§»é™¤ {removed_count} æ¡é‡å¤æ•°æ®")
                        else:
                            print("âœ… æ²¡æœ‰å‘ç°é‡å¤æ•°æ®ï¼Œæ— éœ€ç§»é™¤")

                        # éªŒè¯å»é‡æ˜¯å¦æˆåŠŸ
                        if df_complete.index.duplicated().sum() == 0:
                            print("âœ… å»é‡éªŒè¯æˆåŠŸï¼Œæ²¡æœ‰é‡å¤æ—¶é—´ç‚¹")
                        else:
                            print(f"âŒ å»é‡éªŒè¯å¤±è´¥ï¼Œä»æœ‰ {df_complete.index.duplicated().sum()} ä¸ªé‡å¤æ—¶é—´ç‚¹")

                        print(f"è¡¥å…¨é—´æ–­ {i+1}: æ·»åŠ  {len(missing_data)} æ¡æ•°æ®")

                except Exception as e:
                    print(f"è¡¥å…¨é—´æ–­ {i+1} å¤±è´¥: {e}")
                    continue


            print(f"æ•°æ®è¡¥å…¨å®Œæˆï¼Œæœ€ç»ˆæ•°æ®: {len(df_complete)} æ¡")
            return df_complete

        except Exception as e:
            print(f"ä¸‹è½½ç¼ºå¤±æ•°æ®å¤±è´¥: {e}")
            return df

    def _final_validation(self, df: pd.DataFrame, timeframe: str, symbol: str) -> bool:
        """
        æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿æ•°æ®å®Œç¾ï¼Œåªæœ‰100åˆ†æ‰èƒ½ä¿å­˜
        
        Args:
            df: æ•°æ®DataFrame
            timeframe: æ—¶é—´æ¡†æ¶
            symbol: äº¤æ˜“å¯¹
            
        Returns:
            æ˜¯å¦é€šè¿‡éªŒè¯
        """
        try:
            print("å¼€å§‹æœ€ç»ˆæ•°æ®éªŒè¯...")

            # ç¡®ä¿æ•°æ®æ ¼å¼æ ‡å‡†åŒ–
            df_check = self._ensure_data_format(df)

            # 1. æ£€æŸ¥æ— é‡å¤æ—¶é—´ç‚¹
            duplicate_count = df_check.index.duplicated().sum()
            if duplicate_count > 0:
                print(f"âŒ éªŒè¯å¤±è´¥ï¼šå‘ç° {duplicate_count} ä¸ªé‡å¤æ—¶é—´ç‚¹")
                return False

            print("âœ… æ— é‡å¤æ—¶é—´ç‚¹")

            # 2. æ£€æŸ¥æ— æ—¶é—´é—´æ–­
            gaps = self._detect_data_gaps(df_check, timeframe)
            if gaps:
                print(f"âŒ éªŒè¯å¤±è´¥ï¼šä»æœ‰ {len(gaps)} ä¸ªæ•°æ®é—´æ–­")
                return False

            print("âœ… æ— æ—¶é—´é—´æ–­")

            # 3. æ£€æŸ¥è¦†ç›–ç‡â‰¥95%
            coverage = self._calculate_coverage(df_check, timeframe)
            if coverage < 95.0:
                print(f"âŒ éªŒè¯å¤±è´¥ï¼šæ•°æ®è¦†ç›–ç‡ {coverage:.2f}% < 95%")
                return False

            print(f"âœ… æ•°æ®è¦†ç›–ç‡: {coverage:.2f}%")

            # 4. å¥åº·åº¦æ£€æŸ¥100åˆ†
            # å¥åº·åº¦æ£€æŸ¥å™¨æœŸæœ›æ•°æ®æœ‰dateåˆ—ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦é‡ç½®ç´¢å¼•
            df_for_health_check = self._ensure_data_format_for_health_check(df_check)
            health_report = health_checker.check_data_health(df_for_health_check, timeframe, symbol)
            if not health_report['is_healthy']:
                print(f"âŒ éªŒè¯å¤±è´¥ï¼šå¥åº·åº¦æ£€æŸ¥æœªé€šè¿‡ - {health_report['summary']}")
                return False

            if health_report['health_score'] < 100.0:
                print(f"âŒ éªŒè¯å¤±è´¥ï¼šå¥åº·åº¦åˆ†æ•° {health_report['health_score']} < 100")
                return False

            print(f"âœ… å¥åº·åº¦æ£€æŸ¥é€šè¿‡: {health_report['health_score']}åˆ†")
            print("ğŸ‰ æœ€ç»ˆéªŒè¯é€šè¿‡ï¼æ•°æ®å®Œç¾ï¼Œå¯ä»¥ä¿å­˜")

            return True

        except Exception as e:
            print(f"æœ€ç»ˆéªŒè¯å¤±è´¥: {e}")
            return False

    def _auto_fix_data_issues(self, df: pd.DataFrame, timeframe: str, symbol: str, max_retries: int = 20) -> pd.DataFrame:
        """
        è‡ªåŠ¨ä¿®å¤æ•°æ®é—®é¢˜ï¼Œæœ€å¤šé‡è¯•æŒ‡å®šæ¬¡æ•°
        
        Args:
            df: è¾“å…¥æ•°æ®
            timeframe: æ—¶é—´æ¡†æ¶
            symbol: äº¤æ˜“å¯¹
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            ä¿®å¤åçš„æ•°æ®
        """
        print(f"ğŸ”§ å¼€å§‹è‡ªåŠ¨ä¿®å¤æ•°æ®é—®é¢˜ï¼Œæœ€å¤§é‡è¯•æ¬¡æ•°: {max_retries}")

        df_fixed = df.copy()

        for attempt in range(max_retries):
            print(f"\\nğŸ”„ ä¿®å¤å°è¯• {attempt + 1}/{max_retries}")

            # 1. è‡ªåŠ¨å»é‡
            df_fixed = self._remove_duplicates_auto(df_fixed)

            # 2. æ£€æµ‹å’Œè¡¥å…¨é—´æ–­
            gaps = self._detect_data_gaps(df_fixed, timeframe)
            if gaps:
                print(f"å‘ç° {len(gaps)} ä¸ªæ•°æ®é—´æ–­ï¼Œå¼€å§‹è¡¥å…¨...")
                df_fixed = self._download_missing_data(df_fixed, gaps, symbol, timeframe)
            else:
                print("æ²¡æœ‰å‘ç°æ•°æ®é—´æ–­")

            # 3. éªŒè¯ä¿®å¤ç»“æœ
            if self._final_validation(df_fixed, timeframe, symbol):
                print(f"ğŸ‰ æ•°æ®ä¿®å¤æˆåŠŸï¼åœ¨ç¬¬ {attempt + 1} æ¬¡å°è¯•åé€šè¿‡éªŒè¯")
                return df_fixed
            else:
                print(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡ä¿®å¤åä»æœªé€šè¿‡éªŒè¯ï¼Œç»§ç»­å°è¯•...")

                # å¦‚æœè¿˜æœ‰é‡å¤ï¼Œè¿›è¡Œæ›´å½»åº•çš„æ¸…ç†
                if df_fixed.index.duplicated().sum() > 0:
                    print("æ£€æµ‹åˆ°é‡å¤æ—¶é—´ç‚¹ï¼Œè¿›è¡Œæ·±åº¦æ¸…ç†...")
                    df_fixed = self._deep_clean_duplicates(df_fixed)

                # å¦‚æœè¿˜æœ‰é—´æ–­ï¼Œå°è¯•é‡æ–°ä¸‹è½½è¡¥å…¨
                gaps = self._detect_data_gaps(df_fixed, timeframe)
                if gaps:
                    print("æ£€æµ‹åˆ°æ•°æ®é—´æ–­ï¼Œå°è¯•é‡æ–°ä¸‹è½½è¡¥å…¨...")
                    df_fixed = self._download_missing_data(df_fixed, gaps, symbol, timeframe)

        print(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œæ•°æ®ä¿®å¤å¤±è´¥")
        return df_fixed

    def _ensure_data_format(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç¡®ä¿æ•°æ®æ—¢æœ‰dateç´¢å¼•åˆæœ‰dateåˆ—ï¼Œä¸æ”¹å˜åŸå§‹æ•°æ®
        
        Args:
            df: è¾“å…¥æ•°æ®
            
        Returns:
            æ ¼å¼æ ‡å‡†åŒ–çš„æ•°æ®å‰¯æœ¬
        """
        try:
            df_work = df.copy()  # åˆ›å»ºå‰¯æœ¬ï¼Œä¸ä¿®æ”¹åŸå§‹æ•°æ®

            # å¦‚æœåªæœ‰ç´¢å¼•ï¼Œæ·»åŠ åˆ—
            if df_work.index.name == 'date' and 'date' not in df_work.columns:
                df_work['date'] = df_work.index
                print("âœ… æ•°æ®æ ¼å¼æ ‡å‡†åŒ–ï¼šæ·»åŠ dateåˆ—")
            # å¦‚æœåªæœ‰åˆ—ï¼Œè®¾ç½®ç´¢å¼•
            elif 'date' in df_work.columns and df_work.index.name != 'date':
                df_work = df_work.set_index('date')
                print("âœ… æ•°æ®æ ¼å¼æ ‡å‡†åŒ–ï¼šè®¾ç½®dateç´¢å¼•")
            # å¦‚æœæ—¢æœ‰ç´¢å¼•åˆæœ‰åˆ—ï¼Œç¡®ä¿ä¸€è‡´æ€§
            elif df_work.index.name == 'date' and 'date' in df_work.columns:
                # ç¡®ä¿ç´¢å¼•å’Œåˆ—çš„å€¼ä¸€è‡´
                if not df_work.index.equals(df_work['date']):
                    df_work['date'] = df_work.index
                    print("âœ… æ•°æ®æ ¼å¼æ ‡å‡†åŒ–ï¼šåŒæ­¥dateåˆ—å’Œç´¢å¼•")
                else:
                    print("âœ… æ•°æ®æ ¼å¼å·²æ ‡å‡†åŒ–")
            else:
                print("âš ï¸ æ•°æ®æ ¼å¼å¼‚å¸¸ï¼Œå°è¯•ä¿®å¤...")
                # å°è¯•ä»ç´¢å¼•åˆ›å»ºdateåˆ—
                if df_work.index.name == 'date':
                    df_work['date'] = df_work.index
                # æˆ–è€…ä»åˆ—åˆ›å»ºç´¢å¼•
                elif 'date' in df_work.columns:
                    df_work = df_work.set_index('date')
                else:
                    print("âŒ æ— æ³•è¯†åˆ«æ•°æ®æ ¼å¼")
                    return df

            return df_work

        except Exception as e:
            print(f"æ•°æ®æ ¼å¼æ ‡å‡†åŒ–å¤±è´¥: {e}")
            return df

    def _ensure_data_format_for_health_check(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä¸ºå¥åº·åº¦æ£€æŸ¥å‡†å¤‡æ•°æ®æ ¼å¼ï¼ˆåªæœ‰dateåˆ—ï¼Œæ²¡æœ‰dateç´¢å¼•ï¼‰
        
        Args:
            df: è¾“å…¥æ•°æ®
            
        Returns:
            é€‚åˆå¥åº·åº¦æ£€æŸ¥çš„æ•°æ®æ ¼å¼
        """
        try:
            df_work = df.copy()

            # å¦‚æœæœ‰dateç´¢å¼•ï¼Œé‡ç½®ç´¢å¼•
            if df_work.index.name == 'date':
                # å¦‚æœåŒæ—¶æœ‰dateåˆ—ï¼Œå…ˆé‡å‘½åç´¢å¼•
                if 'date' in df_work.columns:
                    df_work.index.name = 'date_index'
                    print("âœ… ä¸ºå¥åº·åº¦æ£€æŸ¥å‡†å¤‡æ•°æ®ï¼šé‡å‘½åç´¢å¼•é¿å…å†²çª")

                df_work = df_work.reset_index()
                print("âœ… ä¸ºå¥åº·åº¦æ£€æŸ¥å‡†å¤‡æ•°æ®ï¼šé‡ç½®ç´¢å¼•")

                # å¦‚æœç°åœ¨æœ‰date_indexåˆ—ï¼Œå°†å…¶é‡å‘½åä¸ºdateï¼Œå¹¶ç§»é™¤åŸæ¥çš„dateåˆ—
                if 'date_index' in df_work.columns and 'date' in df_work.columns:
                    df_work['date'] = df_work['date_index']
                    df_work = df_work.drop('date_index', axis=1)
                    print("âœ… ä¸ºå¥åº·åº¦æ£€æŸ¥å‡†å¤‡æ•°æ®ï¼šç»Ÿä¸€dateåˆ—")

            # ç¡®ä¿æœ‰dateåˆ—
            if 'date' not in df_work.columns:
                print("âŒ æ•°æ®ç¼ºå°‘dateåˆ—ï¼Œæ— æ³•è¿›è¡Œå¥åº·åº¦æ£€æŸ¥")
                return df

            # ç§»é™¤é‡å¤çš„dateåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if df_work.columns.duplicated().any():
                df_work = df_work.loc[:, ~df_work.columns.duplicated()]
                print("âœ… ç§»é™¤é‡å¤åˆ—")

            return df_work

        except Exception as e:
            print(f"ä¸ºå¥åº·åº¦æ£€æŸ¥å‡†å¤‡æ•°æ®å¤±è´¥: {e}")
            return df

    def _remove_duplicates_auto(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è‡ªåŠ¨å»é™¤é‡å¤æ•°æ®
        
        Args:
            df: è¾“å…¥æ•°æ®
            
        Returns:
            å»é‡åçš„æ•°æ®
        """
        try:
            # ç¡®ä¿æ•°æ®æ ¼å¼æ ‡å‡†åŒ–
            df_work = self._ensure_data_format(df)

            # ç»Ÿè®¡é‡å¤æƒ…å†µ
            duplicate_count = df_work.index.duplicated().sum()
            if duplicate_count == 0:
                print("âœ… æ²¡æœ‰å‘ç°é‡å¤æ—¶é—´ç‚¹")
                return df_work

            print(f"ğŸ” å‘ç° {duplicate_count} ä¸ªé‡å¤æ—¶é—´ç‚¹ï¼Œå¼€å§‹å»é‡...")

            # æ–¹æ³•1: ä¿ç•™æœ€æ–°çš„æ•°æ®
            df_clean = df_work[~df_work.index.duplicated(keep='last')]
            print(f"æ–¹æ³•1å»é‡å: {len(df_clean)} æ¡ (ä¿ç•™æœ€æ–°)")

            # æ–¹æ³•2: å¦‚æœè¿˜æœ‰é‡å¤ï¼Œå°è¯•åŸºäºOHLCVçš„å»é‡
            if df_clean.index.duplicated().sum() > 0:
                print("æ–¹æ³•1ä»æœ‰é‡å¤ï¼Œå°è¯•åŸºäºOHLCVçš„å»é‡...")
                df_clean = self._remove_duplicates_by_ohlcv(df_clean)

            # æ–¹æ³•3: å¦‚æœè¿˜æœ‰é‡å¤ï¼Œå°è¯•åŸºäºæ—¶é—´çª—å£çš„å»é‡
            if df_clean.index.duplicated().sum() > 0:
                print("æ–¹æ³•2ä»æœ‰é‡å¤ï¼Œå°è¯•åŸºäºæ—¶é—´çª—å£çš„å»é‡...")
                df_clean = self._remove_duplicates_by_time_window(df_clean)

            final_duplicates = df_clean.index.duplicated().sum()
            print(f"å»é‡å®Œæˆï¼Œå‰©ä½™é‡å¤: {final_duplicates} ä¸ª")

            return df_clean

        except Exception as e:
            print(f"è‡ªåŠ¨å»é‡å¤±è´¥: {e}")
            return df

    def _remove_duplicates_by_ohlcv(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åŸºäºOHLCVæ•°æ®å»é‡
        
        Args:
            df: è¾“å…¥æ•°æ®
            
        Returns:
            å»é‡åçš„æ•°æ®
        """
        try:
            # ç¡®ä¿æ•°æ®æ ¼å¼æ ‡å‡†åŒ–
            df_work = self._ensure_data_format(df)

            # é‡ç½®ç´¢å¼•ï¼ŒåŸºäºæ‰€æœ‰åˆ—å»é‡
            df_temp = df_work.reset_index()

            # åŸºäºæ—¶é—´å’Œå…¶ä»–åˆ—å»é‡
            df_clean = df_temp.drop_duplicates(subset=['date', 'open', 'high', 'low', 'close', 'volume'], keep='last')

            # é‡æ–°è®¾ç½®ç´¢å¼•
            df_clean = df_clean.set_index('date')

            print(f"åŸºäºOHLCVå»é‡å: {len(df_clean)} æ¡")
            return df_clean

        except Exception as e:
            print(f"åŸºäºOHLCVå»é‡å¤±è´¥: {e}")
            return df

    def _remove_duplicates_by_time_window(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åŸºäºæ—¶é—´çª—å£å»é‡
        
        Args:
            df: è¾“å…¥æ•°æ®
            
        Returns:
            å»é‡åçš„æ•°æ®
        """
        try:
            # ç¡®ä¿æ•°æ®æ ¼å¼æ ‡å‡†åŒ–
            df_work = self._ensure_data_format(df)

            # é‡ç½®ç´¢å¼•
            df_temp = df_work.reset_index()

            # åŸºäºæ—¶é—´çª—å£å»é‡ï¼ˆ1ç§’å†…çš„æ•°æ®è§†ä¸ºé‡å¤ï¼‰
            df_temp['date_rounded'] = df_temp['date'].dt.round('1S')
            df_clean = df_temp.drop_duplicates(subset=['date_rounded'], keep='last')

            # ç§»é™¤è¾…åŠ©åˆ—ï¼Œé‡æ–°è®¾ç½®ç´¢å¼•
            df_clean = df_clean.drop('date_rounded', axis=1).set_index('date')

            print(f"åŸºäºæ—¶é—´çª—å£å»é‡å: {len(df_clean)} æ¡")
            return df_clean

        except Exception as e:
            print(f"åŸºäºæ—¶é—´çª—å£å»é‡å¤±è´¥: {e}")
            return df

    def _deep_clean_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ·±åº¦æ¸…ç†é‡å¤æ•°æ®
        
        Args:
            df: è¾“å…¥æ•°æ®
            
        Returns:
            æ¸…ç†åçš„æ•°æ®
        """
        try:
            print("ğŸ”§ å¼€å§‹æ·±åº¦æ¸…ç†é‡å¤æ•°æ®...")

            # ç¡®ä¿æ•°æ®æ ¼å¼æ ‡å‡†åŒ–
            df_work = self._ensure_data_format(df)

            # é‡ç½®ç´¢å¼•
            df_temp = df_work.reset_index()

            # 1. åŸºäºç²¾ç¡®æ—¶é—´æˆ³å»é‡
            df_clean = df_temp.drop_duplicates(subset=['date'], keep='last')

            # 2. åŸºäºæ—¶é—´çª—å£å»é‡ï¼ˆ1ç§’å†…ï¼‰
            df_clean['date_rounded'] = df_clean['date'].dt.round('1S')
            df_clean = df_clean.drop_duplicates(subset=['date_rounded'], keep='last')

            # 3. åŸºäºOHLCVå»é‡
            df_clean = df_clean.drop_duplicates(subset=['date_rounded', 'open', 'high', 'low', 'close', 'volume'], keep='last')

            # 4. ç§»é™¤è¾…åŠ©åˆ—ï¼Œé‡æ–°è®¾ç½®ç´¢å¼•
            df_clean = df_clean.drop('date_rounded', axis=1).set_index('date')

            # 5. æ’åº
            df_clean = df_clean.sort_index()

            print(f"æ·±åº¦æ¸…ç†å®Œæˆ: {len(df_clean)} æ¡")
            return df_clean

        except Exception as e:
            print(f"æ·±åº¦æ¸…ç†å¤±è´¥: {e}")
            return df





    def _get_timeframe_interval(self, timeframe: str) -> pd.Timedelta:
        """
        è·å–æ—¶é—´æ¡†æ¶å¯¹åº”çš„æ—¶é—´é—´éš”
        
        Args:
            timeframe: æ—¶é—´æ¡†æ¶
            
        Returns:
            æ—¶é—´é—´éš”
        """
        timeframe_intervals = {
            '1m': pd.Timedelta('1 minute'),
            '3m': pd.Timedelta('3 minutes'),
            '5m': pd.Timedelta('5 minutes'),
            '15m': pd.Timedelta('15 minutes'),
            '30m': pd.Timedelta('30 minutes'),
            '1h': pd.Timedelta('1 hour'),
            '2h': pd.Timedelta('2 hours'),
            '4h': pd.Timedelta('4 hours'),
            '6h': pd.Timedelta('6 hours'),
            '8h': pd.Timedelta('8 hours'),
            '12h': pd.Timedelta('12 hours'),
            '1d': pd.Timedelta('1 day'),
        }

        return timeframe_intervals.get(timeframe, pd.Timedelta('1 minute'))

    def _calculate_coverage(self, df: pd.DataFrame, timeframe: str) -> float:
        """
        è®¡ç®—æ•°æ®è¦†ç›–ç‡
        
        Args:
            df: æ•°æ®DataFrame
            timeframe: æ—¶é—´æ¡†æ¶
            
        Returns:
            è¦†ç›–ç‡ç™¾åˆ†æ¯”
        """
        try:
            if df.empty:
                return 0.0

            # ç¡®ä¿æ•°æ®æ ¼å¼æ ‡å‡†åŒ–
            df_check = self._ensure_data_format(df)

            # ç¡®ä¿æ—¶åŒºä¸€è‡´ - ç§»é™¤æ—¶åŒºä¿¡æ¯
            if df_check.index.tz is not None:
                df_check.index = df_check.index.tz_localize(None)

            # è®¡ç®—æ—¶é—´è·¨åº¦
            time_span = df_check.index.max() - df_check.index.min()

            # è®¡ç®—é¢„æœŸæ•°æ®æ¡æ•°
            timeframe_intervals = {
                '1m': pd.Timedelta('1 minute'),
                '3m': pd.Timedelta('3 minutes'),
                '5m': pd.Timedelta('5 minutes'),
                '15m': pd.Timedelta('15 minutes'),
                '30m': pd.Timedelta('30 minutes'),
                '1h': pd.Timedelta('1 hour'),
                '2h': pd.Timedelta('2 hours'),
                '4h': pd.Timedelta('4 hours'),
                '6h': pd.Timedelta('6 hours'),
                '8h': pd.Timedelta('8 hours'),
                '12h': pd.Timedelta('12 hours'),
                '1d': pd.Timedelta('1 day'),
            }

            expected_interval = timeframe_intervals.get(timeframe, pd.Timedelta('1 minute'))
            expected_count = time_span.total_seconds() / expected_interval.total_seconds() + 1
            actual_count = len(df_check)

            coverage = (actual_count / expected_count * 100) if expected_count > 0 else 0
            return round(coverage, 2)

        except Exception as e:
            print(f"è®¡ç®—è¦†ç›–ç‡å¤±è´¥: {e}")
            return 0.0

    def get_available_symbols(self, config_id: int) -> Dict:
        """
        è·å–å¯ç”¨çš„äº¤æ˜“å¯¹
        
        Args:
            config_id: äº¤æ˜“æ‰€é…ç½®ID
            
        Returns:
            å¯ç”¨äº¤æ˜“å¯¹ä¿¡æ¯
        """
        try:
            exchange = self.get_exchange_instance(config_id)
            if not exchange:
                return {'success': False, 'error': 'æ— æ³•åˆ›å»ºäº¤æ˜“æ‰€å®ä¾‹'}

            markets = exchange.load_markets()

            # è¿‡æ»¤USDTäº¤æ˜“å¯¹
            usdt_symbols = [symbol for symbol in markets.keys() if symbol.endswith('/USDT')]

            return {
                'success': True,
                'exchange_name': exchange.name,
                'total_symbols': len(markets),
                'usdt_symbols': usdt_symbols[:50],  # åªè¿”å›å‰50ä¸ª
                'all_symbols': list(markets.keys())[:100]  # åªè¿”å›å‰100ä¸ª
            }

        except Exception as e:
            self.logger.error(f"è·å–äº¤æ˜“å¯¹å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}

    def get_timeframes(self, config_id: int) -> Dict:
        """
        è·å–æ”¯æŒçš„æ—¶é—´æ¡†æ¶
        
        Args:
            config_id: äº¤æ˜“æ‰€é…ç½®ID
            
        Returns:
            æ”¯æŒçš„æ—¶é—´æ¡†æ¶
        """
        try:
            exchange = self.get_exchange_instance(config_id)
            if not exchange:
                return {'success': False, 'error': 'æ— æ³•åˆ›å»ºäº¤æ˜“æ‰€å®ä¾‹'}

            timeframes = exchange.timeframes if hasattr(exchange, 'timeframes') else {}

            return {
                'success': True,
                'exchange_name': exchange.name,
                'timeframes': timeframes
            }

        except Exception as e:
            self.logger.error(f"è·å–æ—¶é—´æ¡†æ¶å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}

    def get_data_info(self, file_path: str) -> Dict:
        """
        è·å–æ•°æ®æ–‡ä»¶ä¿¡æ¯
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ•°æ®ä¿¡æ¯
        """
        try:
            df = pd.read_feather(file_path)

            return {
                'success': True,
                'shape': df.shape,
                'columns': list(df.columns),
                'date_range': {
                    'start': df.index.min().strftime('%Y-%m-%d %H:%M:%S'),
                    'end': df.index.max().strftime('%Y-%m-%d %H:%M:%S')
                },
                'data_points': len(df),
                'file_size': Path(file_path).stat().st_size / 1024 / 1024,  # MB
                'memory_usage': df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
            }

        except Exception as e:
            self.logger.error(f"è·å–æ•°æ®ä¿¡æ¯å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}

    def _fix_data_issues(self, df: pd.DataFrame, health_report: Dict) -> pd.DataFrame:
        """
        ä¿®å¤æ•°æ®é—®é¢˜
        
        Args:
            df: åŸå§‹æ•°æ®
            health_report: å¥åº·åº¦æ£€æŸ¥æŠ¥å‘Š
            
        Returns:
            ä¿®å¤åçš„æ•°æ®
        """
        try:
            df_fixed = df.copy()
            issues = health_report.get('issues', [])

            # é¦–å…ˆæ£€æŸ¥æ•°æ®ç±»å‹é—®é¢˜
            df_fixed = self._fix_data_types(df_fixed)

            for issue in issues:
                if 'OHLCæ•°æ®é€»è¾‘é”™è¯¯' in issue:
                    # ä¿®å¤OHLCé€»è¾‘é”™è¯¯
                    df_fixed = self._fix_ohlc_logic(df_fixed)
                elif 'ä»·æ ¼ä¸º0æˆ–è´Ÿæ•°' in issue:
                    # ä¿®å¤ä»·æ ¼é—®é¢˜
                    df_fixed = self._fix_price_issues(df_fixed)
                elif 'æˆäº¤é‡ä¸ºè´Ÿæ•°' in issue:
                    # ä¿®å¤æˆäº¤é‡é—®é¢˜
                    df_fixed = self._fix_volume_issues(df_fixed)

            # æœ€ç»ˆå»é‡ - ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„åˆ—å
            if 'datetime' in df_fixed.columns:
                df_fixed = data_processor.remove_duplicates(df_fixed, 'datetime')
            elif 'date' in df_fixed.columns:
                df_fixed = data_processor.remove_duplicates(df_fixed, 'date')
            else:
                self.logger.warning("æ‰¾ä¸åˆ°æ—¶é—´åˆ—ï¼Œè·³è¿‡å»é‡")

            self.logger.info(f"æ•°æ®ä¿®å¤å®Œæˆï¼ŒåŸå§‹æ•°æ® {len(df)} æ¡ï¼Œä¿®å¤å {len(df_fixed)} æ¡")
            return df_fixed

        except Exception as e:
            self.logger.error(f"æ•°æ®ä¿®å¤å¤±è´¥: {e}")
            return df

    def _fix_ohlc_logic(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¿®å¤OHLCé€»è¾‘é”™è¯¯"""
        try:
            df_fixed = df.copy()

            # ç¡®ä¿ high >= low
            df_fixed['high'] = df_fixed[['high', 'low']].max(axis=1)
            df_fixed['low'] = df_fixed[['high', 'low']].min(axis=1)

            # ç¡®ä¿ open å’Œ close åœ¨ high å’Œ low ä¹‹é—´
            df_fixed['open'] = df_fixed['open'].clip(df_fixed['low'], df_fixed['high'])
            df_fixed['close'] = df_fixed['close'].clip(df_fixed['low'], df_fixed['high'])

            return df_fixed
        except Exception as e:
            self.logger.error(f"ä¿®å¤OHLCé€»è¾‘å¤±è´¥: {e}")
            return df

    def _fix_price_issues(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¿®å¤ä»·æ ¼é—®é¢˜"""
        try:
            df_fixed = df.copy()

            # å°†0æˆ–è´Ÿæ•°ä»·æ ¼æ›¿æ¢ä¸ºå‰ä¸€ä¸ªæœ‰æ•ˆä»·æ ¼
            for col in ['open', 'high', 'low', 'close']:
                if col in df_fixed.columns:
                    df_fixed[col] = df_fixed[col].replace([0, -np.inf, np.inf], np.nan)
                    df_fixed[col] = df_fixed[col].fillna(method='ffill')

            return df_fixed
        except Exception as e:
            self.logger.error(f"ä¿®å¤ä»·æ ¼é—®é¢˜å¤±è´¥: {e}")
            return df

    def _fix_data_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜"""
        try:
            df_fixed = df.copy()

            # ä¿®å¤OHLCåˆ—çš„æ•°æ®ç±»å‹
            ohlc_columns = ['open', 'high', 'low', 'close']
            for col in ohlc_columns:
                if col in df_fixed.columns:
                    # å¦‚æœåˆ—æ˜¯datetimeç±»å‹ä½†åº”è¯¥æ˜¯æ•°å€¼ï¼Œè¿›è¡Œä¿®å¤
                    if pd.api.types.is_datetime64_any_dtype(df_fixed[col]):
                        self.logger.warning(f"å‘ç° {col} åˆ—ç±»å‹é”™è¯¯ï¼ˆåº”è¯¥æ˜¯æ•°å€¼ä½†å®é™…æ˜¯datetimeï¼‰ï¼Œå°è¯•ä¿®å¤...")
                        try:
                            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                            df_fixed[col] = pd.to_numeric(df_fixed[col], errors='coerce')
                            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……
                            if df_fixed[col].isna().all():
                                self.logger.error(f"æ— æ³•ä¿®å¤ {col} åˆ—ï¼Œå°†ä½¿ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼")
                                df_fixed[col] = df_fixed[col].fillna(method='ffill')
                        except Exception as e:
                            self.logger.error(f"ä¿®å¤ {col} åˆ—å¤±è´¥: {e}")
                            # ä½¿ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……
                            df_fixed[col] = df_fixed[col].fillna(method='ffill')

            return df_fixed

        except Exception as e:
            self.logger.error(f"ä¿®å¤æ•°æ®ç±»å‹å¤±è´¥: {e}")
            return df

    def _fix_volume_issues(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¿®å¤æˆäº¤é‡é—®é¢˜"""
        try:
            df_fixed = df.copy()

            if 'volume' in df_fixed.columns:
                # å°†è´Ÿæ•°æˆäº¤é‡æ›¿æ¢ä¸º0
                df_fixed['volume'] = df_fixed['volume'].clip(lower=0)

            return df_fixed
        except Exception as e:
            self.logger.error(f"ä¿®å¤æˆäº¤é‡é—®é¢˜å¤±è´¥: {e}")
            return df

    def list_downloaded_data(self) -> List[Dict]:
        """
        åˆ—å‡ºå·²ä¸‹è½½çš„æ•°æ®æ–‡ä»¶
        
        Returns:
            æ•°æ®æ–‡ä»¶åˆ—è¡¨
        """
        try:
            data_dir = Path("data/binance")
            if not data_dir.exists():
                return []

            data_files = []
            for file_path in data_dir.glob("*.feather"):
                try:
                    info = self.get_data_info(str(file_path))
                    if info['success']:
                        data_files.append({
                            'filename': file_path.name,
                            'file_path': str(file_path),
                            'file_size': info['file_size'],
                            'data_points': info['data_points'],
                            'date_range': info['date_range']
                        })
                except Exception as e:
                    self.logger.error(f"è¯»å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {file_path}: {e}")

            return sorted(data_files, key=lambda x: x['filename'])

        except Exception as e:
            self.logger.error(f"åˆ—å‡ºæ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
            return []
