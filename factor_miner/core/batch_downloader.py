#!/usr/bin/env python3
"""
æ™ºèƒ½åˆ†æ‰¹ä¸‹è½½å™¨
ä¸ºä¸åŒæ—¶é—´æ¡†æ¶ä¼˜åŒ–åˆ†æ‰¹ä¸‹è½½ç­–ç•¥
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Union, Tuple
from pathlib import Path
import ccxt
from datetime import datetime, timedelta
import time
import logging
from dataclasses import dataclass

from .data_downloader import DataDownloader
from .data_health_checker import health_checker
from .data_processor import data_processor


@dataclass
class BatchConfig:
    """åˆ†æ‰¹ä¸‹è½½é…ç½®"""
    timeframe: str
    batch_days: int  # æ¯æ‰¹ä¸‹è½½çš„å¤©æ•°
    max_candles_per_batch: int  # æ¯æ‰¹æœ€å¤§Kçº¿æ•°é‡
    delay_seconds: float  # æ‰¹æ¬¡é—´å»¶è¿Ÿç§’æ•°
    retry_attempts: int  # é‡è¯•æ¬¡æ•°


class SmartBatchDownloader(DataDownloader):
    """æ™ºèƒ½åˆ†æ‰¹ä¸‹è½½å™¨"""
    
    def __init__(self):
        super().__init__()
        self.logger = logging.getLogger(__name__)
        
        # ä¸ºä¸åŒæ—¶é—´æ¡†æ¶é…ç½®åˆ†æ‰¹ç­–ç•¥
        self.batch_configs = {
            '1m': BatchConfig('1m', 1, 1000, 1.0, 3),      # 1åˆ†é’Ÿï¼šæ¯å¤©ä¸€æ‰¹ï¼Œæœ€å¤š1000æ ¹Kçº¿
            '5m': BatchConfig('5m', 3, 1000, 0.8, 3),      # 5åˆ†é’Ÿï¼šæ¯3å¤©ä¸€æ‰¹
            '15m': BatchConfig('15m', 7, 1000, 0.6, 3),    # 15åˆ†é’Ÿï¼šæ¯å‘¨ä¸€æ‰¹
            '30m': BatchConfig('30m', 14, 1000, 0.5, 3),   # 30åˆ†é’Ÿï¼šæ¯2å‘¨ä¸€æ‰¹
            '1h': BatchConfig('1h', 30, 1000, 0.3, 3),     # 1å°æ—¶ï¼šæ¯æœˆä¸€æ‰¹
            '4h': BatchConfig('4h', 90, 1000, 0.2, 3),     # 4å°æ—¶ï¼šæ¯3ä¸ªæœˆä¸€æ‰¹
            '1d': BatchConfig('1d', 365, 1000, 0.1, 3),    # 1å¤©ï¼šæ¯å¹´ä¸€æ‰¹
        }
    
    def get_batch_config(self, timeframe: str) -> BatchConfig:
        """è·å–æ—¶é—´æ¡†æ¶çš„åˆ†æ‰¹é…ç½®"""
        return self.batch_configs.get(timeframe, self.batch_configs['1h'])
    
    def calculate_optimal_batch_size(self, timeframe: str, start_date: str, end_date: str) -> Tuple[int, int]:
        """
        è®¡ç®—æœ€ä¼˜åˆ†æ‰¹å¤§å°
        
        Args:
            timeframe: æ—¶é—´æ¡†æ¶
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            (æ¯æ‰¹å¤©æ•°, æ€»æ‰¹æ¬¡æ•°)
        """
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        total_days = (end_dt - start_dt).days
        
        config = self.get_batch_config(timeframe)
        batch_days = config.batch_days
        
        # æ ¹æ®æ€»å¤©æ•°è°ƒæ•´æ‰¹æ¬¡å¤§å°
        if total_days <= 7:
            batch_days = 1
        elif total_days <= 30:
            batch_days = 3
        elif total_days <= 90:
            batch_days = 7
        elif total_days <= 365:
            batch_days = 30
        else:
            batch_days = 90
        
        total_batches = (total_days + batch_days - 1) // batch_days
        return batch_days, total_batches
    
    def download_ohlcv_batch(self, config_id: int = None, symbol: str = None, 
                            timeframe: str = None, start_date: str = None, 
                            end_date: str = None, trade_type: str = None, 
                            progress_callback=None) -> Dict:
        """
        æ™ºèƒ½åˆ†æ‰¹ä¸‹è½½OHLCVæ•°æ®
        
        Args:
            config_id: äº¤æ˜“æ‰€é…ç½®ID
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´æ¡†æ¶
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            trade_type: äº¤æ˜“ç±»å‹
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä¸‹è½½ç»“æœ
        """
        try:
            # è®¾ç½®äº¤æ˜“ç±»å‹å±æ€§
            self.trade_type = trade_type
            
            exchange = self.get_exchange_instance(config_id)
            if not exchange:
                return {'success': False, 'error': 'æ— æ³•åˆ›å»ºäº¤æ˜“æ‰€å®ä¾‹'}
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            start_dt = datetime.strptime(start_date, '%Y-%m-%d')
            end_dt = datetime.strptime(end_date, '%Y-%m-%d')
            
            # è·å–åˆ†æ‰¹é…ç½®
            batch_days, total_batches = self.calculate_optimal_batch_size(timeframe, start_date, end_date)
            config = self.get_batch_config(timeframe)
            
            if progress_callback:
                progress_callback(0, f"å¼€å§‹åˆ†æ‰¹ä¸‹è½½ {symbol} {timeframe} æ•°æ®...")
                progress_callback(0, f"æ€»æ‰¹æ¬¡æ•°: {total_batches}, æ¯æ‰¹å¤©æ•°: {batch_days}")
            
            # åˆ†æ‰¹ä¸‹è½½æ•°æ®
            all_data = []
            current_dt = start_dt
            batch_count = 0
            
            while current_dt < end_dt:
                try:
                    batch_count += 1
                    
                    # è®¡ç®—æœ¬æ¬¡ä¸‹è½½çš„ç»“æŸæ—¶é—´
                    batch_end = min(current_dt + timedelta(days=batch_days), end_dt)
                    
                    # è®¡ç®—è¿›åº¦
                    progress = min(95, int((current_dt - start_dt).days / (end_dt - start_dt).days * 90))
                    
                    if progress_callback:
                        progress_callback(progress, f"ä¸‹è½½ç¬¬ {batch_count}/{total_batches} æ‰¹: "
                                        f"{current_dt.strftime('%Y-%m-%d')} åˆ° {batch_end.strftime('%Y-%m-%d')}")
                    
                    # ä¸‹è½½æ•°æ®
                    ohlcv = exchange.fetch_ohlcv(
                        symbol, 
                        timeframe, 
                        int(current_dt.timestamp() * 1000),
                        limit=config.max_candles_per_batch
                    )
                    
                    if ohlcv:
                        all_data.extend(ohlcv)
                        if progress_callback:
                            progress_callback(progress + 5, f"ç¬¬ {batch_count} æ‰¹å®Œæˆï¼Œ"
                                            f"å½“å‰æ€»è®¡: {len(all_data)} æ¡æ•°æ®")
                    else:
                        self.logger.warning(f"ç¬¬ {batch_count} æ‰¹æ²¡æœ‰æ•°æ®")
                    
                    # ç§»åŠ¨åˆ°ä¸‹ä¸€æ‰¹
                    current_dt = batch_end
                    
                    # é™é€Ÿå’Œå»¶è¿Ÿ
                    time.sleep(config.delay_seconds)
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
                    if batch_count % 10 == 0:  # æ¯10æ‰¹æ£€æŸ¥ä¸€æ¬¡
                        time.sleep(config.delay_seconds * 2)  # é¢å¤–å»¶è¿Ÿ
                    
                except Exception as e:
                    self.logger.error(f"ç¬¬ {batch_count} æ‰¹ä¸‹è½½å¤±è´¥: {e}")
                    
                    # é‡è¯•é€»è¾‘
                    retry_count = 0
                    while retry_count < config.retry_attempts:
                        try:
                            time.sleep(config.delay_seconds * 2)
                            retry_count += 1
                            
                            if progress_callback:
                                progress_callback(progress, f"ç¬¬ {batch_count} æ‰¹é‡è¯• {retry_count}/{config.retry_attempts}")
                            
                            ohlcv = exchange.fetch_ohlcv(
                                symbol, 
                                timeframe, 
                                int(current_dt.timestamp() * 1000),
                                limit=config.max_candles_per_batch
                            )
                            
                            if ohlcv:
                                all_data.extend(ohlcv)
                                break
                            else:
                                self.logger.warning(f"ç¬¬ {batch_count} æ‰¹é‡è¯• {retry_count} æ¬¡åä»æ— æ•°æ®")
                                
                        except Exception as retry_e:
                            self.logger.error(f"ç¬¬ {batch_count} æ‰¹é‡è¯• {retry_count} å¤±è´¥: {retry_e}")
                    
                    # å¦‚æœé‡è¯•å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€æ‰¹
                    current_dt = batch_end
            
            if not all_data:
                return {'success': False, 'error': 'æ²¡æœ‰ä¸‹è½½åˆ°æ•°æ®'}
            
            if progress_callback:
                progress_callback(95, f"æ•°æ®ä¸‹è½½å®Œæˆï¼Œå…± {len(all_data)} æ¡ï¼Œæ­£åœ¨å¤„ç†...")
            
            # è½¬æ¢ä¸ºDataFrame - ç›´æ¥å‘½åä¸º dateï¼Œé¿å…åç»­å¤æ‚æ“ä½œ
            df = pd.DataFrame(all_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            # è½¬æ¢æ—¶é—´æˆ³å¹¶å¤„ç†æ—¶åŒºé—®é¢˜ - ç›´æ¥å‘½åä¸º date
            df['date'] = pd.to_datetime(df['timestamp'], unit='ms')
            
            # å¦‚æœæ—¶é—´æˆ³æ˜¯UTCæ—¶é—´ï¼Œè½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´
            # æ³¨æ„ï¼šCCXTè¿”å›çš„æ—¶é—´æˆ³é€šå¸¸æ˜¯UTCæ—¶é—´
            if df['date'].dt.tz is None:
                # å‡è®¾æ˜¯UTCæ—¶é—´ï¼Œè½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´
                df['date'] = df['date'].dt.tz_localize('UTC').dt.tz_convert('Asia/Shanghai')
            
            df.set_index('date', inplace=True)  # è®¾ç½® date ä¸ºç´¢å¼•
            df.drop('timestamp', axis=1, inplace=True)
            
            print(f"åŸå§‹ä¸‹è½½æ•°æ®: {len(df)} æ¡")
            print(f"æ•°æ®æ—¶é—´èŒƒå›´: {df.index.min()} åˆ° {df.index.max()}")
            
            # å»é‡å’Œæ’åºï¼ˆæŒ‰ date ç´¢å¼•ï¼‰
            df = df[~df.index.duplicated(keep='last')].sort_index()
            print(f"å»é‡åæ•°æ®: {len(df)} æ¡")
            
            # é˜¶æ®µ1: ä¸ç°æœ‰æ•°æ®åˆå¹¶å’ŒéªŒè¯
            df_merged = self._merge_with_existing_data(df, symbol, timeframe, start_date, end_date)
            
            # é˜¶æ®µ2: æ£€æµ‹å’Œè¡¥å…¨æ•°æ®é—´æ–­
            df_complete = self._fill_data_gaps(df_merged, symbol, timeframe, start_date, end_date, progress_callback)
            
            # é˜¶æ®µ3: æœ€ç»ˆéªŒè¯ - åªæœ‰100åˆ†æ‰èƒ½ä¿å­˜
            if not self._final_validation(df_complete, timeframe, symbol):
                print("âš ï¸ æ•°æ®éªŒè¯å¤±è´¥ï¼Œå¼€å§‹è‡ªåŠ¨ä¿®å¤...")
                df_complete = self._auto_fix_data_issues(df_complete, timeframe, symbol, max_retries=20)
                
                # å†æ¬¡éªŒè¯
                if not self._final_validation(df_complete, timeframe, symbol):
                    return {'success': False, 'error': 'æ•°æ®éªŒè¯å¤±è´¥ï¼Œè‡ªåŠ¨ä¿®å¤åä»æ— æ³•è¾¾åˆ°100åˆ†æ ‡å‡†'}
                
                print("ğŸ‰ è‡ªåŠ¨ä¿®å¤æˆåŠŸï¼Œæ•°æ®è¾¾åˆ°100åˆ†æ ‡å‡†ï¼")
            
            # å‡†å¤‡ä¿å­˜æ•°æ®
            print("=== ä¿å­˜å‰æ•°æ®æ£€æŸ¥ ===")
            print(f"df_complete ç´¢å¼•å: {df_complete.index.name}")
            print(f"df_complete åˆ—å: {df_complete.columns.tolist()}")
            print(f"df_complete å½¢çŠ¶: {df_complete.shape}")
            print(f"df_complete æ•°æ®ç±»å‹:")
            print(df_complete.dtypes)
            print("========================")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®ç´¢å¼•
            # if df_complete.index.name == 'date' and 'date' in df_complete.columns:
            #     print("âš ï¸ æ£€æµ‹åˆ° date ç´¢å¼•å’Œ date åˆ—å†²çªï¼Œéœ€è¦é‡ç½®ç´¢å¼•")
            #     df_save = df_complete.reset_index()
            # else:
            #     print("âœ… æ²¡æœ‰ç´¢å¼•å’Œåˆ—å†²çªï¼Œç›´æ¥ä½¿ç”¨åŸæ•°æ®")
            #     df_save = df_complete.copy()
            df_save = df_complete.copy()

            print(f"æœ€ç»ˆéªŒè¯é€šè¿‡ï¼Œå‡†å¤‡ä¿å­˜: {len(df_save)} æ¡æ•°æ®")
            
            if progress_callback:
                progress_callback(98, f"æ•°æ®å¤„ç†å®Œæˆï¼Œå‡†å¤‡ä¿å­˜...")
            
            # ä¿å­˜æ•°æ®
            save_result = self._save_data_with_merge(df_save, symbol, timeframe, start_date, end_date)
            
            if progress_callback:
                progress_callback(100, f"ä¸‹è½½å®Œæˆï¼{save_result.get('message', 'æ•°æ®å·²ä¿å­˜')}")
            
            return {
                'success': True,
                'data': df_save,
                'total_records': len(df_save),
                'timeframe': timeframe,
                'symbol': symbol,
                'start_date': start_date,
                'end_date': end_date,
                'batch_info': {
                    'total_batches': total_batches,
                    'batch_days': batch_days,
                    'actual_batches': batch_count
                },
                'message': save_result.get('message', 'æ•°æ®ä¸‹è½½å®Œæˆ')
            }
            
        except Exception as e:
            error_msg = f"ä¸‹è½½å¤±è´¥: {e}"
            self.logger.error(error_msg)
            if progress_callback:
                progress_callback(0, error_msg)
            return {'success': False, 'error': error_msg}
    
    def _save_data_with_merge(self, df_save: pd.DataFrame, symbol: str, timeframe: str, 
                             start_date: str, end_date: str) -> Dict:
        """ä¿å­˜æ•°æ®å¹¶å¤„ç†åˆå¹¶é€»è¾‘"""
        # ç›´æ¥ä¿å­˜æ•°æ®ï¼Œé¿å…é‡å¤è°ƒç”¨
        try:
            # æ„å»ºæ–‡ä»¶å
            if hasattr(self, 'trade_type') and self.trade_type:
                if self.trade_type == 'futures':
                    filename = f"{symbol.replace('/', '_')}_USDT-{timeframe}-futures.feather"
                elif self.trade_type == 'spot':
                    filename = f"{symbol.replace('/', '_')}_USDT-{timeframe}-spot.feather"
                elif self.trade_type in ['perpetual', 'delivery']:
                    filename = f"{symbol.replace('/', '_')}_USDT-{timeframe}-{self.trade_type}.feather"
                else:
                    filename = f"{symbol}_{timeframe}_{start_date}_{end_date}.feather"
            else:
                filename = f"{symbol}_{timeframe}_{start_date}_{end_date}.feather"
            
            # ç¡®å®šå­˜å‚¨ç›®å½•
            if hasattr(self, 'trade_type') and self.trade_type:
                if self.trade_type == 'futures':
                    save_path = Path("data/binance/futures") / filename
                elif self.trade_type == 'spot':
                    save_path = Path("data/binance/spot") / filename
                elif self.trade_type in ['perpetual', 'delivery']:
                    save_path = Path(f"data/binance/{self.trade_type}") / filename
                else:
                    save_path = Path("data/binance") / filename
            else:
                save_path = Path("data/binance") / filename
            
            # æ£€æŸ¥ç°æœ‰æ–‡ä»¶å¹¶åˆå¹¶
            if save_path.exists():
                try:
                    existing_df = pd.read_feather(save_path)
                    
                    # ç¡®ä¿ä¸¤ä¸ªæ•°æ®æ¡†çš„ date åˆ—éƒ½æ˜¯ datetime ç±»å‹
                    if 'date' in existing_df.columns:
                        if existing_df['date'].dtype == 'int64':
                            existing_df['date'] = pd.to_datetime(existing_df['date'], unit='ms')
                        elif existing_df['date'].dtype == 'int32':
                            existing_df['date'] = pd.to_datetime(existing_df['date'], unit='s')
                    
                    # æ–°æ•°æ®å·²ç»æœ‰ date åˆ—ï¼Œæ— éœ€å¤„ç†
                    print("åˆå¹¶æ—¶ï¼šæ•°æ®æ ¼å¼å·²æ­£ç¡®")
                    
                    # åˆå¹¶æ•°æ®ï¼ŒæŒ‰ date å»é‡ï¼Œä¿ç•™æœ€æ–°çš„æ•°æ®
                    combined_df = pd.concat([existing_df, df_save], ignore_index=True)
                    combined_df = combined_df.drop_duplicates(subset=['date'], keep='last').sort_values('date')
                    
                    print(f"åˆå¹¶å®Œæˆï¼šåŸæ•°æ® {len(existing_df)} è¡Œï¼Œæ–°æ•°æ® {len(df_save)} è¡Œï¼Œåˆå¹¶å {len(combined_df)} è¡Œ")
                    df_save = combined_df
                    
                except Exception as e:
                    print(f"åˆå¹¶æ•°æ®å¤±è´¥ï¼Œå°†å¦å­˜ä¸ºæ–°æ–‡ä»¶: {e}")
                    # ç”Ÿæˆæ–°æ–‡ä»¶å
                    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
                    save_path = save_path.with_name(f"{save_path.stem}-new-{ts}{save_path.suffix}")
            
            # ä¿å­˜æ•°æ®
            save_path.parent.mkdir(parents=True, exist_ok=True)
            df_save.to_feather(save_path)
            
            return {
                'success': True,
                'message': f'æ•°æ®ä¿å­˜æˆåŠŸï¼Œå…± {len(df_save)} æ¡è®°å½•',
                'file_path': str(save_path)
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': f'ä¿å­˜å¤±è´¥: {e}'
            }


    def _fix_data_issues(self, df: pd.DataFrame, health_report: Dict) -> pd.DataFrame:
        """
        ä¿®å¤æ•°æ®é—®é¢˜
        
        Args:
            df: åŸå§‹æ•°æ®
            health_report: å¥åº·åº¦æ£€æŸ¥æŠ¥å‘Š
            
        Returns:
            ä¿®å¤åçš„æ•°æ®
        """
        try:
            df_fixed = df.copy()
            issues = health_report.get('issues', [])
            
            # é¦–å…ˆæ£€æŸ¥æ•°æ®ç±»å‹é—®é¢˜
            df_fixed = self._fix_data_types(df_fixed)
            
            for issue in issues:
                if 'OHLCæ•°æ®é€»è¾‘é”™è¯¯' in issue:
                    # ä¿®å¤OHLCé€»è¾‘é”™è¯¯
                    df_fixed = self._fix_ohlc_logic(df_fixed)
                elif 'ä»·æ ¼ä¸º0æˆ–è´Ÿæ•°' in issue:
                    # ä¿®å¤ä»·æ ¼é—®é¢˜
                    df_fixed = self._fix_price_issues(df_fixed)
                elif 'æˆäº¤é‡ä¸ºè´Ÿæ•°' in issue:
                    # ä¿®å¤æˆäº¤é‡é—®é¢˜
                    df_fixed = self._fix_volume_issues(df_fixed)
            
            # æœ€ç»ˆå»é‡ - ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„åˆ—å
            if 'datetime' in df_fixed.columns:
                df_fixed = data_processor.remove_duplicates(df_fixed, 'datetime')
            elif 'date' in df_fixed.columns:
                df_fixed = data_processor.remove_duplicates(df_fixed, 'date')
            else:
                print("âš ï¸  è­¦å‘Šï¼šæ‰¾ä¸åˆ°æ—¶é—´åˆ—ï¼Œè·³è¿‡å»é‡")
            
            print(f"ğŸ”§ æ•°æ®ä¿®å¤å®Œæˆï¼ŒåŸå§‹æ•°æ® {len(df)} æ¡ï¼Œä¿®å¤å {len(df_fixed)} æ¡")
            return df_fixed
            
        except Exception as e:
            print(f"âŒ æ•°æ®ä¿®å¤å¤±è´¥: {e}")
            return df
    
    def _fix_ohlc_logic(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¿®å¤OHLCé€»è¾‘é”™è¯¯"""
        try:
            df_fixed = df.copy()
            
            # ç¡®ä¿ high >= low
            df_fixed['high'] = df_fixed[['high', 'low']].max(axis=1)
            df_fixed['low'] = df_fixed[['high', 'low']].min(axis=1)
            
            # ç¡®ä¿ open å’Œ close åœ¨ high å’Œ low ä¹‹é—´
            df_fixed['open'] = df_fixed['open'].clip(df_fixed['low'], df_fixed['high'])
            df_fixed['close'] = df_fixed['close'].clip(df_fixed['low'], df_fixed['high'])
            
            return df_fixed
        except Exception as e:
            print(f"âŒ ä¿®å¤OHLCé€»è¾‘å¤±è´¥: {e}")
            return df
    
    def _fix_price_issues(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¿®å¤ä»·æ ¼é—®é¢˜"""
        try:
            df_fixed = df.copy()
            
            # å°†0æˆ–è´Ÿæ•°ä»·æ ¼æ›¿æ¢ä¸ºå‰ä¸€ä¸ªæœ‰æ•ˆä»·æ ¼
            for col in ['open', 'high', 'low', 'close']:
                if col in df_fixed.columns:
                    df_fixed[col] = df_fixed[col].replace([0, -np.inf, np.inf], np.nan)
                    df_fixed[col] = df_fixed[col].fillna(method='ffill')
            
            return df_fixed
        except Exception as e:
            print(f"âŒ ä¿®å¤ä»·æ ¼é—®é¢˜å¤±è´¥: {e}")
            return df
    
    def _fix_data_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜"""
        try:
            df_fixed = df.copy()
            
            # ä¿®å¤OHLCåˆ—çš„æ•°æ®ç±»å‹
            ohlc_columns = ['open', 'high', 'low', 'close']
            for col in ohlc_columns:
                if col in df_fixed.columns:
                    # å¦‚æœåˆ—æ˜¯datetimeç±»å‹ä½†åº”è¯¥æ˜¯æ•°å€¼ï¼Œè¿›è¡Œä¿®å¤
                    if pd.api.types.is_datetime64_any_dtype(df_fixed[col]):
                        print(f"âš ï¸  å‘ç° {col} åˆ—ç±»å‹é”™è¯¯ï¼ˆåº”è¯¥æ˜¯æ•°å€¼ä½†å®é™…æ˜¯datetimeï¼‰ï¼Œå°è¯•ä¿®å¤...")
                        try:
                            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                            df_fixed[col] = pd.to_numeric(df_fixed[col], errors='coerce')
                            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……
                            if df_fixed[col].isna().all():
                                print(f"âŒ æ— æ³•ä¿®å¤ {col} åˆ—ï¼Œå°†ä½¿ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼")
                                df_fixed[col] = df_fixed[col].fillna(method='ffill')
                        except Exception as e:
                            print(f"âŒ ä¿®å¤ {col} åˆ—å¤±è´¥: {e}")
                            # ä½¿ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……
                            df_fixed[col] = df_fixed[col].fillna(method='ffill')
            
            return df_fixed
            
        except Exception as e:
            print(f"âŒ ä¿®å¤æ•°æ®ç±»å‹å¤±è´¥: {e}")
            return df
    
    def _fix_volume_issues(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¿®å¤æˆäº¤é‡é—®é¢˜"""
        try:
            df_fixed = df.copy()
            
            if 'volume' in df_fixed.columns:
                # å°†è´Ÿæ•°æˆäº¤é‡æ›¿æ¢ä¸º0
                df_fixed['volume'] = df_fixed['volume'].clip(lower=0)
            
            return df_fixed
        except Exception as e:
            print(f"âŒ ä¿®å¤æˆäº¤é‡é—®é¢˜å¤±è´¥: {e}")
            return df


# åˆ›å»ºå…¨å±€å®ä¾‹
batch_downloader = SmartBatchDownloader()
