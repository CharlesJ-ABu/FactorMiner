"""
æœºå™¨å­¦ä¹ å› å­æ„å»ºæ¨¡å—
ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•æ„å»ºå’Œä¼˜åŒ–å› å­
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional
from pathlib import Path
import pickle
import threading
import time
import signal
from datetime import datetime
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥tqdmï¼Œå¹¶é…ç½®ä¸ºé€‚åˆåå°è¿è¡Œ
try:
    from tqdm import tqdm
    # é…ç½®tqdmä¸ºé€‚åˆåå°è¿è¡Œçš„æ¨¡å¼
    import os
    if os.environ.get('TQDM_DISABLE') != '1':
        # æ£€æŸ¥tqdmç‰ˆæœ¬ï¼Œä½¿ç”¨å…¼å®¹çš„é…ç½®æ–¹å¼
        try:
            # æ–°ç‰ˆæœ¬tqdmæ”¯æŒset_defaults
            tqdm.set_defaults(
                position=0,  # å›ºå®šä½ç½®
                leave=True,  # ä¿ç•™è¿›åº¦æ¡
                ncols=80,    # å›ºå®šå®½åº¦
                ascii=True,  # ä½¿ç”¨ASCIIå­—ç¬¦ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦é—®é¢˜
                dynamic_ncols=False  # å›ºå®šåˆ—æ•°
            )
        except AttributeError:
            # æ—§ç‰ˆæœ¬tqdmä¸æ”¯æŒset_defaultsï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            print("ä½¿ç”¨é»˜è®¤tqdmé…ç½®ï¼ˆæ—§ç‰ˆæœ¬ï¼‰")
except ImportError:
    # å¦‚æœæ²¡æœ‰tqdmï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„è¿›åº¦æ˜¾ç¤º
    class SimpleProgress:
        def __init__(self, iterable=None, desc="", total=None, unit="", **kwargs):
            self.iterable = iterable
            self.desc = desc
            self.total = total or (len(iterable) if iterable else 0)
            self.unit = unit
            self.current = 0
            if desc:
                print(f"{desc}: å¼€å§‹å¤„ç†...")
        
        def __iter__(self):
            if self.iterable:
                for item in self.iterable:
                    yield item
                    self.current += 1
                    if self.current % max(1, self.total // 10) == 0:  # æ¯10%æ˜¾ç¤ºä¸€æ¬¡
                        print(f"{self.desc}: {self.current}/{self.total} ({self.current/self.total*100:.1f}%)")
        
        def write(self, text):
            print(text)
        
        def __enter__(self):
            return self
        
        def __exit__(self, *args):
            if self.desc:
                print(f"{self.desc}: å®Œæˆ ({self.current}/{self.total})")
    
    tqdm = SimpleProgress


class ProgressSimulator:
    """æ¨¡æ‹Ÿè¿ç»­è¿›åº¦æ›´æ–°çš„ç±»ï¼Œç”¨äºåœ¨æ¨¡å‹è®­ç»ƒæœŸé—´æä¾›tqdmå¼çš„è¿›åº¦æ•ˆæœ"""
    
    def __init__(self, progress_callback, stage, start_progress, end_progress, duration, message_template):
        self.progress_callback = progress_callback
        self.stage = stage
        self.start_progress = start_progress
        self.end_progress = end_progress
        self.duration = duration
        self.message_template = message_template
        self.current_progress = start_progress
        self.timer = None
        self.stop_flag = False
        
    def start(self):
        """å¼€å§‹æ¨¡æ‹Ÿè¿›åº¦"""
        self.stop_flag = False
        # ç«‹å³å‘é€ç¬¬ä¸€ä¸ªè¿›åº¦æ›´æ–°
        if self.progress_callback:
            message = self.message_template.format(progress=self.current_progress)
            print(f"ğŸ“ˆ ProgressSimulatorå¼€å§‹: {self.stage} -> {self.current_progress}% | {message}")
            self.progress_callback(stage=self.stage, progress=self.current_progress, message=message)
        self._update_progress()
        
    def stop(self):
        """åœæ­¢æ¨¡æ‹Ÿè¿›åº¦"""
        self.stop_flag = True
        if self.timer:
            self.timer.cancel()
        # ç¡®ä¿æœ€ç»ˆè¿›åº¦ä¸ºç»“æŸå€¼
        if self.progress_callback:
            self.progress_callback(stage=self.stage, progress=self.end_progress, message=self.message_template.format(progress=self.end_progress))
    
    def _update_progress(self):
        """å†…éƒ¨è¿›åº¦æ›´æ–°æ–¹æ³•"""
        if self.stop_flag:
            return
            
        if self.current_progress < self.end_progress:
            # è®¡ç®—ä¸‹ä¸€ä¸ªè¿›åº¦ç‚¹
            increment = max(1, (self.end_progress - self.start_progress) // 10)  # æ¯æ¬¡å¢åŠ è¾ƒå¤§æ­¥é•¿ï¼Œæ›´æ˜æ˜¾
            self.current_progress = min(self.current_progress + increment, self.end_progress)
            
            if self.progress_callback:
                message = self.message_template.format(progress=self.current_progress)
                print(f"ğŸ“ˆ ProgressSimulatoræ›´æ–°: {self.stage} -> {self.current_progress}% | {message}")
                self.progress_callback(stage=self.stage, progress=self.current_progress, message=message)
            
            # è°ƒåº¦ä¸‹ä¸€æ¬¡æ›´æ–° - é€‚ä¸­çš„æ›´æ–°é¢‘ç‡ï¼Œé¿å…è¿‡äºé¢‘ç¹
            interval = max(0.5, self.duration / max(1, (self.end_progress - self.start_progress)))
            self.timer = threading.Timer(interval, self._update_progress)
            self.timer.start()


class MLFactorBuilder:
    """
    æœºå™¨å­¦ä¹ å› å­æ„å»ºå™¨
    ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•æ„å»ºå’Œä¼˜åŒ–å› å­
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–æœºå™¨å­¦ä¹ å› å­æ„å»ºå™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or {}
        self.scaler = StandardScaler()
        self.models = {}
        # æ¨¡å‹æŒä¹…åŒ–ç›®å½•ï¼šfactorlib/models
        self.models_dir = (Path(__file__).parent.parent.parent / "factorlib" / "models")
        self.models_dir.mkdir(parents=True, exist_ok=True)
        print(f"âœ… MLå› å­æ¨¡å‹ç›®å½•: {self.models_dir}")

    def _artifact_path(self, factor_id: str) -> Path:
        artifact_path = self.models_dir / f"{factor_id}.pkl"
        print(f"ğŸ’¾ ä¿å­˜MLæ¨¡å‹artifact: {artifact_path}")
        return artifact_path

    def _save_model_artifact(
        self,
        factor_id: str,
        model,
        feature_columns: List[str],
        scaler: Optional[StandardScaler] = None,
        extra_info: Optional[Dict] = None
    ) -> None:
        try:
            artifact = {
                "model": model,
                "feature_columns": list(feature_columns),
                "scaler": scaler,
                "extra_info": extra_info or {},
                "saved_at": datetime.now().isoformat(),
                "factor_id": factor_id
            }
            with open(self._artifact_path(factor_id), "wb") as f:
                pickle.dump(artifact, f)
            print(f"âœ… æˆåŠŸä¿å­˜æ¨¡å‹æ–‡ä»¶: {factor_id}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹æ–‡ä»¶å¤±è´¥ {factor_id}: {e}")
        
    def build_ensemble_factors(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        æ„å»ºé›†æˆå­¦ä¹ å› å­
        
        Args:
            data: å¸‚åœºæ•°æ®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            é›†æˆå­¦ä¹ å› å­DataFrame
        """
        factors = {}
        progress_callback = kwargs.get('progress_callback')
        
        # å‡†å¤‡ç‰¹å¾ï¼ˆç»Ÿä¸€ç‰¹å¾ç®¡é“ï¼‰
        if progress_callback:
            progress_callback(stage='ml', progress=5, message='é›†æˆæ¨¡å‹è®­ç»ƒ: å‡†å¤‡ç‰¹å¾æ•°æ®...')
        try:
            from factor_miner.core.feature_pipeline import build_ml_features
            features = build_ml_features(data)
        except Exception:
            features = self._prepare_features(data)
        target = self._prepare_target(data)
        
        # ç§»é™¤NaNå€¼ - ç¡®ä¿ç´¢å¼•å¯¹é½
        features_nan_mask = features.isna().any(axis=1)
        target_nan_mask = target.isna()
        
        # é‡æ–°ç´¢å¼•ä»¥ç¡®ä¿å¯¹é½
        common_index = features.index.intersection(target.index)
        features_nan_aligned = features_nan_mask.reindex(common_index, fill_value=True)
        target_nan_aligned = target_nan_mask.reindex(common_index, fill_value=True)
        
        valid_idx = ~(features_nan_aligned | target_nan_aligned)
        features_clean = features.reindex(common_index).loc[valid_idx]
        target_clean = target.reindex(common_index).loc[valid_idx]
        
        if len(features_clean) < 100:
            print("æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•æ„å»ºé›†æˆå› å­")
            return pd.DataFrame(factors, index=data.index)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        if progress_callback:
            progress_callback(stage='ml', progress=15, message='é›†æˆæ¨¡å‹è®­ç»ƒ: æ ‡å‡†åŒ–ç‰¹å¾...')
        
        print(f"ğŸ” å¼€å§‹æ ‡å‡†åŒ–ç‰¹å¾ï¼Œæ•°æ®å½¢çŠ¶: {features_clean.shape}")
        try:
            features_scaled = self.scaler.fit_transform(features_clean)
            print(f"âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆï¼Œå½¢çŠ¶: {features_scaled.shape}")
        except Exception as e:
            print(f"âŒ ç‰¹å¾æ ‡å‡†åŒ–å¤±è´¥: {e}")
            raise
        
        # æ„å»ºå¤šä¸ªæ¨¡å‹ - ä¼˜åŒ–å‚æ•°é¿å…å¡ä½
        models = {
            'random_forest': RandomForestRegressor(
                n_estimators=50,      # å‡å°‘æ ‘çš„æ•°é‡
                max_depth=10,         # é™åˆ¶æ ‘æ·±åº¦
                min_samples_split=10, # å¢åŠ åˆ†è£‚æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°
                min_samples_leaf=5,   # å¢åŠ å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
                random_state=42,
                n_jobs=1             # ä½¿ç”¨å•çº¿ç¨‹ï¼Œé¿å…å¤šçº¿ç¨‹é—®é¢˜
            ),
            'gradient_boosting': GradientBoostingRegressor(
                n_estimators=50,      # å‡å°‘æ ‘çš„æ•°é‡
                max_depth=6,          # é™åˆ¶æ ‘æ·±åº¦
                learning_rate=0.1,    # é™ä½å­¦ä¹ ç‡
                random_state=42
            ),
            'ridge': Ridge(alpha=1.0),
            'lasso': Lasso(alpha=0.01)
        }
        
        # è®­ç»ƒæ¨¡å‹å¹¶ç”Ÿæˆé¢„æµ‹
        for model_idx, (name, model) in enumerate(tqdm(models.items(), desc="è®­ç»ƒé›†æˆæ¨¡å‹", unit="æ¨¡å‹")):
            base_progress = 20 + (model_idx / len(models)) * 60  # 20-80%
            
            # ç«‹å³å‘é€å¼€å§‹è®­ç»ƒçš„è¿›åº¦æ›´æ–°
            if progress_callback:
                progress_callback(stage='ml', progress=int(base_progress), message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: å¼€å§‹è®­ç»ƒ {name} æ¨¡å‹...')
                # ç«‹å³å‘é€ä¸€ä¸ªç¨é«˜çš„è¿›åº¦ï¼Œè®©ç”¨æˆ·çœ‹åˆ°å˜åŒ–
                progress_callback(stage='ml', progress=int(base_progress + 1), message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} åˆå§‹åŒ–...')
            
            try:
                print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {name} æ¨¡å‹...")
                
                # å¼ºåˆ¶æ›´æ–°è¿›åº¦åˆ°å¼€å§‹è®­ç»ƒ
                if progress_callback:
                    progress_callback(stage='ml', progress=int(base_progress + 2), 
                                   message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} å¼€å§‹è®­ç»ƒ...')
                
                # å¯åŠ¨è¿›åº¦æ¨¡æ‹Ÿå™¨æ¨¡æ‹Ÿæ¨¡å‹è®­ç»ƒè¿‡ç¨‹
                simulator = None
                if progress_callback:
                    simulator = ProgressSimulator(
                        progress_callback=progress_callback,
                        stage='ml',
                        start_progress=int(base_progress + 2),
                        end_progress=int(base_progress + 12),
                        duration=1.0,  # å‡å°‘åˆ°1ç§’ï¼Œæ›´å¿«å“åº”
                        message_template=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} è®­ç»ƒä¸­... ' + '{progress}%'
                    )
                    simulator.start()
                
                # å¯åŠ¨ç´§æ€¥è¿›åº¦æ›´æ–°çº¿ç¨‹ï¼Œé˜²æ­¢å¡ä½
                def emergency_progress_update():
                    """ç´§æ€¥è¿›åº¦æ›´æ–°ï¼Œé˜²æ­¢è®­ç»ƒå¡ä½"""
                    for i in range(10):  # 10æ¬¡æ›´æ–°
                        time.sleep(3)  # æ¯3ç§’æ›´æ–°ä¸€æ¬¡
                        if progress_callback:
                            emergency_progress = int(base_progress + 2 + i)
                            progress_callback(stage='ml', progress=emergency_progress, 
                                           message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} ç´§æ€¥è¿›åº¦æ›´æ–° {i+1}/10')
                        print(f"  ğŸš¨ ç´§æ€¥è¿›åº¦æ›´æ–° {i+1}/10: {name} æ¨¡å‹è®­ç»ƒä¸­...")
                
                emergency_thread = threading.Thread(target=emergency_progress_update, daemon=True)
                emergency_thread.start()
                
                # åˆ†æ‰¹è®­ç»ƒé€»è¾‘ - é¿å…å¤§æ•°æ®é›†å¡ä½
                if len(features_clean) > 5000 and hasattr(model, 'partial_fit'):
                    print(f"ğŸ“Š æ•°æ®é‡è¾ƒå¤§({len(features_clean)}æ¡)ï¼Œä½¿ç”¨åˆ†æ‰¹è®­ç»ƒ...")
                    batch_size = 2000
                    total_batches = (len(features_clean) + batch_size - 1) // batch_size
                    
                    # è¶…æ—¶ä¿æŠ¤å‡½æ•°
                    def timeout_handler(signum, frame):
                        raise TimeoutError(f"{name} æ¨¡å‹è®­ç»ƒè¶…æ—¶")
                    
                    # è®¾ç½®30ç§’è¶…æ—¶
                    signal.signal(signal.SIGALRM, timeout_handler)
                    signal.alarm(30)
                    
                    try:
                        for batch_idx in range(0, len(features_clean), batch_size):
                            batch_end = min(batch_idx + batch_size, len(features_clean))
                            batch_features = features_scaled[batch_idx:batch_end]
                            batch_target = target_clean.iloc[batch_idx:batch_end]
                            
                            print(f"  ğŸ”„ è®­ç»ƒæ‰¹æ¬¡ {batch_idx//batch_size + 1}/{total_batches} ({batch_idx}-{batch_end})")
                            
                            if batch_idx == 0:
                                # ç¬¬ä¸€æ‰¹æ•°æ®ï¼Œåˆå§‹åŒ–æ¨¡å‹
                                model.partial_fit(batch_features, batch_target)
                            else:
                                # åç»­æ‰¹æ¬¡ï¼Œç»§ç»­è®­ç»ƒ
                                model.partial_fit(batch_features, batch_target)
                            
                            # å¼ºåˆ¶æ›´æ–°è¿›åº¦ï¼Œé˜²æ­¢å¡ä½
                            batch_progress = (batch_idx + batch_size) / len(features_clean)
                            if progress_callback:
                                current_progress = int(base_progress + batch_progress * 10)
                                progress_callback(stage='ml', progress=current_progress, 
                                               message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} åˆ†æ‰¹è®­ç»ƒä¸­... {batch_idx//batch_size + 1}/{total_batches}')
                            
                            print(f"  âœ… æ‰¹æ¬¡ {batch_idx//batch_size + 1}/{total_batches} å®Œæˆ")
                    finally:
                        # å–æ¶ˆè¶…æ—¶
                        signal.alarm(0)
                else:
                    # æ•°æ®é‡ä¸å¤§ï¼Œç›´æ¥è®­ç»ƒ
                    print(f"ğŸ“Š æ•°æ®é‡é€‚ä¸­({len(features_clean)}æ¡)ï¼Œç›´æ¥è®­ç»ƒ...")
                    model.fit(features_scaled, target_clean)
                
                # åœæ­¢è¿›åº¦æ¨¡æ‹Ÿå™¨
                if simulator:
                    simulator.stop()
                
                print(f"âœ… {name} æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå¼€å§‹ç”Ÿæˆé¢„æµ‹...")
                
                # å¼ºåˆ¶æ›´æ–°è¿›åº¦
                if progress_callback:
                    progress_callback(stage='ml', progress=int(base_progress + 14), 
                                   message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} ç”Ÿæˆé¢„æµ‹...')
                
                predictions = model.predict(features_scaled)
                
                # åˆ›å»ºå› å­åºåˆ—
                factor_series = pd.Series(index=data.index, dtype=float)
                factor_series.loc[valid_idx] = predictions
                
                factor_id = f"ensemble_{name}"
                factors[factor_id] = factor_series
                self.models[name] = model

                if progress_callback:
                    progress_callback(stage='ml', progress=int(base_progress + 12), message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} ä¿å­˜æ¨¡å‹...')

                # æŒä¹…åŒ–æ¨¡å‹ï¼ˆç”¨äºåç»­é€šè¿‡coreåŠ è½½æ¨ç†ï¼‰
                self._save_model_artifact(
                    factor_id=factor_id,
                    model=model,
                    feature_columns=list(features_clean.columns),
                    scaler=self.scaler
                )
                
                tqdm.write(f"âœ“ æˆåŠŸè®­ç»ƒ {name} æ¨¡å‹")
                if progress_callback:
                    completion_progress = 20 + ((model_idx + 1) / len(models)) * 60
                    progress_callback(stage='ml', progress=int(completion_progress), message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} å®Œæˆ')
            
            except TimeoutError as e:
                tqdm.write(f"â° {name} æ¨¡å‹è®­ç»ƒè¶…æ—¶: {e}")
                if progress_callback:
                    progress_callback(stage='ml', progress=int(base_progress + 15), 
                                   message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} è¶…æ—¶ï¼Œè·³è¿‡æ­¤æ¨¡å‹')
                continue
            except Exception as e:
                tqdm.write(f"âœ— è®­ç»ƒ {name} æ¨¡å‹å¤±è´¥: {e}")
                if progress_callback:
                    progress_callback(stage='ml', progress=int(base_progress + 15), 
                                   message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} å¤±è´¥ï¼Œè·³è¿‡æ­¤æ¨¡å‹')
                continue
        
        if progress_callback:
            progress_callback(stage='ml', progress=85, message='é›†æˆæ¨¡å‹è®­ç»ƒ: ä¿å­˜æ¨¡å‹å®Œæˆ')
        
        # éªŒè¯æ¨¡å‹æ–‡ä»¶æ˜¯å¦æˆåŠŸä¿å­˜
        for factor_id in factors.keys():
            artifact_path = self._artifact_path(factor_id)
            if artifact_path.exists():
                print(f"âœ… éªŒè¯: {factor_id} æ¨¡å‹æ–‡ä»¶å­˜åœ¨ ({artifact_path.stat().st_size} bytes)")
            else:
                print(f"âŒ éªŒè¯å¤±è´¥: {factor_id} æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        
        return pd.DataFrame(factors, index=data.index)

    def build_pca_factors(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        æ„å»ºPCAå› å­
        
        Args:
            data: å¸‚åœºæ•°æ®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            PCAå› å­DataFrame
        """
        factors = {}
        progress_callback = kwargs.get('progress_callback')
        
        # å‡†å¤‡ç‰¹å¾
        if progress_callback:
            progress_callback(stage='ml', progress=5, message='PCA é™ç»´: å‡†å¤‡ç‰¹å¾æ•°æ®...')
        print("å‡†å¤‡PCAç‰¹å¾æ•°æ®...")
        features = self._prepare_features(data)
        print(f"PCAç‰¹å¾æ•°æ®å½¢çŠ¶: {features.shape}")
        
        if features.empty:
            print("âŒ PCAç‰¹å¾æ•°æ®ä¸ºç©ºï¼Œæ— æ³•æ„å»ºPCAå› å­")
            return pd.DataFrame(factors, index=data.index)
        
        # ç§»é™¤NaNå€¼
        valid_idx = ~features.isna().any(axis=1)
        features_clean = features.loc[valid_idx]
        
        print(f"PCAæœ‰æ•ˆæ•°æ®ç´¢å¼•æ•°é‡: {valid_idx.sum()}")
        
        if len(features_clean) < 50:
            print(f"âŒ PCAæ•°æ®é‡ä¸è¶³50æ¡ï¼ˆåªæœ‰{len(features_clean)}æ¡ï¼‰ï¼Œæ— æ³•æ„å»ºPCAå› å­")
            return pd.DataFrame(factors, index=data.index)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        if progress_callback:
            progress_callback(stage='ml', progress=20, message='PCA é™ç»´: æ ‡å‡†åŒ–ç‰¹å¾æ•°æ®...')
        print("æ ‡å‡†åŒ–PCAç‰¹å¾æ•°æ®...")
        features_scaled = self.scaler.fit_transform(features_clean)
        
        # è®¡ç®—PCA
        n_components = kwargs.get('n_components', min(10, features_clean.shape[1]))
        print(f"PCAç»„ä»¶æ•°: {n_components}")
        
        try:
            # å¯åŠ¨PCAè¿›åº¦æ¨¡æ‹Ÿå™¨
            simulator = None
            if progress_callback:
                simulator = ProgressSimulator(
                    progress_callback=progress_callback,
                    stage='ml',
                    start_progress=40,
                    end_progress=55,
                    duration=1.5,  # 1.5ç§’çš„æ¨¡æ‹ŸPCAæ—¶é—´
                    message_template='PCA é™ç»´: ä¸»æˆåˆ†åˆ†æè¿›è¡Œä¸­... {progress}%'
                )
                simulator.start()
            
            print("æ‰§è¡ŒPCAé™ç»´...")
            pca = PCA(n_components=n_components)
            pca_components = pca.fit_transform(features_scaled)
            
            # åœæ­¢PCAè¿›åº¦æ¨¡æ‹Ÿå™¨
            if simulator:
                simulator.stop()
            
            print(f"PCAé™ç»´å®Œæˆï¼Œå½¢çŠ¶: {pca_components.shape}")
            
            # åˆ›å»ºå› å­
            if progress_callback:
                progress_callback(stage='ml', progress=60, message='PCA é™ç»´: ç”ŸæˆPCAå› å­...')
            for i in tqdm(range(n_components), desc="åˆ›å»ºPCAå› å­", unit="å› å­"):
                if progress_callback and i % max(1, n_components // 5) == 0:
                    progress_pct = 60 + int((i / n_components) * 30)
                    progress_callback(stage='ml', progress=progress_pct, message=f'PCA é™ç»´: åˆ›å»ºç¬¬ {i+1}/{n_components} ä¸ªç»„ä»¶')
                
                factor_series = pd.Series(index=data.index, dtype=float)
                factor_series.loc[valid_idx] = pca_components[:, i]
                factor_id = f'pca_component_{i+1}'
                factors[factor_id] = factor_series
                
                # è®¾ç½®å› å­å…ƒæ•°æ®ï¼ˆç±»åˆ«ã€æè¿°ç­‰ï¼‰
                if hasattr(factor_series, 'metadata'):
                    factor_series.metadata = {
                        'category': 'ml',
                        'subcategory': 'pca',
                        'component_index': i
                    }
            
            # ä¿å­˜è§£é‡Šæ–¹å·®æ¯”ä¾‹
            explained_variance_ratio = pca.explained_variance_ratio_
            tqdm.write(f"PCAè§£é‡Šæ–¹å·®æ¯”ä¾‹: {explained_variance_ratio[:5]}")
            print(f"å‰5ä¸ªç»„ä»¶ç´¯è®¡è§£é‡Šæ–¹å·®: {explained_variance_ratio[:5].sum():.3f}")
            
            # ä¿å­˜PCAæ¨¡å‹å’Œç‰¹å¾ä¿¡æ¯
            if progress_callback:
                progress_callback(stage='ml', progress=90, message='PCA é™ç»´: ä¿å­˜æ¨¡å‹...')
            
            # ä¸ºæ¯ä¸ªPCAç»„ä»¶ä¿å­˜æ¨¡å‹
            for i in range(n_components):
                factor_id = f'pca_component_{i+1}'
                try:
                    # ä¿å­˜PCAæ¨¡å‹å’Œç‰¹å¾ä¿¡æ¯
                    self._save_model_artifact(
                        factor_id=factor_id,
                        model=pca,
                        feature_columns=list(features_clean.columns),
                        scaler=self.scaler,
                        extra_info={
                            'component_index': i,
                            'explained_variance_ratio': float(explained_variance_ratio[i]),
                            'cumulative_variance_ratio': float(explained_variance_ratio[:i+1].sum()),
                            'n_components': n_components,
                            'n_features': len(features_clean.columns)
                        }
                    )
                    print(f"âœ… å·²ä¿å­˜PCAç»„ä»¶ {i+1} æ¨¡å‹")
                except Exception as e:
                    print(f"âŒ ä¿å­˜PCAç»„ä»¶ {i+1} æ¨¡å‹å¤±è´¥: {e}")
                
        except Exception as e:
            print(f"âŒ PCAè®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"âœ… PCAå› å­æ„å»ºå®Œæˆï¼Œå…± {len(factors)} ä¸ªå› å­")
        
        # éªŒè¯æ¨¡å‹æ–‡ä»¶æ˜¯å¦æˆåŠŸä¿å­˜
        for i in range(n_components):
            factor_id = f'pca_component_{i+1}'
            artifact_path = self._artifact_path(factor_id)
            if artifact_path.exists():
                print(f"âœ… éªŒè¯: {factor_id} æ¨¡å‹æ–‡ä»¶å­˜åœ¨ ({artifact_path.stat().st_size} bytes)")
            else:
                print(f"âŒ éªŒè¯å¤±è´¥: {factor_id} æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        
        return pd.DataFrame(factors, index=data.index)
    
    def build_feature_selection_factors(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        æ„å»ºç‰¹å¾é€‰æ‹©å› å­
        
        Args:
            data: å¸‚åœºæ•°æ®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç‰¹å¾é€‰æ‹©å› å­DataFrame
        """
        factors = {}
        progress_callback = kwargs.get('progress_callback')
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
        if progress_callback:
            progress_callback(stage='ml', progress=5, message='ç‰¹å¾é€‰æ‹©: å‡†å¤‡ç‰¹å¾æ•°æ®...')
        features = self._prepare_features(data)
        target = self._prepare_target(data)
        
        # ç§»é™¤NaNå€¼ - ç¡®ä¿ç´¢å¼•å¯¹é½
        features_nan_mask = features.isna().any(axis=1)
        target_nan_mask = target.isna()
        
        # é‡æ–°ç´¢å¼•ä»¥ç¡®ä¿å¯¹é½
        common_index = features.index.intersection(target.index)
        features_nan_aligned = features_nan_mask.reindex(common_index, fill_value=True)
        target_nan_aligned = target_nan_mask.reindex(common_index, fill_value=True)
        
        valid_idx = ~(features_nan_aligned | target_nan_aligned)
        features_clean = features.reindex(common_index).loc[valid_idx]
        target_clean = target.reindex(common_index).loc[valid_idx]
        
        if len(features_clean) < 50:
            print("æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•æ„å»ºç‰¹å¾é€‰æ‹©å› å­")
            return pd.DataFrame(factors, index=data.index)
        
        # ç‰¹å¾é€‰æ‹©æ–¹æ³•
        selection_methods = {
            'f_regression': f_regression,
            'mutual_info': mutual_info_regression
        }
        
        k_best = kwargs.get('k_best', min(20, features_clean.shape[1]))
        
        for method_idx, (method_name, method_func) in enumerate(tqdm(selection_methods.items(), desc="ç‰¹å¾é€‰æ‹©", unit="æ–¹æ³•")):
            try:
                if progress_callback:
                    base_progress = 10 + (method_idx / len(selection_methods)) * 70
                    progress_callback(stage='ml', progress=int(base_progress), message=f'ç‰¹å¾é€‰æ‹©: æ‰§è¡Œ {method_name} ç®—æ³•...')
                
                # ç‰¹å¾é€‰æ‹©
                selector = SelectKBest(score_func=method_func, k=k_best)
                selector.fit(features_clean, target_clean)
                
                # è·å–é€‰ä¸­çš„ç‰¹å¾
                selected_features = features_clean.iloc[:, selector.get_support()]
                
                # è®¡ç®—é€‰ä¸­ç‰¹å¾çš„ç»„åˆ
                if len(selected_features.columns) > 0:
                    # ç®€å•å¹³å‡ç»„åˆ
                    factor_series = pd.Series(index=data.index, dtype=float)
                    factor_series.loc[valid_idx] = selected_features.mean(axis=1)
                    factors[f'feature_selection_{method_name}_mean'] = factor_series
                    
                    # åŠ æƒç»„åˆï¼ˆåŸºäºç‰¹å¾é‡è¦æ€§ï¼‰
                    scores = selector.scores_[selector.get_support()]
                    weights = scores / scores.sum()
                    weighted_factor = (selected_features * weights).sum(axis=1)
                    
                    factor_series = pd.Series(index=data.index, dtype=float)
                    factor_series.loc[valid_idx] = weighted_factor
                    factors[f'feature_selection_{method_name}_weighted'] = factor_series
                
                tqdm.write(f"âœ“ æˆåŠŸå®Œæˆ {method_name} ç‰¹å¾é€‰æ‹©")
                if progress_callback:
                    completion_progress = 10 + ((method_idx + 1) / len(selection_methods)) * 70
                    progress_callback(stage='ml', progress=int(completion_progress), message=f'ç‰¹å¾é€‰æ‹©: {method_name} å®Œæˆ, é€‰æ‹©äº† {len(selected_features.columns)} ä¸ªç‰¹å¾')
                
                # ä¿å­˜ç‰¹å¾é€‰æ‹©æ¨¡å‹
                try:
                    # ä¿å­˜å¹³å‡ç»„åˆå› å­æ¨¡å‹
                    mean_factor_id = f'feature_selection_{method_name}_mean'
                    self._save_model_artifact(
                        factor_id=mean_factor_id,
                        model=selector,
                        feature_columns=list(features_clean.columns),
                        scaler=self.scaler,
                        extra_info={
                            'selection_method': method_name,
                            'k_best': k_best,
                            'selected_features': list(selected_features.columns),
                            'feature_scores': selector.scores_.tolist(),
                            'combination_method': 'mean'
                        }
                    )
                    print(f"âœ… å·²ä¿å­˜ç‰¹å¾é€‰æ‹©å¹³å‡å› å­æ¨¡å‹: {mean_factor_id}")
                    
                    # ä¿å­˜åŠ æƒç»„åˆå› å­æ¨¡å‹
                    weighted_factor_id = f'feature_selection_{method_name}_weighted'
                    self._save_model_artifact(
                        factor_id=weighted_factor_id,
                        model=selector,
                        feature_columns=list(features_clean.columns),
                        scaler=self.scaler,
                        extra_info={
                            'selection_method': method_name,
                            'k_best': k_best,
                            'selected_features': list(selected_features.columns),
                            'feature_scores': selector.scores_.tolist(),
                            'combination_method': 'weighted',
                            'weights': weights.tolist()
                        }
                    )
                    print(f"âœ… å·²ä¿å­˜ç‰¹å¾é€‰æ‹©åŠ æƒå› å­æ¨¡å‹: {weighted_factor_id}")
                    
                except Exception as e:
                    print(f"âŒ ä¿å­˜ç‰¹å¾é€‰æ‹©æ¨¡å‹å¤±è´¥: {e}")
                
            except Exception:
                tqdm.write(f"âœ— ç‰¹å¾é€‰æ‹© {method_name} å¤±è´¥")
                continue
        
        # éªŒè¯æ¨¡å‹æ–‡ä»¶æ˜¯å¦æˆåŠŸä¿å­˜
        for factor_id in factors.keys():
            artifact_path = self._artifact_path(factor_id)
            if artifact_path.exists():
                print(f"âœ… éªŒè¯: {factor_id} æ¨¡å‹æ–‡ä»¶å­˜åœ¨ ({artifact_path.stat().st_size} bytes)")
            else:
                print(f"âŒ éªŒè¯å¤±è´¥: {factor_id} æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        
        return pd.DataFrame(factors, index=data.index)
    
    def build_rolling_ml_factors(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        æ„å»ºæ»šåŠ¨æœºå™¨å­¦ä¹ å› å­
        
        Args:
            data: å¸‚åœºæ•°æ®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            æ»šåŠ¨MLå› å­DataFrame
        """
        factors = {}
        progress_callback = kwargs.get('progress_callback')
        
        # å‡†å¤‡ç‰¹å¾
        if progress_callback:
            progress_callback(stage='ml', progress=5, message='æ»šåŠ¨ML: å‡†å¤‡æ—¶é—´åºåˆ—ç‰¹å¾...')
        features = self._prepare_features(data)
        target = self._prepare_target(data)
        
        # æ»šåŠ¨çª—å£å‚æ•°
        window = kwargs.get('window', 252)  # ä¸€å¹´
        step = kwargs.get('step', 21)  # ä¸€ä¸ªæœˆ
        
        # åˆå§‹åŒ–å› å­åºåˆ—
        factor_series = pd.Series(index=data.index, dtype=float)
        
        # æ»šåŠ¨è®­ç»ƒå’Œé¢„æµ‹
        total_steps = len(range(window, len(data), step))
        for step_idx, i in enumerate(tqdm(range(window, len(data), step), desc="æ»šåŠ¨MLè®­ç»ƒ", total=total_steps, unit="çª—å£")):
            # æ›´é¢‘ç¹çš„è¿›åº¦æ›´æ–°
            if progress_callback:
                progress_pct = 10 + int((step_idx / total_steps) * 80)
                progress_callback(stage='ml', progress=progress_pct, message=f'æ»šåŠ¨ML: çª—å£ {step_idx+1}/{total_steps}, è®­ç»ƒæœŸ {i-window}~{i}')
                
                # åœ¨çª—å£è®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ ä¸­é—´è¿›åº¦æ›´æ–°
                if step_idx % max(1, total_steps // 40) == 0:  # æ¯2.5%æ›´æ–°ä¸€æ¬¡
                    progress_callback(stage='ml', progress=progress_pct + 1, message=f'æ»šåŠ¨ML: å¤„ç†æ•°æ®çª—å£ {step_idx+1}/{total_steps}...')
            
            # è®­ç»ƒçª—å£
            train_features = features.iloc[i-window:i]
            train_target = target.iloc[i-window:i]
            
            # ç§»é™¤NaNå€¼ - ç¡®ä¿ç´¢å¼•å¯¹é½
            features_nan_mask = train_features.isna().any(axis=1)
            target_nan_mask = train_target.isna()
            
            # é‡æ–°ç´¢å¼•ä»¥ç¡®ä¿å¯¹é½
            common_index = train_features.index.intersection(train_target.index)
            features_nan_aligned = features_nan_mask.reindex(common_index, fill_value=True)
            target_nan_aligned = target_nan_mask.reindex(common_index, fill_value=True)
            
            valid_idx = ~(features_nan_aligned | target_nan_aligned)
            train_features_clean = train_features.reindex(common_index).loc[valid_idx]
            train_target_clean = train_target.reindex(common_index).loc[valid_idx]
            
            if len(train_features_clean) < 50:
                continue
            
            # æ ‡å‡†åŒ–ç‰¹å¾
            features_scaled = self.scaler.fit_transform(train_features_clean)
            
            # è®­ç»ƒæ¨¡å‹
            model = RandomForestRegressor(n_estimators=50, random_state=42)
            
            try:
                model.fit(features_scaled, train_target_clean)
                
                # ä¿å­˜æœ€åä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹å’Œç‰¹å¾
                last_model = model
                last_features = train_features_clean
                
                # é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹
                if i < len(data):
                    next_features = features.iloc[i:i+1]
                    if not next_features.isna().any().any():
                        next_features_scaled = self.scaler.transform(next_features)
                        prediction = model.predict(next_features_scaled)[0]
                        factor_series.iloc[i] = prediction
                
            except Exception:
                continue
        
        factors['rolling_ml_factor'] = factor_series
        
        # ä¿å­˜æ»šåŠ¨MLæ¨¡å‹
        try:
            # ä¿å­˜æœ€åä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹ä½œä¸ºä»£è¡¨æ€§æ¨¡å‹
            if 'last_model' in locals() and 'last_features' in locals():
                self._save_model_artifact(
                    factor_id='rolling_ml_factor',
                    model=last_model,
                    feature_columns=list(last_features.columns),
                    scaler=self.scaler,
                    extra_info={
                        'window_size': window,
                        'step_size': step,
                        'total_steps': total_steps,
                        'model_type': 'RandomForestRegressor',
                        'training_samples': len(last_features)
                    }
                )
                print(f"âœ… å·²ä¿å­˜æ»šåŠ¨MLå› å­æ¨¡å‹")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ»šåŠ¨MLå› å­æ¨¡å‹å¤±è´¥: {e}")
        
        # éªŒè¯æ¨¡å‹æ–‡ä»¶æ˜¯å¦æˆåŠŸä¿å­˜
        for factor_id in factors.keys():
            artifact_path = self._artifact_path(factor_id)
            if artifact_path.exists():
                print(f"âœ… éªŒè¯: {factor_id} æ¨¡å‹æ–‡ä»¶å­˜åœ¨ ({artifact_path.stat().st_size} bytes)")
            else:
                print(f"âŒ éªŒè¯å¤±è´¥: {factor_id} æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        
        return pd.DataFrame(factors, index=data.index)
    
    def build_adaptive_ml_factors(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        æ„å»ºè‡ªé€‚åº”æœºå™¨å­¦ä¹ å› å­
        
        Args:
            data: å¸‚åœºæ•°æ®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            è‡ªé€‚åº”MLå› å­DataFrame
        """
        factors = {}
        progress_callback = kwargs.get('progress_callback')
        
        # å‡†å¤‡ç‰¹å¾
        if progress_callback:
            progress_callback(stage='ml', progress=5, message='è‡ªé€‚åº”MLè®­ç»ƒ: å‡†å¤‡ç‰¹å¾ä¸ç¯å¢ƒæ£€æµ‹...')
        features = self._prepare_features(data)
        target = self._prepare_target(data)
        
        # è·å–æ­£ç¡®çš„åˆ—å
        close_col = 'close' if 'close' in data.columns else 'S_DQ_CLOSE'
        
        # å¸‚åœºçŠ¶æ€æ£€æµ‹
        returns = data[close_col].pct_change()
        volatility = returns.rolling(window=20).std()
        
        # æ ¹æ®æ³¢åŠ¨ç‡è°ƒæ•´æ¨¡å‹å‚æ•°
        high_vol_threshold = volatility.quantile(0.8)
        low_vol_threshold = volatility.quantile(0.2)
        
        # åˆå§‹åŒ–å› å­åºåˆ—
        factor_series = pd.Series(index=data.index, dtype=float)
        
        window = kwargs.get('window', 252)
        
        total_steps = len(range(window, len(data)))
        for step_idx, i in enumerate(tqdm(range(window, len(data)), desc="è‡ªé€‚åº”MLè®­ç»ƒ", total=total_steps, unit="æ—¶é—´ç‚¹")):
            if progress_callback and step_idx % max(1, total_steps // 50) == 0:  # æ¯2%æŠ¥å‘Šä¸€æ¬¡
                progress_pct = 10 + int((step_idx / total_steps) * 80)
                current_vol = volatility.iloc[i]
                vol_level = "é«˜æ³¢åŠ¨" if current_vol > high_vol_threshold else "ä½æ³¢åŠ¨" if current_vol < low_vol_threshold else "ä¸­ç­‰æ³¢åŠ¨"
                progress_callback(stage='ml', progress=progress_pct, message=f'è‡ªé€‚åº”MLè®­ç»ƒ: {step_idx+1}/{total_steps}, {vol_level}ç¯å¢ƒ')
            
            # é¢å¤–çš„ç»†ç²’åº¦æ›´æ–°
            if progress_callback and step_idx % max(1, total_steps // 100) == 0:  # æ¯1%ä¸€æ¬¡å¾®æ›´æ–°
                progress_pct = 10 + int((step_idx / total_steps) * 80)
                progress_callback(stage='ml', progress=progress_pct, message=f'è‡ªé€‚åº”MLè®­ç»ƒ: åˆ†ææ—¶é—´ç‚¹ {step_idx+1}/{total_steps}...')
            
            # è·å–å½“å‰æ³¢åŠ¨ç‡
            current_vol = volatility.iloc[i]
            
            # æ ¹æ®æ³¢åŠ¨ç‡é€‰æ‹©æ¨¡å‹å‚æ•°
            if current_vol > high_vol_threshold:
                # é«˜æ³¢åŠ¨ç‡ç¯å¢ƒï¼šä½¿ç”¨æ›´ä¿å®ˆçš„æ¨¡å‹
                model = Ridge(alpha=10.0)
            elif current_vol < low_vol_threshold:
                # ä½æ³¢åŠ¨ç‡ç¯å¢ƒï¼šä½¿ç”¨æ›´æ¿€è¿›çš„æ¨¡å‹
                model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)
            else:
                # ä¸­ç­‰æ³¢åŠ¨ç‡ç¯å¢ƒï¼šä½¿ç”¨å¹³è¡¡çš„æ¨¡å‹
                model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
            
            # è®­ç»ƒæ•°æ®
            train_features = features.iloc[i-window:i]
            train_target = target.iloc[i-window:i]
            
            # ç§»é™¤NaNå€¼ - ç¡®ä¿ç´¢å¼•å¯¹é½
            features_nan_mask = train_features.isna().any(axis=1)
            target_nan_mask = train_target.isna()
            
            # é‡æ–°ç´¢å¼•ä»¥ç¡®ä¿å¯¹é½
            common_index = train_features.index.intersection(train_target.index)
            features_nan_aligned = features_nan_mask.reindex(common_index, fill_value=True)
            target_nan_aligned = target_nan_mask.reindex(common_index, fill_value=True)
            
            valid_idx = ~(features_nan_aligned | target_nan_aligned)
            train_features_clean = train_features.reindex(common_index).loc[valid_idx]
            train_target_clean = train_target.reindex(common_index).loc[valid_idx]
            
            if len(train_features_clean) < 50:
                continue
            
            # æ ‡å‡†åŒ–ç‰¹å¾
            features_scaled = self.scaler.fit_transform(train_features_clean)
            
            try:
                model.fit(features_scaled, train_target_clean)
                
                # ä¿å­˜æœ€åä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹å’Œç‰¹å¾
                last_adaptive_model = model
                last_adaptive_features = train_features_clean
                
                # é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹
                if i < len(data):
                    next_features = features.iloc[i:i+1]
                    if not next_features.isna().any().any():
                        next_features_scaled = self.scaler.transform(next_features)
                        prediction = model.predict(next_features_scaled)[0]
                        factor_series.iloc[i] = prediction
                
            except Exception:
                continue
        
        factors['adaptive_ml_factor'] = factor_series
        
        # ä¿å­˜è‡ªé€‚åº”MLæ¨¡å‹
        try:
            if 'last_adaptive_model' in locals() and 'last_adaptive_features' in locals():
                self._save_model_artifact(
                    factor_id='adaptive_ml_factor',
                    model=last_adaptive_model,
                    feature_columns=list(last_adaptive_features.columns),
                    scaler=self.scaler,
                    extra_info={
                        'window_size': window,
                        'model_type': 'RandomForestRegressor',
                        'training_samples': len(last_adaptive_features),
                        'adaptation_method': 'volatility_based'
                    }
                )
                print(f"âœ… å·²ä¿å­˜è‡ªé€‚åº”MLå› å­æ¨¡å‹")
        except Exception as e:
            print(f"âŒ ä¿å­˜è‡ªé€‚åº”MLå› å­æ¨¡å‹å¤±è´¥: {e}")
        
        # éªŒè¯æ¨¡å‹æ–‡ä»¶æ˜¯å¦æˆåŠŸä¿å­˜
        for factor_id in factors.keys():
            artifact_path = self._artifact_path(factor_id)
            if artifact_path.exists():
                print(f"âœ… éªŒè¯: {factor_id} æ¨¡å‹æ–‡ä»¶å­˜åœ¨ ({artifact_path.stat().st_size} bytes)")
            else:
                print(f"âŒ éªŒè¯å¤±è´¥: {factor_id} æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        
        return pd.DataFrame(factors, index=data.index)
    
    def _prepare_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        å‡†å¤‡ç‰¹å¾æ•°æ®
        
        Args:
            data: å¸‚åœºæ•°æ®
            
        Returns:
            ç‰¹å¾DataFrame
        """
        features = pd.DataFrame(index=data.index)
        
        # è·å–æ­£ç¡®çš„åˆ—å
        close_col = 'close' if 'close' in data.columns else 'S_DQ_CLOSE'
        volume_col = 'volume' if 'volume' in data.columns else 'S_DQ_VOLUME'
        high_col = 'high' if 'high' in data.columns else 'S_DQ_HIGH'
        low_col = 'low' if 'low' in data.columns else 'S_DQ_LOW'
        open_col = 'open' if 'open' in data.columns else 'S_DQ_OPEN'
        
        # ä»·æ ¼ç‰¹å¾
        features['returns'] = data[close_col].pct_change()
        features['log_returns'] = np.log(data[close_col] / data[close_col].shift(1))
        features['high_low_ratio'] = data[high_col] / (data[low_col] + 1e-8)
        features['close_open_ratio'] = data[close_col] / (data[open_col] + 1e-8)
        
        # ç§»åŠ¨å¹³å‡
        for window in [5, 10, 20, 50]:
            features[f'ma_{window}'] = data[close_col].rolling(window=window).mean()
            features[f'ma_ratio_{window}'] = data[close_col] / (features[f'ma_{window}'] + 1e-8)
        
        # æ³¢åŠ¨ç‡ç‰¹å¾
        for window in [5, 10, 20]:
            features[f'volatility_{window}'] = features['returns'].rolling(window=window).std()
        
        # æˆäº¤é‡ç‰¹å¾
        features['volume_ratio'] = data[volume_col] / (data[volume_col].rolling(window=20).mean() + 1e-8)
        features['volume_ma_5'] = data[volume_col].rolling(window=5).mean()
        features['volume_ma_20'] = data[volume_col].rolling(window=20).mean()
        
        # åŠ¨é‡ç‰¹å¾
        for period in [1, 5, 10, 20]:
            features[f'momentum_{period}'] = data[close_col] / data[close_col].shift(period) - 1
        
        # æŠ€æœ¯æŒ‡æ ‡
        # RSI
        for window in [14, 21]:
            delta = data[close_col].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
            rs = gain / (loss + 1e-8)
            features[f'rsi_{window}'] = 100 - (100 / (1 + rs))
        
        # ä»·æ ¼ä½ç½®
        for window in [20, 50]:
            min_price = data[close_col].rolling(window=window).min()
            max_price = data[close_col].rolling(window=window).max()
            features[f'price_position_{window}'] = (data[close_col] - min_price) / (max_price - min_price + 1e-8)
        
        return features
    
    def _prepare_target(self, data: pd.DataFrame) -> pd.Series:
        """
        å‡†å¤‡ç›®æ ‡å˜é‡
        
        Args:
            data: å¸‚åœºæ•°æ®
            
        Returns:
            ç›®æ ‡å˜é‡Series
        """
        # è·å–æ­£ç¡®çš„åˆ—å
        close_col = 'close' if 'close' in data.columns else 'S_DQ_CLOSE'
        
        # âš ï¸ æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æœªæ¥1æœŸæ”¶ç›Šç‡ä½œä¸ºMLæ¨¡å‹çš„è®­ç»ƒç›®æ ‡
        # è¿™æ˜¯MLæ¨¡å‹è®­ç»ƒçš„æ­£å¸¸åšæ³•ï¼Œä¸æ˜¯å› å­è®¡ç®—ä¸­çš„æœªæ¥å‡½æ•°é—®é¢˜
        # åœ¨å®ç›˜ä½¿ç”¨æ—¶ï¼Œæ¨¡å‹å·²ç»è®­ç»ƒå®Œæˆï¼Œåªä½¿ç”¨å†å²ç‰¹å¾è¿›è¡Œé¢„æµ‹
        future_returns = data[close_col].shift(-1) / data[close_col] - 1
        return future_returns
    
    def build_all_ml_factors(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        æ„å»ºæ‰€æœ‰æœºå™¨å­¦ä¹ å› å­
        
        Args:
            data: å¸‚åœºæ•°æ®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            æ‰€æœ‰MLå› å­DataFrame
        """
        print("æ„å»ºæœºå™¨å­¦ä¹ å› å­...")
        
        all_factors = pd.DataFrame(index=data.index)
        
        # æ„å»ºå„ç±»MLå› å­
        factor_types = [
            'ensemble_factors',
            'pca_factors',
            'feature_selection_factors',
            'rolling_ml_factors',
            'adaptive_ml_factors'
        ]
        
        progress_callback = kwargs.get('progress_callback')
        
        # ç«‹å³å‘é€åˆå§‹è¿›åº¦æ›´æ–°
        if progress_callback:
            progress_callback(stage='ml', progress=1, message='æœºå™¨å­¦ä¹ å› å­: åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹æ„å»º...')
        
        for idx, factor_type in enumerate(tqdm(factor_types, desc="æ„å»ºMLå› å­", unit="ç±»å‹")):
            try:
                base_progress = int((idx / len(factor_types)) * 90)  # æ¯ä¸ªé˜¶æ®µå 90%ä¸­çš„ä¸€éƒ¨åˆ†
                
                if factor_type == 'ensemble_factors':
                    if progress_callback:
                        progress_callback(stage='ml', progress=base_progress, message='é›†æˆæ¨¡å‹è®­ç»ƒ: å¼€å§‹è®­ç»ƒé›†æˆæ¨¡å‹...')
                        progress_callback(stage='ml', progress=base_progress + 1, message='é›†æˆæ¨¡å‹è®­ç»ƒ: å‡†å¤‡ç‰¹å¾ä¸è®­ç»ƒé›†...')
                    factors = self.build_ensemble_factors(data, progress_callback=progress_callback, **kwargs)
                elif factor_type == 'pca_factors':
                    if progress_callback:
                        progress_callback(stage='ml', progress=base_progress + 2, message='PCA é™ç»´: å¼€å§‹ä¸»æˆåˆ†åˆ†æ...')
                    factors = self.build_pca_factors(data, progress_callback=progress_callback, **kwargs)
                elif factor_type == 'feature_selection_factors':
                    if progress_callback:
                        progress_callback(stage='ml', progress=base_progress + 2, message='ç‰¹å¾é€‰æ‹©: åˆ†æç‰¹å¾é‡è¦æ€§...')
                    factors = self.build_feature_selection_factors(data, progress_callback=progress_callback, **kwargs)
                elif factor_type == 'rolling_ml_factors':
                    if progress_callback:
                        progress_callback(stage='ml', progress=base_progress + 2, message='æ»šåŠ¨ML: å¼€å§‹æ—¶é—´çª—å£è®­ç»ƒ...')
                    factors = self.build_rolling_ml_factors(data, progress_callback=progress_callback, **kwargs)
                elif factor_type == 'adaptive_ml_factors':
                    if progress_callback:
                        progress_callback(stage='ml', progress=base_progress + 2, message='è‡ªé€‚åº”MLè®­ç»ƒ: åˆ†æå¸‚åœºç¯å¢ƒ...')
                    factors = self.build_adaptive_ml_factors(data, progress_callback=progress_callback, **kwargs)
                
                all_factors = pd.concat([all_factors, factors], axis=1)
                tqdm.write(f"âœ“ æˆåŠŸæ„å»º {factor_type}: {len(factors.columns)} ä¸ªå› å­")
                if progress_callback:
                    completion_progress = int(((idx+1) / len(factor_types)) * 90)
                    progress_callback(stage='ml', progress=completion_progress, message=f'{factor_type} å®Œæˆ: {len(factors.columns)} ä¸ªå› å­')
                
            except Exception as e:
                tqdm.write(f"âœ— æ„å»º {factor_type} å¤±è´¥: {e}")
                continue
        
        # å¤„ç†å¼‚å¸¸å€¼
        all_factors = all_factors.replace([np.inf, -np.inf], np.nan)
        all_factors = all_factors.fillna(method='ffill').fillna(method='bfill').fillna(0)
        
        print(f"æ€»å…±æ„å»ºäº† {len(all_factors.columns)} ä¸ªæœºå™¨å­¦ä¹ å› å­")
        if progress_callback:
            progress_callback(stage='ml', progress=100, message='MLå› å­æ„å»ºå®Œæˆ')
        
        return all_factors 