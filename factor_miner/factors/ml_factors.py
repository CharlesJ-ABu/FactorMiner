"""
æœºå™¨å­¦ä¹ å› å­æ„å»ºæ¨¡å—
ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•æ„å»ºå’Œä¼˜åŒ–å› å­
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional
from pathlib import Path
import pickle
import threading
import time
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥tqdmï¼Œå¹¶é…ç½®ä¸ºé€‚åˆåå°è¿è¡Œ
try:
    from tqdm import tqdm
    # é…ç½®tqdmä¸ºé€‚åˆåå°è¿è¡Œçš„æ¨¡å¼
    import os
    if os.environ.get('TQDM_DISABLE') != '1':
        # æ£€æŸ¥tqdmç‰ˆæœ¬ï¼Œä½¿ç”¨å…¼å®¹çš„é…ç½®æ–¹å¼
        try:
            # æ–°ç‰ˆæœ¬tqdmæ”¯æŒset_defaults
            tqdm.set_defaults(
                position=0,  # å›ºå®šä½ç½®
                leave=True,  # ä¿ç•™è¿›åº¦æ¡
                ncols=80,    # å›ºå®šå®½åº¦
                ascii=True,  # ä½¿ç”¨ASCIIå­—ç¬¦ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦é—®é¢˜
                dynamic_ncols=False  # å›ºå®šåˆ—æ•°
            )
        except AttributeError:
            # æ—§ç‰ˆæœ¬tqdmä¸æ”¯æŒset_defaultsï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            print("ä½¿ç”¨é»˜è®¤tqdmé…ç½®ï¼ˆæ—§ç‰ˆæœ¬ï¼‰")
except ImportError:
    # å¦‚æœæ²¡æœ‰tqdmï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„è¿›åº¦æ˜¾ç¤º
    class SimpleProgress:
        def __init__(self, iterable=None, desc="", total=None, unit="", **kwargs):
            self.iterable = iterable
            self.desc = desc
            self.total = total or (len(iterable) if iterable else 0)
            self.unit = unit
            self.current = 0
            if desc:
                print(f"{desc}: å¼€å§‹å¤„ç†...")
        
        def __iter__(self):
            if self.iterable:
                for item in self.iterable:
                    yield item
                    self.current += 1
                    if self.current % max(1, self.total // 10) == 0:  # æ¯10%æ˜¾ç¤ºä¸€æ¬¡
                        print(f"{self.desc}: {self.current}/{self.total} ({self.current/self.total*100:.1f}%)")
        
        def write(self, text):
            print(text)
        
        def __enter__(self):
            return self
        
        def __exit__(self, *args):
            if self.desc:
                print(f"{self.desc}: å®Œæˆ ({self.current}/{self.total})")
    
    tqdm = SimpleProgress


class ProgressSimulator:
    """æ¨¡æ‹Ÿè¿ç»­è¿›åº¦æ›´æ–°çš„ç±»ï¼Œç”¨äºåœ¨æ¨¡å‹è®­ç»ƒæœŸé—´æä¾›tqdmå¼çš„è¿›åº¦æ•ˆæœ"""
    
    def __init__(self, progress_callback, stage, start_progress, end_progress, duration, message_template):
        self.progress_callback = progress_callback
        self.stage = stage
        self.start_progress = start_progress
        self.end_progress = end_progress
        self.duration = duration
        self.message_template = message_template
        self.current_progress = start_progress
        self.timer = None
        self.stop_flag = False
        
    def start(self):
        """å¼€å§‹æ¨¡æ‹Ÿè¿›åº¦"""
        self.stop_flag = False
        # ç«‹å³å‘é€ç¬¬ä¸€ä¸ªè¿›åº¦æ›´æ–°
        if self.progress_callback:
            message = self.message_template.format(progress=self.current_progress)
            print(f"ğŸ“ˆ ProgressSimulatorå¼€å§‹: {self.stage} -> {self.current_progress}% | {message}")
            self.progress_callback(stage=self.stage, progress=self.current_progress, message=message)
        self._update_progress()
        
    def stop(self):
        """åœæ­¢æ¨¡æ‹Ÿè¿›åº¦"""
        self.stop_flag = True
        if self.timer:
            self.timer.cancel()
        # ç¡®ä¿æœ€ç»ˆè¿›åº¦ä¸ºç»“æŸå€¼
        if self.progress_callback:
            self.progress_callback(stage=self.stage, progress=self.end_progress, message=self.message_template.format(progress=self.end_progress))
    
    def _update_progress(self):
        """å†…éƒ¨è¿›åº¦æ›´æ–°æ–¹æ³•"""
        if self.stop_flag:
            return
            
        if self.current_progress < self.end_progress:
            # è®¡ç®—ä¸‹ä¸€ä¸ªè¿›åº¦ç‚¹
            increment = max(1, (self.end_progress - self.start_progress) // 10)  # æ¯æ¬¡å¢åŠ è¾ƒå¤§æ­¥é•¿ï¼Œæ›´æ˜æ˜¾
            self.current_progress = min(self.current_progress + increment, self.end_progress)
            
            if self.progress_callback:
                message = self.message_template.format(progress=self.current_progress)
                print(f"ğŸ“ˆ ProgressSimulatoræ›´æ–°: {self.stage} -> {self.current_progress}% | {message}")
                self.progress_callback(stage=self.stage, progress=self.current_progress, message=message)
            
            # è°ƒåº¦ä¸‹ä¸€æ¬¡æ›´æ–° - é€‚ä¸­çš„æ›´æ–°é¢‘ç‡ï¼Œé¿å…è¿‡äºé¢‘ç¹
            interval = max(0.5, self.duration / max(1, (self.end_progress - self.start_progress)))
            self.timer = threading.Timer(interval, self._update_progress)
            self.timer.start()


class MLFactorBuilder:
    """
    æœºå™¨å­¦ä¹ å› å­æ„å»ºå™¨
    ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•æ„å»ºå’Œä¼˜åŒ–å› å­
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–æœºå™¨å­¦ä¹ å› å­æ„å»ºå™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or {}
        self.scaler = StandardScaler()
        self.models = {}
        # æ¨¡å‹æŒä¹…åŒ–ç›®å½•ï¼šfactorlib/models
        self.models_dir = (Path(__file__).parent.parent.parent / "factorlib" / "models")
        self.models_dir.mkdir(parents=True, exist_ok=True)

    def _artifact_path(self, factor_id: str) -> Path:
        return self.models_dir / f"{factor_id}.pkl"

    def _save_model_artifact(
        self,
        factor_id: str,
        model,
        feature_columns: List[str],
        scaler: Optional[StandardScaler] = None
    ) -> None:
        try:
            artifact = {
                "model": model,
                "feature_columns": list(feature_columns),
                "scaler": scaler,
            }
            with open(self._artifact_path(factor_id), "wb") as f:
                pickle.dump(artifact, f)
        except Exception as _:
            print(f"âŒ ä¿å­˜æ¨¡å‹æ–‡ä»¶å¤±è´¥ {factor_id}")
        
    def build_ensemble_factors(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        æ„å»ºé›†æˆå­¦ä¹ å› å­
        
        Args:
            data: å¸‚åœºæ•°æ®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            é›†æˆå­¦ä¹ å› å­DataFrame
        """
        factors = {}
        progress_callback = kwargs.get('progress_callback')
        
        # å‡†å¤‡ç‰¹å¾ï¼ˆç»Ÿä¸€ç‰¹å¾ç®¡é“ï¼‰
        if progress_callback:
            progress_callback(stage='ml', progress=5, message='é›†æˆæ¨¡å‹è®­ç»ƒ: å‡†å¤‡ç‰¹å¾æ•°æ®...')
        try:
            from factor_miner.core.feature_pipeline import build_ml_features
            features = build_ml_features(data)
        except Exception:
            features = self._prepare_features(data)
        target = self._prepare_target(data)
        
        # ç§»é™¤NaNå€¼ - ç¡®ä¿ç´¢å¼•å¯¹é½
        features_nan_mask = features.isna().any(axis=1)
        target_nan_mask = target.isna()
        
        # é‡æ–°ç´¢å¼•ä»¥ç¡®ä¿å¯¹é½
        common_index = features.index.intersection(target.index)
        features_nan_aligned = features_nan_mask.reindex(common_index, fill_value=True)
        target_nan_aligned = target_nan_mask.reindex(common_index, fill_value=True)
        
        valid_idx = ~(features_nan_aligned | target_nan_aligned)
        features_clean = features.reindex(common_index).loc[valid_idx]
        target_clean = target.reindex(common_index).loc[valid_idx]
        
        if len(features_clean) < 100:
            print("æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•æ„å»ºé›†æˆå› å­")
            return pd.DataFrame(factors, index=data.index)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        if progress_callback:
            progress_callback(stage='ml', progress=15, message='é›†æˆæ¨¡å‹è®­ç»ƒ: æ ‡å‡†åŒ–ç‰¹å¾...')
        features_scaled = self.scaler.fit_transform(features_clean)
        
        # æ„å»ºå¤šä¸ªæ¨¡å‹
        models = {
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
            'ridge': Ridge(alpha=1.0),
            'lasso': Lasso(alpha=0.01)
        }
        
        # è®­ç»ƒæ¨¡å‹å¹¶ç”Ÿæˆé¢„æµ‹
        for model_idx, (name, model) in enumerate(tqdm(models.items(), desc="è®­ç»ƒé›†æˆæ¨¡å‹", unit="æ¨¡å‹")):
            base_progress = 20 + (model_idx / len(models)) * 60  # 20-80%
            
            # ç«‹å³å‘é€å¼€å§‹è®­ç»ƒçš„è¿›åº¦æ›´æ–°
            if progress_callback:
                progress_callback(stage='ml', progress=int(base_progress), message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: å¼€å§‹è®­ç»ƒ {name} æ¨¡å‹...')
                # ç«‹å³å‘é€ä¸€ä¸ªç¨é«˜çš„è¿›åº¦ï¼Œè®©ç”¨æˆ·çœ‹åˆ°å˜åŒ–
                progress_callback(stage='ml', progress=int(base_progress + 1), message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} åˆå§‹åŒ–...')
            
            try:
                # å¯åŠ¨è¿›åº¦æ¨¡æ‹Ÿå™¨æ¨¡æ‹Ÿæ¨¡å‹è®­ç»ƒè¿‡ç¨‹
                simulator = None
                if progress_callback:
                    simulator = ProgressSimulator(
                        progress_callback=progress_callback,
                        stage='ml',
                        start_progress=int(base_progress),
                        end_progress=int(base_progress + 12),
                        duration=2.0,  # 2ç§’çš„æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
                        message_template=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} è®­ç»ƒä¸­... ' + '{progress}%'
                    )
                    simulator.start()
                
                model.fit(features_scaled, target_clean)
                
                # åœæ­¢è¿›åº¦æ¨¡æ‹Ÿå™¨
                if simulator:
                    simulator.stop()
                
                if progress_callback:
                    progress_callback(stage='ml', progress=int(base_progress + 14), message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} ç”Ÿæˆé¢„æµ‹...')
                
                predictions = model.predict(features_scaled)
                
                # åˆ›å»ºå› å­åºåˆ—
                factor_series = pd.Series(index=data.index, dtype=float)
                factor_series.loc[valid_idx] = predictions
                
                factor_id = f"ensemble_{name}"
                factors[factor_id] = factor_series
                self.models[name] = model

                if progress_callback:
                    progress_callback(stage='ml', progress=int(base_progress + 12), message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} ä¿å­˜æ¨¡å‹...')

                # æŒä¹…åŒ–æ¨¡å‹ï¼ˆç”¨äºåç»­é€šè¿‡coreåŠ è½½æ¨ç†ï¼‰
                self._save_model_artifact(
                    factor_id=factor_id,
                    model=model,
                    feature_columns=list(features_clean.columns),
                    scaler=self.scaler
                )
                
                tqdm.write(f"âœ“ æˆåŠŸè®­ç»ƒ {name} æ¨¡å‹")
                if progress_callback:
                    completion_progress = 20 + ((model_idx + 1) / len(models)) * 60
                    progress_callback(stage='ml', progress=int(completion_progress), message=f'é›†æˆæ¨¡å‹è®­ç»ƒ: {name} å®Œæˆ')
            
            except Exception:
                tqdm.write(f"âœ— è®­ç»ƒ {name} æ¨¡å‹å¤±è´¥")
                continue
        
        if progress_callback:
            progress_callback(stage='ml', progress=85, message='é›†æˆæ¨¡å‹è®­ç»ƒ: ä¿å­˜æ¨¡å‹å®Œæˆ')
        
        return pd.DataFrame(factors, index=data.index)

    def build_pca_factors(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        æ„å»ºPCAå› å­
        
        Args:
            data: å¸‚åœºæ•°æ®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            PCAå› å­DataFrame
        """
        factors = {}
        progress_callback = kwargs.get('progress_callback')
        
        # å‡†å¤‡ç‰¹å¾
        if progress_callback:
            progress_callback(stage='ml', progress=5, message='PCA é™ç»´: å‡†å¤‡ç‰¹å¾æ•°æ®...')
        print("å‡†å¤‡PCAç‰¹å¾æ•°æ®...")
        features = self._prepare_features(data)
        print(f"PCAç‰¹å¾æ•°æ®å½¢çŠ¶: {features.shape}")
        
        if features.empty:
            print("âŒ PCAç‰¹å¾æ•°æ®ä¸ºç©ºï¼Œæ— æ³•æ„å»ºPCAå› å­")
            return pd.DataFrame(factors, index=data.index)
        
        # ç§»é™¤NaNå€¼
        valid_idx = ~features.isna().any(axis=1)
        features_clean = features.loc[valid_idx]
        
        print(f"PCAæœ‰æ•ˆæ•°æ®ç´¢å¼•æ•°é‡: {valid_idx.sum()}")
        
        if len(features_clean) < 50:
            print(f"âŒ PCAæ•°æ®é‡ä¸è¶³50æ¡ï¼ˆåªæœ‰{len(features_clean)}æ¡ï¼‰ï¼Œæ— æ³•æ„å»ºPCAå› å­")
            return pd.DataFrame(factors, index=data.index)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        if progress_callback:
            progress_callback(stage='ml', progress=20, message='PCA é™ç»´: æ ‡å‡†åŒ–ç‰¹å¾æ•°æ®...')
        print("æ ‡å‡†åŒ–PCAç‰¹å¾æ•°æ®...")
        features_scaled = self.scaler.fit_transform(features_clean)
        
        # è®¡ç®—PCA
        n_components = kwargs.get('n_components', min(10, features_clean.shape[1]))
        print(f"PCAç»„ä»¶æ•°: {n_components}")
        
        try:
            # å¯åŠ¨PCAè¿›åº¦æ¨¡æ‹Ÿå™¨
            simulator = None
            if progress_callback:
                simulator = ProgressSimulator(
                    progress_callback=progress_callback,
                    stage='ml',
                    start_progress=40,
                    end_progress=55,
                    duration=1.5,  # 1.5ç§’çš„æ¨¡æ‹ŸPCAæ—¶é—´
                    message_template='PCA é™ç»´: ä¸»æˆåˆ†åˆ†æè¿›è¡Œä¸­... {progress}%'
                )
                simulator.start()
            
            print("æ‰§è¡ŒPCAé™ç»´...")
            pca = PCA(n_components=n_components)
            pca_components = pca.fit_transform(features_scaled)
            
            # åœæ­¢PCAè¿›åº¦æ¨¡æ‹Ÿå™¨
            if simulator:
                simulator.stop()
            
            print(f"PCAé™ç»´å®Œæˆï¼Œå½¢çŠ¶: {pca_components.shape}")
            
            # åˆ›å»ºå› å­
            if progress_callback:
                progress_callback(stage='ml', progress=60, message='PCA é™ç»´: ç”ŸæˆPCAå› å­...')
            for i in tqdm(range(n_components), desc="åˆ›å»ºPCAå› å­", unit="å› å­"):
                if progress_callback and i % max(1, n_components // 5) == 0:
                    progress_pct = 60 + int((i / n_components) * 30)
                    progress_callback(stage='ml', progress=progress_pct, message=f'PCA é™ç»´: åˆ›å»ºç¬¬ {i+1}/{n_components} ä¸ªç»„ä»¶')
                
                factor_series = pd.Series(index=data.index, dtype=float)
                factor_series.loc[valid_idx] = pca_components[:, i]
                factors[f'pca_component_{i+1}'] = factor_series
            
            # ä¿å­˜è§£é‡Šæ–¹å·®æ¯”ä¾‹
            explained_variance_ratio = pca.explained_variance_ratio_
            tqdm.write(f"PCAè§£é‡Šæ–¹å·®æ¯”ä¾‹: {explained_variance_ratio[:5]}")
            print(f"å‰5ä¸ªç»„ä»¶ç´¯è®¡è§£é‡Šæ–¹å·®: {explained_variance_ratio[:5].sum():.3f}")
                
        except Exception as e:
            print(f"âŒ PCAè®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"âœ… PCAå› å­æ„å»ºå®Œæˆï¼Œå…± {len(factors)} ä¸ªå› å­")
        return pd.DataFrame(factors, index=data.index)
    
    def build_feature_selection_factors(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        æ„å»ºç‰¹å¾é€‰æ‹©å› å­
        
        Args:
            data: å¸‚åœºæ•°æ®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç‰¹å¾é€‰æ‹©å› å­DataFrame
        """
        factors = {}
        progress_callback = kwargs.get('progress_callback')
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
        if progress_callback:
            progress_callback(stage='ml', progress=5, message='ç‰¹å¾é€‰æ‹©: å‡†å¤‡ç‰¹å¾æ•°æ®...')
        features = self._prepare_features(data)
        target = self._prepare_target(data)
        
        # ç§»é™¤NaNå€¼ - ç¡®ä¿ç´¢å¼•å¯¹é½
        features_nan_mask = features.isna().any(axis=1)
        target_nan_mask = target.isna()
        
        # é‡æ–°ç´¢å¼•ä»¥ç¡®ä¿å¯¹é½
        common_index = features.index.intersection(target.index)
        features_nan_aligned = features_nan_mask.reindex(common_index, fill_value=True)
        target_nan_aligned = target_nan_mask.reindex(common_index, fill_value=True)
        
        valid_idx = ~(features_nan_aligned | target_nan_aligned)
        features_clean = features.reindex(common_index).loc[valid_idx]
        target_clean = target.reindex(common_index).loc[valid_idx]
        
        if len(features_clean) < 50:
            print("æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•æ„å»ºç‰¹å¾é€‰æ‹©å› å­")
            return pd.DataFrame(factors, index=data.index)
        
        # ç‰¹å¾é€‰æ‹©æ–¹æ³•
        selection_methods = {
            'f_regression': f_regression,
            'mutual_info': mutual_info_regression
        }
        
        k_best = kwargs.get('k_best', min(20, features_clean.shape[1]))
        
        for method_idx, (method_name, method_func) in enumerate(tqdm(selection_methods.items(), desc="ç‰¹å¾é€‰æ‹©", unit="æ–¹æ³•")):
            try:
                if progress_callback:
                    base_progress = 10 + (method_idx / len(selection_methods)) * 70
                    progress_callback(stage='ml', progress=int(base_progress), message=f'ç‰¹å¾é€‰æ‹©: æ‰§è¡Œ {method_name} ç®—æ³•...')
                
                # ç‰¹å¾é€‰æ‹©
                selector = SelectKBest(score_func=method_func, k=k_best)
                selector.fit(features_clean, target_clean)
                
                # è·å–é€‰ä¸­çš„ç‰¹å¾
                selected_features = features_clean.iloc[:, selector.get_support()]
                
                # è®¡ç®—é€‰ä¸­ç‰¹å¾çš„ç»„åˆ
                if len(selected_features.columns) > 0:
                    # ç®€å•å¹³å‡ç»„åˆ
                    factor_series = pd.Series(index=data.index, dtype=float)
                    factor_series.loc[valid_idx] = selected_features.mean(axis=1)
                    factors[f'feature_selection_{method_name}_mean'] = factor_series
                    
                    # åŠ æƒç»„åˆï¼ˆåŸºäºç‰¹å¾é‡è¦æ€§ï¼‰
                    scores = selector.scores_[selector.get_support()]
                    weights = scores / scores.sum()
                    weighted_factor = (selected_features * weights).sum(axis=1)
                    
                    factor_series = pd.Series(index=data.index, dtype=float)
                    factor_series.loc[valid_idx] = weighted_factor
                    factors[f'feature_selection_{method_name}_weighted'] = factor_series
                
                tqdm.write(f"âœ“ æˆåŠŸå®Œæˆ {method_name} ç‰¹å¾é€‰æ‹©")
                if progress_callback:
                    completion_progress = 10 + ((method_idx + 1) / len(selection_methods)) * 70
                    progress_callback(stage='ml', progress=int(completion_progress), message=f'ç‰¹å¾é€‰æ‹©: {method_name} å®Œæˆ, é€‰æ‹©äº† {len(selected_features.columns)} ä¸ªç‰¹å¾')
                
            except Exception:
                tqdm.write(f"âœ— ç‰¹å¾é€‰æ‹© {method_name} å¤±è´¥")
                continue
        
        return pd.DataFrame(factors, index=data.index)
    
    def build_rolling_ml_factors(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        æ„å»ºæ»šåŠ¨æœºå™¨å­¦ä¹ å› å­
        
        Args:
            data: å¸‚åœºæ•°æ®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            æ»šåŠ¨MLå› å­DataFrame
        """
        factors = {}
        progress_callback = kwargs.get('progress_callback')
        
        # å‡†å¤‡ç‰¹å¾
        if progress_callback:
            progress_callback(stage='ml', progress=5, message='æ»šåŠ¨ML: å‡†å¤‡æ—¶é—´åºåˆ—ç‰¹å¾...')
        features = self._prepare_features(data)
        target = self._prepare_target(data)
        
        # æ»šåŠ¨çª—å£å‚æ•°
        window = kwargs.get('window', 252)  # ä¸€å¹´
        step = kwargs.get('step', 21)  # ä¸€ä¸ªæœˆ
        
        # åˆå§‹åŒ–å› å­åºåˆ—
        factor_series = pd.Series(index=data.index, dtype=float)
        
        # æ»šåŠ¨è®­ç»ƒå’Œé¢„æµ‹
        total_steps = len(range(window, len(data), step))
        for step_idx, i in enumerate(tqdm(range(window, len(data), step), desc="æ»šåŠ¨MLè®­ç»ƒ", total=total_steps, unit="çª—å£")):
            # æ›´é¢‘ç¹çš„è¿›åº¦æ›´æ–°
            if progress_callback:
                progress_pct = 10 + int((step_idx / total_steps) * 80)
                progress_callback(stage='ml', progress=progress_pct, message=f'æ»šåŠ¨ML: çª—å£ {step_idx+1}/{total_steps}, è®­ç»ƒæœŸ {i-window}~{i}')
                
                # åœ¨çª—å£è®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ ä¸­é—´è¿›åº¦æ›´æ–°
                if step_idx % max(1, total_steps // 40) == 0:  # æ¯2.5%æ›´æ–°ä¸€æ¬¡
                    progress_callback(stage='ml', progress=progress_pct + 1, message=f'æ»šåŠ¨ML: å¤„ç†æ•°æ®çª—å£ {step_idx+1}/{total_steps}...')
            
            # è®­ç»ƒçª—å£
            train_features = features.iloc[i-window:i]
            train_target = target.iloc[i-window:i]
            
            # ç§»é™¤NaNå€¼ - ç¡®ä¿ç´¢å¼•å¯¹é½
            features_nan_mask = train_features.isna().any(axis=1)
            target_nan_mask = train_target.isna()
            
            # é‡æ–°ç´¢å¼•ä»¥ç¡®ä¿å¯¹é½
            common_index = train_features.index.intersection(train_target.index)
            features_nan_aligned = features_nan_mask.reindex(common_index, fill_value=True)
            target_nan_aligned = target_nan_mask.reindex(common_index, fill_value=True)
            
            valid_idx = ~(features_nan_aligned | target_nan_aligned)
            train_features_clean = train_features.reindex(common_index).loc[valid_idx]
            train_target_clean = train_target.reindex(common_index).loc[valid_idx]
            
            if len(train_features_clean) < 50:
                continue
            
            # æ ‡å‡†åŒ–ç‰¹å¾
            features_scaled = self.scaler.fit_transform(train_features_clean)
            
            # è®­ç»ƒæ¨¡å‹
            model = RandomForestRegressor(n_estimators=50, random_state=42)
            
            try:
                model.fit(features_scaled, train_target_clean)
                
                # é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹
                if i < len(data):
                    next_features = features.iloc[i:i+1]
                    if not next_features.isna().any().any():
                        next_features_scaled = self.scaler.transform(next_features)
                        prediction = model.predict(next_features_scaled)[0]
                        factor_series.iloc[i] = prediction
                
            except Exception:
                continue
        
        factors['rolling_ml_factor'] = factor_series
        
        return pd.DataFrame(factors, index=data.index)
    
    def build_adaptive_ml_factors(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        æ„å»ºè‡ªé€‚åº”æœºå™¨å­¦ä¹ å› å­
        
        Args:
            data: å¸‚åœºæ•°æ®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            è‡ªé€‚åº”MLå› å­DataFrame
        """
        factors = {}
        progress_callback = kwargs.get('progress_callback')
        
        # å‡†å¤‡ç‰¹å¾
        if progress_callback:
            progress_callback(stage='ml', progress=5, message='è‡ªé€‚åº”MLè®­ç»ƒ: å‡†å¤‡ç‰¹å¾ä¸ç¯å¢ƒæ£€æµ‹...')
        features = self._prepare_features(data)
        target = self._prepare_target(data)
        
        # è·å–æ­£ç¡®çš„åˆ—å
        close_col = 'close' if 'close' in data.columns else 'S_DQ_CLOSE'
        
        # å¸‚åœºçŠ¶æ€æ£€æµ‹
        returns = data[close_col].pct_change()
        volatility = returns.rolling(window=20).std()
        
        # æ ¹æ®æ³¢åŠ¨ç‡è°ƒæ•´æ¨¡å‹å‚æ•°
        high_vol_threshold = volatility.quantile(0.8)
        low_vol_threshold = volatility.quantile(0.2)
        
        # åˆå§‹åŒ–å› å­åºåˆ—
        factor_series = pd.Series(index=data.index, dtype=float)
        
        window = kwargs.get('window', 252)
        
        total_steps = len(range(window, len(data)))
        for step_idx, i in enumerate(tqdm(range(window, len(data)), desc="è‡ªé€‚åº”MLè®­ç»ƒ", total=total_steps, unit="æ—¶é—´ç‚¹")):
            if progress_callback and step_idx % max(1, total_steps // 50) == 0:  # æ¯2%æŠ¥å‘Šä¸€æ¬¡
                progress_pct = 10 + int((step_idx / total_steps) * 80)
                current_vol = volatility.iloc[i]
                vol_level = "é«˜æ³¢åŠ¨" if current_vol > high_vol_threshold else "ä½æ³¢åŠ¨" if current_vol < low_vol_threshold else "ä¸­ç­‰æ³¢åŠ¨"
                progress_callback(stage='ml', progress=progress_pct, message=f'è‡ªé€‚åº”MLè®­ç»ƒ: {step_idx+1}/{total_steps}, {vol_level}ç¯å¢ƒ')
            
            # é¢å¤–çš„ç»†ç²’åº¦æ›´æ–°
            if progress_callback and step_idx % max(1, total_steps // 100) == 0:  # æ¯1%ä¸€æ¬¡å¾®æ›´æ–°
                progress_pct = 10 + int((step_idx / total_steps) * 80)
                progress_callback(stage='ml', progress=progress_pct, message=f'è‡ªé€‚åº”MLè®­ç»ƒ: åˆ†ææ—¶é—´ç‚¹ {step_idx+1}/{total_steps}...')
            
            # è·å–å½“å‰æ³¢åŠ¨ç‡
            current_vol = volatility.iloc[i]
            
            # æ ¹æ®æ³¢åŠ¨ç‡é€‰æ‹©æ¨¡å‹å‚æ•°
            if current_vol > high_vol_threshold:
                # é«˜æ³¢åŠ¨ç‡ç¯å¢ƒï¼šä½¿ç”¨æ›´ä¿å®ˆçš„æ¨¡å‹
                model = Ridge(alpha=10.0)
            elif current_vol < low_vol_threshold:
                # ä½æ³¢åŠ¨ç‡ç¯å¢ƒï¼šä½¿ç”¨æ›´æ¿€è¿›çš„æ¨¡å‹
                model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)
            else:
                # ä¸­ç­‰æ³¢åŠ¨ç‡ç¯å¢ƒï¼šä½¿ç”¨å¹³è¡¡çš„æ¨¡å‹
                model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
            
            # è®­ç»ƒæ•°æ®
            train_features = features.iloc[i-window:i]
            train_target = target.iloc[i-window:i]
            
            # ç§»é™¤NaNå€¼ - ç¡®ä¿ç´¢å¼•å¯¹é½
            features_nan_mask = train_features.isna().any(axis=1)
            target_nan_mask = train_target.isna()
            
            # é‡æ–°ç´¢å¼•ä»¥ç¡®ä¿å¯¹é½
            common_index = train_features.index.intersection(train_target.index)
            features_nan_aligned = features_nan_mask.reindex(common_index, fill_value=True)
            target_nan_aligned = target_nan_mask.reindex(common_index, fill_value=True)
            
            valid_idx = ~(features_nan_aligned | target_nan_aligned)
            train_features_clean = train_features.reindex(common_index).loc[valid_idx]
            train_target_clean = train_target.reindex(common_index).loc[valid_idx]
            
            if len(train_features_clean) < 50:
                continue
            
            # æ ‡å‡†åŒ–ç‰¹å¾
            features_scaled = self.scaler.fit_transform(train_features_clean)
            
            try:
                model.fit(features_scaled, train_target_clean)
                
                # é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹
                if i < len(data):
                    next_features = features.iloc[i:i+1]
                    if not next_features.isna().any().any():
                        next_features_scaled = self.scaler.transform(next_features)
                        prediction = model.predict(next_features_scaled)[0]
                        factor_series.iloc[i] = prediction
                
            except Exception:
                continue
        
        factors['adaptive_ml_factor'] = factor_series
        
        return pd.DataFrame(factors, index=data.index)
    
    def _prepare_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        å‡†å¤‡ç‰¹å¾æ•°æ®
        
        Args:
            data: å¸‚åœºæ•°æ®
            
        Returns:
            ç‰¹å¾DataFrame
        """
        features = pd.DataFrame(index=data.index)
        
        # è·å–æ­£ç¡®çš„åˆ—å
        close_col = 'close' if 'close' in data.columns else 'S_DQ_CLOSE'
        volume_col = 'volume' if 'volume' in data.columns else 'S_DQ_VOLUME'
        high_col = 'high' if 'high' in data.columns else 'S_DQ_HIGH'
        low_col = 'low' if 'low' in data.columns else 'S_DQ_LOW'
        open_col = 'open' if 'open' in data.columns else 'S_DQ_OPEN'
        
        # ä»·æ ¼ç‰¹å¾
        features['returns'] = data[close_col].pct_change()
        features['log_returns'] = np.log(data[close_col] / data[close_col].shift(1))
        features['high_low_ratio'] = data[high_col] / (data[low_col] + 1e-8)
        features['close_open_ratio'] = data[close_col] / (data[open_col] + 1e-8)
        
        # ç§»åŠ¨å¹³å‡
        for window in [5, 10, 20, 50]:
            features[f'ma_{window}'] = data[close_col].rolling(window=window).mean()
            features[f'ma_ratio_{window}'] = data[close_col] / (features[f'ma_{window}'] + 1e-8)
        
        # æ³¢åŠ¨ç‡ç‰¹å¾
        for window in [5, 10, 20]:
            features[f'volatility_{window}'] = features['returns'].rolling(window=window).std()
        
        # æˆäº¤é‡ç‰¹å¾
        features['volume_ratio'] = data[volume_col] / (data[volume_col].rolling(window=20).mean() + 1e-8)
        features['volume_ma_5'] = data[volume_col].rolling(window=5).mean()
        features['volume_ma_20'] = data[volume_col].rolling(window=20).mean()
        
        # åŠ¨é‡ç‰¹å¾
        for period in [1, 5, 10, 20]:
            features[f'momentum_{period}'] = data[close_col] / data[close_col].shift(period) - 1
        
        # æŠ€æœ¯æŒ‡æ ‡
        # RSI
        for window in [14, 21]:
            delta = data[close_col].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
            rs = gain / (loss + 1e-8)
            features[f'rsi_{window}'] = 100 - (100 / (1 + rs))
        
        # ä»·æ ¼ä½ç½®
        for window in [20, 50]:
            min_price = data[close_col].rolling(window=window).min()
            max_price = data[close_col].rolling(window=window).max()
            features[f'price_position_{window}'] = (data[close_col] - min_price) / (max_price - min_price + 1e-8)
        
        return features
    
    def _prepare_target(self, data: pd.DataFrame) -> pd.Series:
        """
        å‡†å¤‡ç›®æ ‡å˜é‡
        
        Args:
            data: å¸‚åœºæ•°æ®
            
        Returns:
            ç›®æ ‡å˜é‡Series
        """
        # è·å–æ­£ç¡®çš„åˆ—å
        close_col = 'close' if 'close' in data.columns else 'S_DQ_CLOSE'
        
        # âš ï¸ æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æœªæ¥1æœŸæ”¶ç›Šç‡ä½œä¸ºMLæ¨¡å‹çš„è®­ç»ƒç›®æ ‡
        # è¿™æ˜¯MLæ¨¡å‹è®­ç»ƒçš„æ­£å¸¸åšæ³•ï¼Œä¸æ˜¯å› å­è®¡ç®—ä¸­çš„æœªæ¥å‡½æ•°é—®é¢˜
        # åœ¨å®ç›˜ä½¿ç”¨æ—¶ï¼Œæ¨¡å‹å·²ç»è®­ç»ƒå®Œæˆï¼Œåªä½¿ç”¨å†å²ç‰¹å¾è¿›è¡Œé¢„æµ‹
        future_returns = data[close_col].shift(-1) / data[close_col] - 1
        return future_returns
    
    def build_all_ml_factors(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        æ„å»ºæ‰€æœ‰æœºå™¨å­¦ä¹ å› å­
        
        Args:
            data: å¸‚åœºæ•°æ®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            æ‰€æœ‰MLå› å­DataFrame
        """
        print("æ„å»ºæœºå™¨å­¦ä¹ å› å­...")
        
        all_factors = pd.DataFrame(index=data.index)
        
        # æ„å»ºå„ç±»MLå› å­
        factor_types = [
            'ensemble_factors',
            'pca_factors',
            'feature_selection_factors',
            'rolling_ml_factors',
            'adaptive_ml_factors'
        ]
        
        progress_callback = kwargs.get('progress_callback')
        
        # ç«‹å³å‘é€åˆå§‹è¿›åº¦æ›´æ–°
        if progress_callback:
            progress_callback(stage='ml', progress=1, message='æœºå™¨å­¦ä¹ å› å­: åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹æ„å»º...')
        
        for idx, factor_type in enumerate(tqdm(factor_types, desc="æ„å»ºMLå› å­", unit="ç±»å‹")):
            try:
                base_progress = int((idx / len(factor_types)) * 90)  # æ¯ä¸ªé˜¶æ®µå 90%ä¸­çš„ä¸€éƒ¨åˆ†
                
                if factor_type == 'ensemble_factors':
                    if progress_callback:
                        progress_callback(stage='ml', progress=base_progress, message='é›†æˆæ¨¡å‹è®­ç»ƒ: å¼€å§‹è®­ç»ƒé›†æˆæ¨¡å‹...')
                        progress_callback(stage='ml', progress=base_progress + 1, message='é›†æˆæ¨¡å‹è®­ç»ƒ: å‡†å¤‡ç‰¹å¾ä¸è®­ç»ƒé›†...')
                    factors = self.build_ensemble_factors(data, progress_callback=progress_callback, **kwargs)
                elif factor_type == 'pca_factors':
                    if progress_callback:
                        progress_callback(stage='ml', progress=base_progress + 2, message='PCA é™ç»´: å¼€å§‹ä¸»æˆåˆ†åˆ†æ...')
                    factors = self.build_pca_factors(data, progress_callback=progress_callback, **kwargs)
                elif factor_type == 'feature_selection_factors':
                    if progress_callback:
                        progress_callback(stage='ml', progress=base_progress + 2, message='ç‰¹å¾é€‰æ‹©: åˆ†æç‰¹å¾é‡è¦æ€§...')
                    factors = self.build_feature_selection_factors(data, progress_callback=progress_callback, **kwargs)
                elif factor_type == 'rolling_ml_factors':
                    if progress_callback:
                        progress_callback(stage='ml', progress=base_progress + 2, message='æ»šåŠ¨ML: å¼€å§‹æ—¶é—´çª—å£è®­ç»ƒ...')
                    factors = self.build_rolling_ml_factors(data, progress_callback=progress_callback, **kwargs)
                elif factor_type == 'adaptive_ml_factors':
                    if progress_callback:
                        progress_callback(stage='ml', progress=base_progress + 2, message='è‡ªé€‚åº”MLè®­ç»ƒ: åˆ†æå¸‚åœºç¯å¢ƒ...')
                    factors = self.build_adaptive_ml_factors(data, progress_callback=progress_callback, **kwargs)
                
                all_factors = pd.concat([all_factors, factors], axis=1)
                tqdm.write(f"âœ“ æˆåŠŸæ„å»º {factor_type}: {len(factors.columns)} ä¸ªå› å­")
                if progress_callback:
                    completion_progress = int(((idx+1) / len(factor_types)) * 90)
                    progress_callback(stage='ml', progress=completion_progress, message=f'{factor_type} å®Œæˆ: {len(factors.columns)} ä¸ªå› å­')
                
            except Exception as e:
                tqdm.write(f"âœ— æ„å»º {factor_type} å¤±è´¥: {e}")
                continue
        
        # å¤„ç†å¼‚å¸¸å€¼
        all_factors = all_factors.replace([np.inf, -np.inf], np.nan)
        all_factors = all_factors.fillna(method='ffill').fillna(method='bfill').fillna(0)
        
        print(f"æ€»å…±æ„å»ºäº† {len(all_factors.columns)} ä¸ªæœºå™¨å­¦ä¹ å› å­")
        if progress_callback:
            progress_callback(stage='ml', progress=100, message='MLå› å­æ„å»ºå®Œæˆ')
        
        return all_factors 